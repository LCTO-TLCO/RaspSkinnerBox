{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as collections\n",
    "import matplotlib.markers as markers\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# debug = True\n",
    "debug = False\n",
    "\n",
    "\n",
    "class task_data:\n",
    "    def __init__(self, mice: list, tasks, logpath):\n",
    "        global debug\n",
    "        self.data_file = \"\"\n",
    "        self.data = None\n",
    "        self.data_ci = None\n",
    "        self.delta = None\n",
    "        self.mouse_no = mice\n",
    "        self.tasks = tasks\n",
    "        self.pattern_prob = {}\n",
    "        self.probability = None\n",
    "        self.mice_task = None\n",
    "        self.task_prob = {}\n",
    "        self.mice_delta = {}\n",
    "        self.entropy_analyze = None\n",
    "        self.mice_entropy = None\n",
    "        self.logpath = logpath\n",
    "        self.session_id = 0\n",
    "        self.burst_id = 0\n",
    "        self.data_not_omission = None\n",
    "        self.fig_prob_tmp = None\n",
    "        self.fig_prob = {}\n",
    "        self.bit = 4\n",
    "\n",
    "        print('reading data...', end='')\n",
    "\n",
    "        # TODO debug\n",
    "        # 1.self.data, 2.probability, 3.task_prob, 4.self.delta, 5.self.fig_prob_tmp, 6.pattern, 7.self.entropy_analyze\n",
    "        # to:dict, add:dict[task]\n",
    "        # to:dict, add:dict(fig) or DF\n",
    "        def append_dataframe(to: Union[pd.DataFrame, dict, None], add: Union[pd.DataFrame, dict, None], mouse_id: int,\n",
    "                             task=None, fig_num=None):\n",
    "            if isinstance(add, dict):\n",
    "\n",
    "                # 二回目の場合Fig確定\n",
    "                if not isinstance(task, type(None)):\n",
    "                    # print(add)\n",
    "                    ret_val = {}\n",
    "                    [ret_val.update(append_dataframe(to.get(task, {}), add[fig], mouse_id, task=task, fig_num=fig)) for\n",
    "                     fig in [\"fig1\", \"fig2\", \"fig3\"]]\n",
    "                    return {task: ret_val}\n",
    "                # taskごと\n",
    "                # to;dict, add:dict[task]\n",
    "                for add_task, add_dict in add.items():\n",
    "                    # append_dataframe(to, append_dataframe(to, add_dict, mouse_id, task=add_task), mouse_id)\n",
    "                    to.update(append_dataframe(to, add_dict, mouse_id, task=add_task))\n",
    "                return to\n",
    "            if isinstance(to, dict):\n",
    "                # Fig二回目入力\n",
    "                if not isinstance(fig_num, type(None)):\n",
    "                    return {fig_num: append_dataframe(to.get(fig_num, None), add, mouse_id)}\n",
    "                return {\n",
    "                    task: append_dataframe(to.get(task, None), add, mouse_id)}\n",
    "            if isinstance(to, type(None)):\n",
    "                return add.assign(mouse_id=mouse_id)\n",
    "            else:\n",
    "                return to.append(add.assign(mouse_id=mouse_id), ignore_index=True)\n",
    "\n",
    "        if debug:\n",
    "            for mouse_id in self.mouse_no:\n",
    "                print('mouse_id={}'.format(mouse_id))\n",
    "                self.mice_task[mouse_id], self.probability[mouse_id], self.task_prob[mouse_id], self.mice_delta[\n",
    "                    mouse_id], self.fig_prob[mouse_id] = self.dev_read_data(mouse_id)\n",
    "                # tmp = self.dev_read_data(mouse_id)\n",
    "                # self.mice_task = append_dataframe(self.mice_task, tmp[0], mouse_id)\n",
    "                # self.probability = append_dataframe(self.probability, tmp[1], mouse_id)\n",
    "                # self.task_prob = append_dataframe(self.task_prob, tmp[2], mouse_id)\n",
    "                # self.mice_delta = append_dataframe(self.mice_delta, tmp[3], mouse_id)\n",
    "                # # append_dataframe(self.fig_prob, tmp[4], mouse_id)\n",
    "                # self.fig_prob[mouse_id] = self.fig_prob[mouse_id].append(tmp[4])\n",
    "                # self.pattern_prob = append_dataframe(self.pattern_prob, tmp[5], mouse_id)\n",
    "                # TODO entropy_analyze\n",
    "        else:\n",
    "            # all 0: 2.probability\n",
    "            # 3.task_prob, 4.self.delta, 5.self.fig_prob_tmp, 6.pattern\n",
    "            for mouse_id in self.mouse_no:\n",
    "                try:\n",
    "                    print('mouse_id={}'.format(mouse_id))\n",
    "                    # self.data_file = \"{}no{:03d}_action.csv\".format(self.logpath, mouse_id)\n",
    "                    # self.mice_task[mouse_id], self.probability[mouse_id], self.task_prob[mouse_id], self.mice_delta[\n",
    "                    #     mouse_id], self.fig_prob[mouse_id], self.pattern_prob[mouse_id] = self.read_data()\n",
    "                    self.data_file = os.path.join(self.logpath, \"no{:03d}_action.csv\".format(mouse_id))\n",
    "                    tmp = self.read_data()\n",
    "                    self.mice_task = append_dataframe(self.mice_task, tmp[0], mouse_id)\n",
    "                    # 0\n",
    "#                     self.probability = append_dataframe(self.probability, tmp[1], mouse_id)\n",
    "                    self.task_prob = append_dataframe(self.task_prob, tmp[2], mouse_id)\n",
    "#                     self.mice_delta = append_dataframe(self.mice_delta, tmp[3], mouse_id)\n",
    "                    # 単体\n",
    "#                     self.fig_prob = append_dataframe(self.fig_prob, tmp[4], mouse_id)\n",
    "#                     self.pattern_prob = append_dataframe(self.pattern_prob, tmp[5], mouse_id)\n",
    "#                     self.mice_entropy = append_dataframe(self.mice_entropy, tmp[6], mouse_id)\n",
    "                except Exception as e:\n",
    "                    print(\"error! no {}\".format(mouse_id))\n",
    "                    print(e)\n",
    "                    continue\n",
    "            self.export_csv()\n",
    "        print('done')\n",
    "\n",
    "    def read_data(self):\n",
    "\n",
    "        def rehash_session_id():\n",
    "            data = pd.read_csv(self.data_file, names=header, parse_dates=[0], dtype={'hole_no': 'str'})\n",
    "            self.session_id = 0\n",
    "            print(\"max_id_col:{}\".format(len(data)))\n",
    "\n",
    "            def remove_terminate(index):\n",
    "                if data.at[index, \"event_type\"] == data.at[index + 1, \"event_type\"] and data.at[\n",
    "                    index, \"event_type\"] == \"start\":\n",
    "                    data.drop(index, inplace=True)\n",
    "\n",
    "            def rehash(x_index):\n",
    "                start_task = data.head(1).task.values[0]\n",
    "                if data.at[data.index[x_index], \"task\"] == start_task:\n",
    "                    if (x_index == 0 or data.shift(1).at[data.index[x_index], \"event_type\"] == \"start\") and \\\n",
    "                            len(data[:x_index][data.session_id == 0]) == 0:\n",
    "                        self.session_id = 0\n",
    "                        return 0\n",
    "                    self.session_id = self.session_id + 1\n",
    "                    return self.session_id\n",
    "                if data.at[data.index[x_index], \"event_type\"] == \"start\":\n",
    "                    self.session_id = self.session_id + 1\n",
    "                    return self.session_id\n",
    "                else:\n",
    "                    return self.session_id\n",
    "\n",
    "            list(map(remove_terminate, data.index[:-1]))\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            data[\"session_id\"] = list(map(rehash, data.index))\n",
    "            data = data[\n",
    "                data.session_id.isin(data.session_id[data.event_type.isin([\"reward\", \"failure\", \"time over\"])])]\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            self.session_id = 0\n",
    "            data[\"session_id\"] = list(map(rehash, data.index))\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "            return data\n",
    "\n",
    "        def add_timedelta():\n",
    "            data = self.data\n",
    "            data = data[data.session_id.isin(data[data.event_type.isin(['reward', 'failure'])][\"session_id\"])]\n",
    "            deltas = {}\n",
    "            for task in self.tasks:\n",
    "                def calculate(session):\n",
    "                    delta_df = pd.DataFrame()\n",
    "                    # reaction time\n",
    "                    current_target = data[data.session_id.isin([session])]\n",
    "                    if bool(sum(current_target[\"event_type\"].isin([\"task called\"]))):\n",
    "                        task_call = current_target[current_target[\"event_type\"] == \"task called\"]\n",
    "                        task_end = current_target[current_target[\"event_type\"].isin([\"nose poke\", \"failure\"])]\n",
    "                        reaction_time = task_end.at[task_end.index[0], \"timestamps\"] - task_call.at[\n",
    "                            task_call.index[0], \"timestamps\"]\n",
    "                        # 連続無報酬期間\n",
    "                        previous_reward = data[\n",
    "                            (data[\"event_type\"] == \"reward\") & (\n",
    "                                    data[\"timestamps\"] < task_call.at[task_call.index[0], \"timestamps\"])].tail(1)\n",
    "                        norewarded_time = task_call.at[task_call.index[0], \"timestamps\"] - previous_reward.at[\n",
    "                            previous_reward.index[0], \"timestamps\"]\n",
    "                        correct_failure = \"correct\" if bool(\n",
    "                            sum(current_target[\"event_type\"].isin([\"reward\"]))) else \"failure\"\n",
    "                        # df 追加\n",
    "                        delta_df = delta_df.append(\n",
    "                            {'session_id': session,\n",
    "                             'type': 'reaction_time',\n",
    "                             'noreward_duration_sec': pd.to_timedelta(norewarded_time) / np.timedelta64(1, 's'),\n",
    "                             'reaction_time_sec': pd.to_timedelta(reaction_time) / np.timedelta64(1, 's'),\n",
    "                             'correct_failure': correct_failure},\n",
    "                            ignore_index=True)\n",
    "                    # reward latency\n",
    "                    if bool(sum(current_target[\"event_type\"].isin([\"reward\"]))) and bool(\n",
    "                            sum(current_target[\"event_type\"].isin([\"task called\"]))):\n",
    "                        nose_poke = current_target[current_target[\"event_type\"] == \"nose poke\"]\n",
    "                        reward_latency = current_target[current_target[\"event_type\"] == \"magazine nose poked\"]\n",
    "                        reward_latency = reward_latency.at[reward_latency.index[0], \"timestamps\"] - \\\n",
    "                                         nose_poke.at[nose_poke.index[0], \"timestamps\"]\n",
    "                        previous_reward = data[\n",
    "                            (data[\"event_type\"] == \"reward\") & (\n",
    "                                    data[\"timestamps\"] < nose_poke.at[nose_poke.index[0], \"timestamps\"])].tail(1)\n",
    "                        norewarded_time = nose_poke.at[nose_poke.index[0], \"timestamps\"] - previous_reward.at[\n",
    "                            previous_reward.index[0], \"timestamps\"]\n",
    "                        delta_df = delta_df.append(\n",
    "                            {'session_id': session,\n",
    "                             'type': 'reward_latency',\n",
    "                             'noreward_duration_sec': pd.to_timedelta(norewarded_time) / np.timedelta64(1, 's'),\n",
    "                             'reward_latency_sec': pd.to_timedelta(reward_latency) / np.timedelta64(1, 's')\n",
    "                             }, ignore_index=True)\n",
    "                    return delta_df\n",
    "\n",
    "                delta_df = data[data.task == task].session_id.drop_duplicates().map(calculate)\n",
    "                deltas[task] = pd.concat(list(delta_df), sort=False) if len(delta_df) else delta_df\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "            return deltas\n",
    "\n",
    "        def add_hot_vector():\n",
    "            #    data = data[data[\"event_type\"].isin([\"reward\", \"failure\", \"time over\"])]\n",
    "            data = self.data\n",
    "            # data = data[data[\".seevent_type\"].isin([\"reward\", \"failure\", \"time over\"])]\n",
    "            # data = data[data[\"task\"].isin(self.tasks)]\n",
    "\n",
    "            data = data.reset_index(drop=True)\n",
    "            # task interval\n",
    "            task_start_index = [0]\n",
    "            for i in range(1, len(data)):\n",
    "                if not data[\"task\"][i] == data[\"task\"][i - 1]:\n",
    "                    task_start_index.append(i)\n",
    "\n",
    "            data[\"hole_correct\"] = -1\n",
    "            data[\"hole_failure\"] = -1\n",
    "            data[\"is_correct\"] = -1\n",
    "            data[\"is_failure\"] = -1\n",
    "            data[\"is_omission\"] = -1\n",
    "            data[\"cumsum_correct\"] = -1\n",
    "            data[\"cumsum_failure\"] = -1\n",
    "            data[\"cumsum_omission\"] = -1\n",
    "            data[\"cumsum_correct_taskreset\"] = -1\n",
    "            data[\"cumsum_failure_taskreset\"] = -1\n",
    "            data[\"cumsum_omission_taskreset\"] = -1\n",
    "            for hole_no in range(1, 9 + 1, 2):\n",
    "                data[\"is_hole{}\".format(str(hole_no))] = -1\n",
    "\n",
    "            # print(data)\n",
    "\n",
    "            # hole_information\n",
    "            # warning mettyaderu zone\n",
    "            # SettingWithCopyWarning\n",
    "            data.loc[data[\"event_type\"].isin([\"reward\"]), 'hole_correct'] = data[\"hole_no\"]\n",
    "            data.loc[~data[\"event_type\"].isin([\"reward\"]), 'hole_correct'] = np.nan\n",
    "            data.loc[data[\"event_type\"].isin([\"failure\"]), 'hole_failure'] = data[\"hole_no\"]\n",
    "            data.loc[~data[\"event_type\"].isin([\"failure\"]), 'hole_failure'] = np.nan\n",
    "\n",
    "            data.loc[data[\"event_type\"].isin([\"reward\"]), 'is_correct'] = 1\n",
    "            data.loc[~data[\"event_type\"].isin([\"reward\"]), 'is_correct'] = np.nan\n",
    "            data.loc[data[\"event_type\"].isin([\"failure\"]), 'is_failure'] = 1\n",
    "            data.loc[~data[\"event_type\"].isin([\"failure\"]), 'is_failure'] = np.nan\n",
    "            data.loc[data[\"event_type\"].isin([\"time over\"]), 'is_omission'] = 1\n",
    "            data.loc[~data[\"event_type\"].isin([\"time over\"]), 'is_omission'] = np.nan\n",
    "\n",
    "            data[\"cumsum_correct\"] = data[\"is_correct\"].cumsum(axis=0)\n",
    "            data[\"cumsum_failure\"] = data[\"is_failure\"].cumsum(axis=0)\n",
    "            data[\"cumsum_omission\"] = data[\"is_omission\"].cumsum(axis=0)\n",
    "\n",
    "            for hole_no in range(1, 9 + 1, 2):\n",
    "                data.loc[data['hole_no'] == str(hole_no), \"is_hole{}\".format(hole_no)] = 1\n",
    "                data.loc[~(data['hole_no'] == str(hole_no)), \"is_hole{}\".format(hole_no)] = None\n",
    "\n",
    "            # cumsum\n",
    "            # for i in range(0, len(task_start_index)):\n",
    "            #     index_start = task_start_index[i]\n",
    "            #     index_end = len(data)\n",
    "            #     if i < len(task_start_index) - 1:\n",
    "            #         index_end = task_start_index[i + 1]\n",
    "            #     pre_correct = data[\"cumsum_correct\"][index_start] if not i == 0 else 0\n",
    "            #     pre_incorrect = data[\"cumsum_incorrect\"][index_start] if not i == 0 else 0\n",
    "            #     pre_omission = data[\"cumsum_omission\"][index_start] if not i == 0 else 0\n",
    "            #     # warning mettyaderu zone\n",
    "            #     data[\"cumsum_correct_taskreset\"][index_start:index_end] = data[\"cumsum_correct\"][\n",
    "            #                                                               index_start:index_end] - \\\n",
    "            #                                                               pre_correct\n",
    "            #     data[\"cumsum_incorrect_taskreset\"][index_start:index_end] = data[\"cumsum_incorrect\"][\n",
    "            #                                                                 index_start:index_end] - \\\n",
    "            #                                                                 pre_incorrect\n",
    "            #     data[\"cumsum_omission_taskreset\"][index_start:index_end] = data[\"cumsum_omission\"][\n",
    "            #                                                                index_start:index_end] - \\\n",
    "            #                                                                pre_omission\n",
    "            def add_cumsum():\n",
    "                data[\"cumsum_correct_taskreset\"] = data[\"is_correct\"].fillna(0)\n",
    "                data[\"cumsum_failure_taskreset\"] = data[\"is_failure\"].fillna(0)\n",
    "                data[\"cumsum_omission_taskreset\"] = data[\"is_omission\"].fillna(0)\n",
    "                data[\"cumsum_correct_taskreset\"] = data.groupby(\"task\")[\"cumsum_correct_taskreset\"].cumsum()\n",
    "                data[\"cumsum_failure_taskreset\"] = data.groupby(\"task\")[\"cumsum_failure_taskreset\"].cumsum()\n",
    "                data[\"cumsum_omission_taskreset\"] = data.groupby(\"task\")[\"cumsum_omission_taskreset\"].cumsum()\n",
    "\n",
    "#             add_cumsum()\n",
    "\n",
    "            # burst\n",
    "            # data[\"burst_group\"] = 1\n",
    "            # for i in range(1, len(data)):\n",
    "            #     if data[\"timestamps\"][i] - data[\"timestamps\"][i - 1] <= datetime.timedelta(seconds=60):\n",
    "            #         data[\"burst_group\"][i] = data[\"burst_group\"][i - 1]\n",
    "            #         continue\n",
    "            #     data[\"burst_group\"][i] = data[\"burst_group\"][i - 1] + 1\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "            return data\n",
    "\n",
    "        def calc_entropy(section=150):\n",
    "            data = self.data[self.data.event_type.isin(['reward', 'failure'])]\n",
    "\n",
    "            def min_max(x, axis=None):\n",
    "                np.array(x)\n",
    "                min = np.array(x).min(axis=axis)\n",
    "                max = np.array(x).max(axis=axis)\n",
    "                result = (x - min) / (max - min)\n",
    "                return result\n",
    "\n",
    "            # entropy\n",
    "            ent = [np.nan] * section\n",
    "            for i in range(0, len(data) - section):\n",
    "                denominator = float(section)\n",
    "                # sum([data[\"is_hole{}\".format(str(hole_no))][i:i + 150].sum() for hole_no in range(1, 9 + 1, 2)])\n",
    "                current_entropy = min_max(\n",
    "                    [data[\"is_hole{}\".format(str(hole_no))][i:i + section].sum() /\n",
    "                     denominator for hole_no in [1, 3, 5, 7, 9]])\n",
    "                ent.append(entropy(current_entropy, base=2))\n",
    "            # region Description\n",
    "            # data[data.event_type.isin(['reward', 'failure'])][\"hole_choice_entropy\"] = ent\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "            return pd.DataFrame(ent).fillna(0.0).values.tolist()\n",
    "            # endregion\n",
    "\n",
    "        def count_task() -> dict:\n",
    "            dc = self.data[self.data[\"event_type\"].isin([\"reward\", \"failure\"])]\n",
    "            # dc = self.data[self.data[\"event_type\"].isin([\"reward\", \"failure\", \"time over\"])]\n",
    "\n",
    "            dc = dc.reset_index()\n",
    "\n",
    "            after_c_all_task = {}\n",
    "            after_f_all_task = {}\n",
    "\n",
    "            after_c_starts_task = {}\n",
    "            after_f_starts_task = {}\n",
    "\n",
    "            prob_index = [\"c_same\", \"c_diff\", \"c_omit\", \"c_checksum\", \"f_same\", \"f_diff\", \"f_omit\", \"f_checksum\",\n",
    "                          \"c_NotMax\",\n",
    "                          \"f_NotMax\", \"o_NotMax\"]\n",
    "            forward_trace = 10\n",
    "\n",
    "            for task in self.tasks:\n",
    "                after_c_starts_task[task] = dc[(dc[\"is_correct\"] == 1) & (dc[\"task\"] == task)]\n",
    "                after_f_starts_task[task] = dc[(dc[\"is_failure\"] == 1) & (dc[\"task\"] == task)]\n",
    "                after_c_all_task[task] = float(len(after_c_starts_task[task]))\n",
    "                after_f_all_task[task] = float(len(after_f_starts_task[task]))\n",
    "\n",
    "                prob = pd.DataFrame(columns=prob_index, index=range(1, forward_trace)).fillna(0.0)\n",
    "                # correctスタート\n",
    "                for idx, dt in after_c_starts_task[task].iterrows():\n",
    "                    for j in range(1, min(forward_trace, len(dc) - idx)):\n",
    "                        #                    for j in range(1, min(forward_trace, len(self.data_cio) - idx)):\n",
    "                        # 報酬を得たときと同じ選択(CF両方)をしたときの処理\n",
    "                        if dt[\"hole_no\"] == dc[\"hole_no\"][idx + j]:\n",
    "                            prob[\"c_same\"][j] = prob[\"c_same\"][j] + 1\n",
    "                        # omissionの場合\n",
    "                        elif dc[\"is_omission\"][idx + j] == 1:\n",
    "                            prob[\"c_omit\"][j] = prob[\"c_omit\"][j] + 1\n",
    "                        elif dt[\"hole_no\"] != dc[\"hole_no\"][idx + j]:\n",
    "                            prob[\"c_diff\"][j] = prob[\"c_diff\"][j] + 1\n",
    "\n",
    "                # incorrectスタート\n",
    "                for idx, dt in after_f_starts_task[task].iterrows():\n",
    "                    for j in range(1, min(forward_trace, len(dc) - idx)):\n",
    "                        #                    for j in range(1, min(forward_trace, len(self.data_cio) - idx)):\n",
    "                        if dt[\"hole_no\"] == dc[\"hole_no\"][idx + j]:\n",
    "                            prob[\"f_same\"][j] = prob[\"f_same\"][j] + 1\n",
    "                        elif dc[\"is_omission\"][idx + j] == 1:\n",
    "                            prob[\"f_omit\"][j] = prob[\"f_omit\"][j] + 1\n",
    "                        elif dt[\"hole_no\"] != dc[\"hole_no\"][idx + j]:\n",
    "                            prob[\"f_diff\"][j] = prob[\"f_diff\"][j] + 1\n",
    "\n",
    "                # calculate\n",
    "                prob[\"c_same\"] = prob[\"c_same\"] / after_c_all_task[task] if not after_c_all_task[task] == 0 else 0.0\n",
    "                prob[\"c_diff\"] = prob[\"c_diff\"] / after_c_all_task[task] if not after_c_all_task[task] == 0 else 0.0\n",
    "                prob[\"c_omit\"] = prob[\"c_omit\"] / after_c_all_task[task] if not after_c_all_task[task] == 0 else 0.0\n",
    "                prob[\"c_checksum\"] = prob[\"c_same\"] + prob[\"c_diff\"] + prob[\"c_omit\"]\n",
    "                prob[\"f_same\"] = prob[\"f_same\"] / after_f_all_task[task] if not after_f_all_task[task] == 0 else 0.0\n",
    "                prob[\"f_diff\"] = prob[\"f_diff\"] / after_f_all_task[task] if not after_f_all_task[task] == 0 else 0.0\n",
    "                prob[\"f_omit\"] = prob[\"f_omit\"] / after_f_all_task[task] if not after_f_all_task[task] == 0 else 0.0\n",
    "                prob[\"f_checksum\"] = prob[\"f_same\"] + prob[\"f_diff\"] + prob[\"f_omit\"]\n",
    "\n",
    "                task_prob[task] = prob\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "\n",
    "        # TODO 結構な確率でエラー吐く\n",
    "        def analyze_pattern(bit=self.bit):\n",
    "            fig_prob = {}\n",
    "            pattern_range = range(0, pow(2, bit))\n",
    "            for task in self.tasks:\n",
    "                pattern[task] = {}\n",
    "                fig_prob[task] = {\"fig1\": pd.DataFrame(columns=[\"{:b}\".format(i).zfill(bit) for i in pattern_range]\n",
    "                                                       ).fillna(0.0),\n",
    "                                  \"fig2\": pd.DataFrame(columns=[\"{:b}\".format(i).zfill(bit) for i in pattern_range]\n",
    "                                                       ).fillna(0.0),\n",
    "                                  \"fig3\": pd.DataFrame(columns=[\"{:b}\".format(i).zfill(bit) for i in pattern_range],\n",
    "                                                       ).fillna(0.0)}\n",
    "                data = self.data[\n",
    "                    (self.data.task == task) & (\n",
    "                        self.data.event_type.isin([\"reward\", \"failure\", \"time over\"]))].reset_index(drop=True)\n",
    "                data_ci = data[data.event_type.isin([\"reward\", \"failure\"])].reset_index(drop=True)\n",
    "                # search pattern\n",
    "\n",
    "                f_pattern_matching = lambda x: sum([\n",
    "                    (not np.isnan(data_ci.at[x + (bit - i - 1), \"is_correct\"])) * pow(2, i)\n",
    "                    for i in range(0, bit)])\n",
    "                pattern[task] = data_ci[:-(bit - 1)].assign(pattern=data_ci[:-(bit - 1)].index.map(f_pattern_matching))\n",
    "                # count\n",
    "\n",
    "                f_same_base = lambda x: [data_ci.at[data_ci[data_ci.session_id == x].index[0], \"hole_no\"] == \\\n",
    "                                         data_ci.at[data_ci[data_ci.session_id == x].index[0] + idx, \"hole_no\"] for idx\n",
    "                                         in range(1, bit)]\n",
    "                f_same_prev = lambda x: [data_ci.at[data_ci[data_ci.session_id == x].index[0] + idx - 1, \"hole_no\"] == \\\n",
    "                                         data_ci.at[data_ci[data_ci.session_id == x].index[0] + idx, \"hole_no\"] for idx\n",
    "                                         in range(1, bit)]\n",
    "                f_omit = lambda x: [bool(data.at[data[data.session_id == x].index[0] + idx, \"is_omission\"]) for idx in\n",
    "                                    range(1, bit)]\n",
    "                functions = lambda x: [f_same_base(x), f_same_prev(x), f_omit(x)]\n",
    "                # pattern count -> probability\n",
    "                for pat_tmp in pattern_range:\n",
    "                    f_p = pd.DataFrame(list(pattern[task][pattern[task].pattern == pat_tmp].session_id.map(functions)),\n",
    "                                       columns=[\"fig1\", \"fig2\", \"fig3\"]).fillna(0.0)\n",
    "                    if len(f_p):\n",
    "                        fig_prob[task][\"fig1\"][\"{:b}\".format(pat_tmp).zfill(bit)] = pd.DataFrame(\n",
    "                            list(f_p.fig1)).sum().fillna(0.0) / len(pattern[task][pattern[task].pattern == pat_tmp])\n",
    "                        fig_prob[task][\"fig2\"][\"{:b}\".format(pat_tmp).zfill(bit)] = pd.DataFrame(\n",
    "                            list(f_p.fig2)).sum().fillna(0.0) / len(pattern[task][pattern[task].pattern == pat_tmp])\n",
    "                        fig_prob[task][\"fig3\"][\"{:b}\".format(pat_tmp).zfill(bit)] = pd.DataFrame(\n",
    "                            list(f_p.fig3)).sum().fillna(0.0) / len(pattern[task][pattern[task].pattern == pat_tmp])\n",
    "                    else:\n",
    "                        for figure in list(f_p.columns):\n",
    "                            fig_prob[task][figure][\"{:b}\".format(pat_tmp).zfill(bit)] = fig_prob[task][figure][\n",
    "                                \"{:b}\".format(pat_tmp).zfill(bit)].fillna(0.0)\n",
    "                    for figure in list(f_p.columns):\n",
    "                        fig_prob[task][figure].at[\"n\", \"{:b}\".format(pat_tmp).zfill(bit)] = len(\n",
    "                            pattern[task][pattern[task].pattern == pat_tmp])\n",
    "            # save\n",
    "            self.fig_prob_tmp = fig_prob\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "            return pattern\n",
    "\n",
    "        def burst():\n",
    "            def calc_burst(session):\n",
    "                if session == 0:\n",
    "                    self.burst_id = 0\n",
    "                    return self.burst_id\n",
    "                if data.at[data.index[data.session_id == session][0], \"timestamps\"] - \\\n",
    "                        data.at[data.index[data.session_id == session - 1][0], \"timestamps\"] >= timedelta(\n",
    "                    seconds=60):\n",
    "                    self.burst_id = self.burst_id + 1\n",
    "                return self.burst_id\n",
    "\n",
    "            data = self.data[self.data.event_type.isin([\"reward\", \"failure\", \"time over\"])]\n",
    "            self.data = self.data.merge(\n",
    "                pd.DataFrame({\"session_id\": self.data.session_id.unique(),\n",
    "                              \"burst\": list(map(calc_burst, self.data.session_id.unique()))}),\n",
    "                on=\"session_id\", how=\"left\")\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "\n",
    "        def entropy_analyzing(section=10, bit=self.bit):\n",
    "            data = self.data[(self.data.event_type.isin([\"reward\", \"failure\"])) & (self.data.task.isin(self.tasks))]\n",
    "            entropy_df = data[\n",
    "                [\"session_id\", \"task\", \"entropy_{}\".format(section), \"entropy_after_{}\".format(section), \"pattern\"]]\n",
    "            count_correct = lambda pat: np.nan if np.isnan(pat) else \"{:b}\".format(int(pat)).zfill(bit).count(\"1\")\n",
    "            entropy_df[\"correctnum_{}bit\".format(bit)] = list(map(count_correct, entropy_df.pattern))\n",
    "            # entropy_df[\"mouse_no\"] = self.mouse_no\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "            return entropy_df\n",
    "\n",
    "        # main\n",
    "        header = [\"timestamps\", \"task\", \"session_id\", \"correct_times\", \"event_type\", \"hole_no\"]\n",
    "        pattern = {}\n",
    "        task_prob = {}\n",
    "        self.data = rehash_session_id()\n",
    "        self.data = add_hot_vector()\n",
    "        self.data_ci = self.data\n",
    "#         self.data.loc[\n",
    "#             self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"hole_choice_entropy\"] = calc_entropy()\n",
    "        # ent_section = 10\n",
    "        # self.data.loc[\n",
    "        #     self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"entropy_10\"] = calc_entropy(ent_section)\n",
    "        # self.data.loc[\n",
    "        #     self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"entropy_after_10\"] = \\\n",
    "        #     self.data.loc[self.data.index[self.data.event_type.isin(\n",
    "        #         ['reward', 'failure'])], \"entropy_10\"][(ent_section + self.bit - 1):].to_list() + \\\n",
    "        #     ([np.nan] * (ent_section + self.bit - 1))\n",
    "#         ent_section3 = 150\n",
    "#         self.data.loc[\n",
    "#             self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"entropy_{}\".format(\n",
    "#                 ent_section3)] = calc_entropy(ent_section3)\n",
    "#         ent_section2 = 300\n",
    "#         self.data.loc[\n",
    "#             self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"entropy_{}\".format(\n",
    "#                 ent_section2)] = calc_entropy(ent_section2)\n",
    "#         ent_section = 50\n",
    "#         self.data.loc[\n",
    "#             self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"entropy_{}\".format(\n",
    "#                 ent_section)] = calc_entropy(ent_section)\n",
    "#         self.data.loc[\n",
    "#             self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"entropy_after_{}\".format(\n",
    "#                 ent_section)] = \\\n",
    "#             self.data.loc[self.data.index[self.data.event_type.isin(\n",
    "#                 ['reward', 'failure'])], \"entropy_{}\".format(ent_section)][(ent_section + self.bit - 1):].tolist() + \\\n",
    "#             ([np.nan] * (ent_section + self.bit - 1))\n",
    "#         self.delta = add_timedelta()\n",
    "        self.data_not_omission = self.data[\n",
    "            ~self.data.session_id.isin(self.data.session_id[self.data.event_type.isin([\"time over\"])])]\n",
    "\n",
    "        # action Probability\n",
    "        after_c_all = float(len(self.data[self.data[\"is_correct\"] == 1]))\n",
    "        after_f_all = float(len(self.data[self.data[\"is_failure\"] == 1]))\n",
    "        after_c_starts = self.data[self.data[\"is_correct\"] == 1]\n",
    "        after_f_starts = self.data[self.data[\"is_failure\"] == 1]\n",
    "        after_c_all_task = {}\n",
    "        after_f_all_task = {}\n",
    "        after_c_starts_task = {}\n",
    "        after_f_starts_task = {}\n",
    "        for task in self.tasks:\n",
    "            after_c_starts_task[task] = self.data[(self.data[\"is_correct\"] == 1) & (self.data[\"task\"] == task)]\n",
    "            after_f_starts_task[task] = self.data[(self.data[\"is_failure\"] == 1) & (self.data[\"task\"] == task)]\n",
    "            after_c_all_task[task] = float(len(after_c_starts_task[task]))\n",
    "            after_f_all_task[task] = float(len(after_f_starts_task[task]))\n",
    "\n",
    "        # after_o_all = len(data[data[\"event_type\"] == \"time over\"])\n",
    "        forward_trace = 5\n",
    "        prob_index = [\"c_same\", \"c_diff\", \"c_omit\", \"c_checksum\", \"f_same\", \"f_diff\", \"f_omit\", \"f_checksum\",\n",
    "                      \"c_NotMax\",\n",
    "                      \"f_NotMax\", \"o_NotMax\"]\n",
    "        probability = pd.DataFrame(columns=prob_index, index=range(1, forward_trace + 1)).fillna(0.0)\n",
    "\n",
    "        #        count_all()\n",
    "        count_task()\n",
    "        # bit analyze\n",
    "#         pp = analyze_pattern(self.bit)\n",
    "#         pp = pd.concat([pp[task].loc[:, pp[task].columns.isin([\"session_id\", \"pattern\"])] for task in self.tasks])\n",
    "#         self.data = pd.merge(self.data, pp, how='left')\n",
    "#         # 2 bit analyze\n",
    "#         pp = analyze_pattern(2)\n",
    "#         pp = pd.concat([pp[task].loc[:, pp[task].columns.isin([\"session_id\", \"pattern\"])] for task in self.tasks])\n",
    "#         pp = pp.rename(columns={\"pattern\": \"pattern_2bit\"})\n",
    "#         self.data = pd.merge(self.data, pp, how='left')\n",
    "#         burst()\n",
    "        # entropy analyzing\n",
    "#         self.entropy_analyze = entropy_analyzing(section=50)\n",
    "        # self.entropy_analyze.concat(entropy_analyzing(section=50))\n",
    "        return self.data, probability, task_prob, self.delta, self.fig_prob_tmp, pattern, self.entropy_analyze\n",
    "\n",
    "    def dev_read_data(self, mouse_no):\n",
    "        task_prob = {}\n",
    "        delta = {}\n",
    "        fig_prob = {}\n",
    "        pattern_prob = {}\n",
    "        data = pd.read_csv(os.path.join(self.logpath, 'data/no{:03d}_{}_data.csv'.format(mouse_no, \"all\")))\n",
    "        probability = pd.read_csv(os.path.join(self.logpath, 'data/no{:03d}_{}_prob.csv'.format(mouse_no, \"all\")))\n",
    "\n",
    "        for task in self.tasks:\n",
    "            delta[task] = pd.read_csv(os.path.join(self.logpath, 'data/no{:03d}_{}_time.csv'.format(mouse_no, task)))\n",
    "            task_prob[task] = pd.read_csv(\n",
    "                os.path.join(self.logpath, 'data/no{:03d}_{}_prob.csv'.format(mouse_no, task)))\n",
    "            fig_prob[task] = {}\n",
    "            for fig_num in [\"fig1\", \"fig2\", \"fig3\"]:\n",
    "                fig_prob[task][fig_num] = pd.read_csv(\n",
    "                    os.path.join(self.logpath, 'data/no{:03d}_{}_{}_prob_fig.csv'.format(mouse_no, task, fig_num)),\n",
    "                    index_col=0)\n",
    "            pattern_prob[task] = pd.read_csv(\n",
    "                os.path.join(self.logpath, 'data/no{:03d}_{}_pattern.csv'.format(mouse_no, task)))\n",
    "        return data, probability, task_prob, delta, fig_prob, pattern_prob\n",
    "\n",
    "    def export_csv(self, mouse_no=None):\n",
    "        self.mice_task.to_csv(os.path.join(self.logpath, 'data/{}_data.csv'.format(\"all\")))\n",
    "#         self.probability.to_csv(\n",
    "#             os.path.join(self.logpath, 'data/{}_prob.csv'.format(\"all\")))\n",
    "        for task in self.tasks:\n",
    "#             self.mice_delta[task].to_csv(os.path.join(self.logpath, 'data/{}_time.csv'.format(task)))\n",
    "            # AttributeError: 'Series' object has no attribute 'type'\n",
    "#             reward_latency_data = self.mice_delta[task][self.mice_delta[task].type == \"reward_latency\"]\n",
    "#             reward_latency_data.to_csv(os.path.join(self.logpath, 'data/{}_rewardlatency.csv'.format(task)))\n",
    "            self.task_prob[task].to_csv(os.path.join(self.logpath, 'data/{}_prob.csv'.format(task)))\n",
    "#             self.pattern_prob[task].to_csv(os.path.join(self.logpath, 'data/{}_pattern.csv'.format(task)))\n",
    "#             [self.fig_prob[task][fig_num].to_csv(\n",
    "#                 os.path.join(self.logpath, 'data/prob_fig{}_{}.csv'.format(fig_num, task))) for\n",
    "#                 fig_num in [\"fig1\", \"fig2\", \"fig3\"]]\n",
    "            # pattern\n",
    "            # [self.entropy_analyze[\n",
    "            #      (self.entropy_analyze[\"correctnum_{}bit\".format(10,self.bit)] == count) &\n",
    "            #      (self.entropy_analyze[\"task\"] == task)  # & (\n",
    "            #      # self.entropy_analyze[\"mouse_no\"] == mouse_no)\n",
    "            #      ][10:-10].to_csv(\n",
    "            #     '{}data/pattern_entropy/summary/no{:03d}_{}_entropy_pattern_count_{}_summary.csv'.format(\n",
    "            #         self.logpath, mouse_no, task, int(count))) for count in\n",
    "            #     self.entropy_analyze[\"correctnum_{}bit\".format(10,self.bit)][\n",
    "            #         ~np.isnan(self.entropy_analyze[\"correctnum_{}bit\".format(10,self.bit)])].unique()]\n",
    "            # [self.entropy_analyze[\n",
    "            #      (self.entropy_analyze[\"pattern\"] == pattern) &\n",
    "            #      (self.entropy_analyze[\"task\"] == task)  # & (\n",
    "            #      # self.entropy_analyze[\"mouse_no\"] == mouse_no)\n",
    "            #      ][10:-10].to_csv(\n",
    "            #     '{}data/pattern_entropy/no{:03d}_{}_entropy_pattern_{:04b}.csv'.format(\n",
    "            #         self.logpath, mouse_no, task, int(pattern))) for\n",
    "            #     pattern in self.data.pattern[~np.isnan(self.data.pattern)].unique()]\n",
    "#             [self.mice_entropy[(self.mice_entropy[\"task\"] == task)  # & (\n",
    "#                  # self.entropy_analyze[\"mouse_no\"] == mouse_no)\n",
    "#              ][50:-50][(self.mice_entropy[\"correctnum_{}bit\".format(self.bit)] == count)].to_csv(\n",
    "#                 '{}data/pattern_entropy/summary/{}_entropy_pattern{:d}_count_{}_summary.csv'.format(\n",
    "#                     self.logpath, task, 50, int(count))) for count in\n",
    "#                 # self.entropy_analyze[\"correctnum_{}bit\".format(self.bit)][\n",
    "#                 # ~np.isnan(self.entropy_analyze[\"correctnum_{}bit\".format(self.bit)])].unique()]\n",
    "#                 range(0, self.bit)]\n",
    "#             [self.mice_entropy[\n",
    "#                  (self.mice_entropy[\"task\"] == task)  # & (\n",
    "#                  # self.entropy_analyze[\"mouse_no\"] == mouse_no)\n",
    "#              ][50:-50][(self.mice_entropy[\"pattern\"] == pattern)].to_csv(\n",
    "#                 '{}/data/pattern_entropy/{}_entropy{:d}_pattern_{:04b}.csv'.format(\n",
    "#                     self.logpath, task, 50, int(pattern))) for\n",
    "#                 pattern in self.data.pattern[~np.isnan(self.data.pattern)].unique()]\n",
    "\n",
    "        print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "\n",
    "\n",
    "def export_onehole_csv(tdata, mice, tasks):\n",
    "    df = dict(zip(tasks, [pd.DataFrame() for _ in tasks]))\n",
    "    for task in tasks:\n",
    "        for mouse_id in mice:\n",
    "            data_timedelta = tdata.mice_delta[task][(tdata.mice_delta[task].mouse_id.isin([mouse_id]))]\n",
    "            data_action = tdata.mice_task[\n",
    "                (tdata.mice_task.task.isin([task])) & (tdata.mice_task.mouse_id.isin([mouse_id]))]\n",
    "            tmp_df = data_action.merge(\n",
    "                data_timedelta[(data_timedelta.type.isin([\"reward_latency\"]))][\n",
    "                    [\"reward_latency_sec\", \"session_id\"]],\n",
    "                on=\"session_id\", how='left')\n",
    "            tmp_df = tmp_df.merge(\n",
    "                data_timedelta[(data_timedelta.type.isin([\"reaction_time\"]))].drop(\n",
    "                    columns=\"reward_latency_sec\"),\n",
    "                on=\"session_id\", how='left')\n",
    "            tmp_df.loc[\n",
    "                tmp_df.index[tmp_df.event_type.isin([\"failure\", \"omission\"])].to_list(), \"reaction_time_sec\"] = \\\n",
    "                data_timedelta.reaction_time_sec[\n",
    "                    (data_timedelta.type.isin([\"reaction_time\"])) & (~data_timedelta.session_id.isin(\n",
    "                        (data_timedelta.session_id[\n",
    "                             data_timedelta.type.isin([\"reward_latency\"])].drop_duplicates().to_list())))].to_list()\n",
    "\n",
    "            # nosepoke after reward\n",
    "\n",
    "            def check_poke_after(correct_num):\n",
    "                data = data_action[data_action.cumsum_correct_taskreset.isin([correct_num])]\n",
    "                return bool(sum(data.event_type.isin([\"nose poke after rew\"])))\n",
    "\n",
    "            poke_after = list(map(check_poke_after, data_action[\n",
    "                data_action.event_type.isin([\"reward\", \"failure\", \"omission\"])].cumsum_correct_taskreset.unique()))\n",
    "            tmp_df = tmp_df.assign(poke_after=np.nan)[tmp_df.event_type.isin([\"reward\", \"failure\", \"time over\"])]\n",
    "            tmp_df.loc[tmp_df.index[tmp_df.event_type.isin([\"reward\"])], \"poke_after\"] = poke_after\n",
    "            tmp_df = tmp_df[\n",
    "                [\"timestamps\", \"task\", \"event_type\", \"reaction_time_sec\", \"reward_latency_sec\", \"poke_after\"]]\n",
    "            df[task] = pd.concat([df[task], tmp_df])\n",
    "        df[task].to_csv(os.path.join(\"data\", \"{}_task-{}_1holedata.csv\".format(\"all\", task)), index=False)\n",
    "\n",
    "\n",
    "def view_averaged_prob_same_prev(tdata, mice, tasks):\n",
    "    m = []\n",
    "    t = []\n",
    "    csame = []\n",
    "    fsame = []\n",
    "\n",
    "    for mouse_id in mice:\n",
    "        for task in tasks:\n",
    "            m += [mouse_id]\n",
    "            t += [task]\n",
    "            csame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['c_same']]\n",
    "            fsame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['f_same']]\n",
    "\n",
    "    after_prob_df = pd.DataFrame(\n",
    "        data={'mouse_id': m, 'task': t, 'c_same': csame, 'f_same': fsame},\n",
    "        columns=['mouse_id', 'task', 'c_same', 'f_same']\n",
    "    )\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(8, 4), dpi=100)\n",
    "    for task in tasks:\n",
    "        plt.subplot(1, len(tasks), tasks.index(task) + 1)\n",
    "\n",
    "        c_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['c_same'].to_list())\n",
    "        c_same_avg = np.mean(c_same, axis=0)\n",
    "        c_same_std = np.std(c_same, axis=0)\n",
    "        c_same_var = np.var(c_same, axis=0)\n",
    "\n",
    "        f_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['f_same'].to_list())\n",
    "        f_same_avg = np.mean(f_same, axis=0)\n",
    "        f_same_var = np.var(f_same, axis=0)\n",
    "\n",
    "        xlen = len(c_same_avg)\n",
    "        xax = np.array(range(1, xlen + 1))\n",
    "        plt.plot(xax, c_same_avg, label=\"rewarded start\")\n",
    "        plt.errorbar(xax, c_same_avg, yerr=c_same_var, capsize=2, fmt='o', markersize=1, ecolor='black',\n",
    "                     markeredgecolor=\"black\", color='w', lolims=True)\n",
    "\n",
    "        plt.plot(np.array(range(1, xlen + 1)), f_same_avg, label=\"no-rewarded start\")\n",
    "        plt.errorbar(xax, f_same_avg, yerr=f_same_var, capsize=2, fmt='o', markersize=1, ecolor='black',\n",
    "                     markeredgecolor=\"black\", color='w', uplims=True)\n",
    "\n",
    "        # plt.ion()\n",
    "        plt.xticks(np.arange(1, xlen + 1, 1))\n",
    "        plt.xlim(0.5, xlen + 0.5)\n",
    "        plt.ylim(0, 1.05)\n",
    "        if tasks.index(task) == 0:\n",
    "            plt.ylabel('P (same choice)')\n",
    "            plt.legend()\n",
    "        plt.xlabel('Trial')\n",
    "        plt.title('{}'.format(task))\n",
    "        \n",
    "    # plt.savefig('fig/{}_prob_all4.png'.format(graph_ins.exportpath))\n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "    plt.savefig('fig/prob_all4_{}.png'.format(tasks[0]))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def view_summary(tdata, mice, tasks, x=\"session_id\"):\n",
    "    for mouse_id in mice:\n",
    "        def plot(mdf, task=\"all\"):\n",
    "            labels = [\"failure\", \"correct\", \"omission\"]\n",
    "            df = mdf[mdf[\"event_type\"].isin([\"reward\", \"failure\", \"time over\"])]\n",
    "\n",
    "            # past time\n",
    "            df.timestamps = (df.timestamps - df.iat[0, 0]).apply(lambda time: time.total_seconds())\n",
    "\n",
    "            # entropy\n",
    "            fig, ax = plt.subplots(4, 1, sharex=\"all\", figsize=(15, 8), dpi=100)\n",
    "            fig.suptitle('no{:03} summary {}'.format(mouse_id, task), y=1.0)\n",
    "            plt.subplots_adjust(hspace=0, bottom=0)\n",
    "\n",
    "            ax[0].plot(df[df.event_type.isin([\"reward\", \"failure\"])][x],\n",
    "                       df[df.event_type.isin([\"reward\", \"failure\"])]['hole_choice_entropy'])\n",
    "            ax[0].set_ylabel('Entropy (bit)')\n",
    "            ax[0].set_xlim(df[x].min(), df[x].max())\n",
    "            if task == \"all\":\n",
    "                collection = collections.BrokenBarHCollection.span_where(df[x].to_numpy(), ymin=-100, ymax=100,\n",
    "                                                                         where=(df.task.isin(tasks[0::2])),\n",
    "                                                                         facecolor='lightblue', alpha=0.3)\n",
    "                ax[0].add_collection(collection)\n",
    "\n",
    "            # scatter\n",
    "            colors = [\"red\", \"blue\", \"black\"]\n",
    "            size = dict(zip(labels, [25, 50, 25]))\n",
    "            pos = dict(zip(labels, [\"bottom\", \"full\", \"bottom\"]))\n",
    "            datasets = ([df[df[\"is_{}\".format(flag)] == 1] for flag in labels])\n",
    "            leg = []\n",
    "            for dt, la, cl in zip(datasets, labels, colors):\n",
    "                marker = markers.MarkerStyle(\"|\", pos[la])\n",
    "                leg.append(ax[1].scatter(dt[x], dt['is_hole1'] * 1, s=size[la], color=cl, marker=marker, label=la))\n",
    "                ax[1].scatter(dt[x], dt['is_hole3'] * 2, s=size[la], color=cl, marker=marker)\n",
    "                ax[1].scatter(dt[x], dt['is_hole5'] * 3, s=size[la], color=cl, marker=marker)\n",
    "                ax[1].scatter(dt[x], dt['is_hole7'] * 4, s=size[la], color=cl, marker=marker)\n",
    "                ax[1].scatter(dt[x], dt['is_hole9'] * 5, s=size[la], color=cl, marker=marker)\n",
    "                ax[1].scatter(dt[x], dt['is_omission'] * 0, s=size[la], color=cl, marker=marker)\n",
    "            ax[1].set_ylabel(\"Hole\")\n",
    "            ax[1].set_yticks([1, 2, 3, 4, 5])\n",
    "            ax[1].legend()\n",
    "            if task == \"all\":\n",
    "                collection = collections.BrokenBarHCollection.span_where(df[x].to_numpy(), ymin=-2, ymax=6,\n",
    "                                                                         where=(df.task.isin(tasks[0::2])),\n",
    "                                                                         facecolor='lightblue', alpha=0.3)\n",
    "                ax[1].add_collection(collection)\n",
    "\n",
    "            # cumsum\n",
    "            ax[2].plot(df[x], df['cumsum_correct_taskreset'])\n",
    "            ax[2].plot(df[x], df['cumsum_incorrect_taskreset'])\n",
    "            ax[2].plot(df[x], df['cumsum_omission_taskreset'])\n",
    "            ax[2].set_ylabel('Cumulative')\n",
    "            # ax[2].set_xlabel('Trial')\n",
    "            ax[2].legend([\"correct\", \"incorrect\", \"omission\"])\n",
    "            if task == \"all\":\n",
    "                collection = collections.BrokenBarHCollection.span_where(df[x].to_numpy(), ymin=-20, ymax=1000,\n",
    "                                                                         where=(df.task.isin(tasks[0::2])),\n",
    "                                                                         facecolor='lightblue', alpha=0.3)\n",
    "                ax[2].add_collection(collection)\n",
    "\n",
    "            # 100 step move average\n",
    "            # make dataframe\n",
    "            df_o = df[df[\"event_type\"].isin([\"reward\", \"failure\"])]\n",
    "            # data = pd.DataFrame(columns=[\"is_hole{}\".format(i) for i in range(1, 10, 2)])\n",
    "            data_tmp = pd.DataFrame()\n",
    "            add_average = lambda idx: [df_o[max(0, idx - 100):idx][\"is_hole{}\".format(i)].sum() /\n",
    "                                       max(df_o[max(0, idx - 100):idx][\"is_hole{}\".format(i)].size, 1) for i in\n",
    "                                       range(1, 10, 2)]\n",
    "            data_tmp = data_tmp.append(list(map(add_average, list(range(0, len(df_o))))), ignore_index=True)\n",
    "            # plot\n",
    "            ax[3].plot(df_o[x], data_tmp)\n",
    "            ax[3].set_ylabel(\"moving average action rate\")\n",
    "            # legend\n",
    "            ax[3].legend([\"hole{}\".format(i) for i in range(1, 10, 2)])\n",
    "            if task == \"all\":\n",
    "                collection = collections.BrokenBarHCollection.span_where(df_o[x].to_numpy(), ymin=-20,\n",
    "                                                                         ymax=1000,\n",
    "                                                                         where=(df_o.task.isin(tasks[0::2])),\n",
    "                                                                         facecolor='lightblue', alpha=0.3)\n",
    "                ax[3].add_collection(collection)\n",
    "            # savefig\n",
    "            fig.savefig('fig/no{:03d}_{}_summary_{}.png'.format(mouse_id, task, x))\n",
    "            fig.show()\n",
    "\n",
    "        data = tdata.mice_task[tdata.mice_task.mouse_id == mouse_id]\n",
    "        plot(data)\n",
    "        list(map(plot, [data[data.task == task] for task in tdata.tasks], tdata.tasks))\n",
    "\n",
    "\n",
    "def view_trial_per_datetime(tdata, mice=[18], task=\"All5_30\"):\n",
    "    \"\"\" for debug \"\"\"\n",
    "    # for mouse_no in mice:\n",
    "    data = tdata.data[\n",
    "        (tdata.data.event_type.isin([\"reward\", \"failure\", \"time over\"]))\n",
    "        & (tdata.data.task == task)\n",
    "        # &(tdata.data.mouse_id == mouse_no)\n",
    "        ].set_index(\"timestamps\").resample(\"1H\").sum()\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "    data.plot.bar(y=[\"is_correct\", \"is_incorrect\", \"is_omission\"], stacked=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def view_scatter_vs_times_with_burst(tdata, mice=[18], task=\"All5_30\", burst=1):\n",
    "    \"\"\" fig1 B \"\"\"\n",
    "    for mouse_id in mice:\n",
    "\n",
    "        labels = [\"correct\", \"incorrect\", \"omission\"]\n",
    "\n",
    "        data = tdata.data.assign(\n",
    "            timestamps=(tdata.data.timestamps - tdata.data.timestamps[0]).dt.total_seconds())  # [mouse_id]\n",
    "        data = data[data[\"event_type\"].isin([\"reward\", \"failure\", \"time over\"])]\n",
    "        # data = data[data.burst.isin(data.burst.unique()[data.groupby(\"burst\").burst.count() > burst])]\n",
    "        burst_time = list(data.burst.unique()[data.groupby(\"burst\").burst.count() > burst])\n",
    "        fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "        fig_subplot = fig.add_subplot(1, 1, 1)\n",
    "        # plt.title('{:03} summary'.format(mouse_id))\n",
    "        #    nose_poke_raster(mouse_id, fig.add_subplot(3, 1, 2))\n",
    "\n",
    "        colors = [\"blue\", \"red\", \"black\"]\n",
    "        for single_burst in burst_time:\n",
    "            d = data[data.burst == single_burst]\n",
    "            datasets = [(d[d[\"is_{}\".format(flag)] == 1]) for flag in labels]\n",
    "            for dt, la, cl in zip(datasets, labels, colors):\n",
    "                plt.scatter(dt.timestamps, dt['is_hole1'] * 1, s=15, c=cl)\n",
    "                plt.scatter(dt.timestamps, dt['is_hole3'] * 2, s=15, c=cl)\n",
    "                plt.scatter(dt.timestamps, dt['is_hole5'] * 3, s=15, c=cl)\n",
    "                plt.scatter(dt.timestamps, dt['is_hole7'] * 4, s=15, c=cl)\n",
    "                plt.scatter(dt.timestamps, dt['is_hole9'] * 5, s=15, c=cl)\n",
    "                plt.scatter(dt.timestamps, dt['is_omission'] * 0, s=15, c=cl)\n",
    "            plt.ylabel(\"Hole\")\n",
    "            plt.xlim(d.timestamps.min() - 30, d.timestamps.max() + 30)\n",
    "            plt.ylim(0, 5)\n",
    "            #    plt.xlim(0, len(mdf))\n",
    "\n",
    "            collection = collections.BrokenBarHCollection.span_where(data.timestamps.to_numpy(), ymin=0, ymax=5,\n",
    "                                                                     where=(data.burst.isin(burst_time)),\n",
    "                                                                     facecolor='pink', alpha=0.3)\n",
    "            fig_subplot.add_collection(collection)\n",
    "            # save\n",
    "            # plt.show()\n",
    "            burst_len = d.timestamps.count()\n",
    "            if not os.path.isdir(os.path.join(os.getcwd(), \"fig\", \"burst\", \"len\" + str(burst_len))):\n",
    "                os.mkdir(os.path.join(os.getcwd(), \"fig\", \"burst\", \"len\" + str(burst_len)))\n",
    "            plt.savefig(os.path.join(os.getcwd(), 'fig', 'burst', \"len\" + str(burst_len),\n",
    "                                     'no{:03d}_burst{}_hole_pasttime_burst.png'.format(mouse_id, single_burst)))\n",
    "\n",
    "\n",
    "def view_trial_per_time(tdata, mice=[18], task=\"All5_30\"):\n",
    "    \"\"\" fig1 C \"\"\"\n",
    "    data = tdata.data[\n",
    "        (tdata.data.event_type.isin([\"reward\", \"failure\", \"time over\"])) &\n",
    "        (tdata.data.task == task)\n",
    "        ].set_index(\"timestamps\").resample(\"1H\").sum()\n",
    "    data = data.set_index(data.index.time).groupby(level=0).mean()\n",
    "    fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "    data.plot.bar(y=[\"is_correct\", \"is_incorrect\", \"is_omission\"], stacked=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def view_prob_same_choice_burst(tdata, mice, tasks, burst=1):\n",
    "    \"\"\" fig4 \"\"\"\n",
    "    tdata_ci = tdata[tdata.event_type.isin([\"reward\", \"failure\"])]\n",
    "    tdata_ci = tdata_ci[\n",
    "        tdata_ci.burst.isin(tdata_ci.burst.unique()[tdata_ci.groupby(\"burst\").burst.count() > burst])].reset_index()\n",
    "\n",
    "    # burst_len limit なし\n",
    "    # after_prob_df = pd.concat([tdata.task_prob[task].assign(task=task) for task in tasks])\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig, ax = plt.subplots(1, len(tasks), sharey=\"all\", sharex=\"all\", figsize=(8, 4), dpi=100)\n",
    "    forward_trace = 7\n",
    "\n",
    "    def calc(mouse_id):\n",
    "        prob_index = [\"c_same\", \"f_same\", \"task\", \"mouse_id\"]\n",
    "        after_prob_df = pd.DataFrame(columns=prob_index)\n",
    "        lgnd = None\n",
    "        for task in tasks:\n",
    "            data_tmp = tdata_ci[(tdata_ci.task.isin([task])) & (tdata_ci.mouse_id == mouse_id)]  # .groupby(\"burst\")\n",
    "            \"burst ごと確率を出す\"\n",
    "            for bst in data_tmp.burst.unique():\n",
    "                data = data_tmp[data_tmp.burst.isin([bst])].reset_index(drop=True)\n",
    "                after_correct_all = data.burst[:-forward_trace][data.is_correct == 1].count()\n",
    "                after_incorrect_all = data.burst[:-forward_trace][data.is_incorrect == 1].count()\n",
    "                correct_index = data[:-forward_trace][data.is_correct == 1].index\n",
    "                incorrect_index = data[:-forward_trace][data.is_incorrect == 1].index\n",
    "                df = pd.DataFrame(columns=range(forward_trace))\n",
    "                same_correct = \\\n",
    "                    df.append([data[idx:idx + min(forward_trace, len(data))].hole_no == data.hole_no[idx] for idx in\n",
    "                               correct_index]).sum() if len(correct_index) else df.sum()\n",
    "                same_incorrect = \\\n",
    "                    df.append([data[idx:idx + min(forward_trace, len(data))].hole_no == data.hole_no[idx] for idx in\n",
    "                               incorrect_index]).sum() if len(incorrect_index) else df.sum()\n",
    "                after_prob_df = after_prob_df.append(pd.DataFrame({\"c_same\": same_correct / after_correct_all,\n",
    "                                                                   \"f_same\": same_incorrect / after_incorrect_all,\n",
    "                                                                   \"task\": task, \"mouse_id\": mouse_id,\n",
    "                                                                   \"burst\": bst}).fillna(0.0))\n",
    "\n",
    "                # after_prob_df = after_prob_df.append(pd.DataFrame({\"c_same\": (same_correct / after_correct_all).mean(),\n",
    "                #                                                    \"f_same\": (same_incorrect / after_incorrect_all).mean(),\n",
    "                #                                                    \"task\": task, \"mouse_id\": mouse_id,\n",
    "                #                                                    \"burst\": bst}).fillna(0.0), ignore_index=True)\n",
    "            \"\"\" burstごと確率 を平均する\"\"\"\n",
    "            c_same = after_prob_df[\n",
    "                (after_prob_df['task'].isin([task])) &\n",
    "                (after_prob_df[\"mouse_id\"] == mouse_id)\n",
    "                ]['c_same'].groupby(level=0)\n",
    "            c_same_avg = c_same.mean()[:forward_trace + 1]\n",
    "            c_same_var = c_same.var()[:forward_trace + 1]\n",
    "\n",
    "            f_same = after_prob_df[\n",
    "                (after_prob_df['task'].isin([task])) &\n",
    "                (after_prob_df[\"mouse_id\"] == mouse_id)\n",
    "                ]['f_same'].groupby(level=0)\n",
    "            f_same_avg = f_same.mean()[:forward_trace + 1]\n",
    "            f_same_var = f_same.var()[:forward_trace + 1]\n",
    "\n",
    "            # ここから描画\n",
    "            xlen = c_same_avg.size\n",
    "            # xax = np.array(range(1, xlen + 1))\n",
    "            xax = np.array(range(forward_trace + 1))\n",
    "            ax.plot(xax, c_same_avg, color=\"orange\", label=\"rewarded start\")\n",
    "            ax.errorbar(xax, c_same_avg, yerr=c_same_var, capsize=2, fmt='o', markersize=1,\n",
    "                        ecolor='black',\n",
    "                        markeredgecolor=\"black\", color='w', lolims=True)\n",
    "\n",
    "            ax.plot(xax, f_same_avg, color=\"blue\", label=\"no-rewarded start\")\n",
    "            ax.errorbar(xax, f_same_avg, yerr=f_same_var, capsize=2, fmt='o', markersize=1,\n",
    "                        ecolor='black',\n",
    "                        markeredgecolor=\"black\", color='w', uplims=True)\n",
    "\n",
    "            # plt.ion()\n",
    "            ax.set_xticks(xax)\n",
    "            ax.set_xlim(-0.5, xlen + 0.5)\n",
    "            ax.set_ylim(0, 1.05)\n",
    "            if tasks.index(task) == 0:\n",
    "                ax.set_ylabel('P (same choice)')\n",
    "            if tasks.index(task) == int(len(tasks) / 2):\n",
    "                lgnd = ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.05), ncol=2,\n",
    "                                 mode=\"expand\")\n",
    "            if tasks.index(task) in [0, len(tasks) - 1]:\n",
    "                ax.set_xlabel('Trial')\n",
    "            ax.set_title('{}'.format(task))\n",
    "        # label\n",
    "        # plt.legend()\n",
    "        lgnd.get_frame().set_linewidth(0.0)\n",
    "        plt.savefig(\"no{:03d}_prob4.png\".format(mouse_id))\n",
    "        plt.show()\n",
    "\n",
    "    list(map(calc, mice))\n",
    "\n",
    "\n",
    "def view_sigletask_prob(tdata, mice, task):\n",
    "    \"\"\" fig5 A \"\"\"\n",
    "    tdata_ci = tdata.mice_task[tdata.mice_task.event_type.isin([\"reward\", \"failure\"])]\n",
    "    tdata_ci = tdata_ci[tdata_ci.task.isin([task])].reset_index(drop=True)\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig, ax = plt.subplots(1, 2, sharey=\"all\", sharex=\"all\", figsize=(8, 4), dpi=100)\n",
    "    forward_trace = 7\n",
    "    range_lim = 50\n",
    "\n",
    "    def calc(mouse_id):\n",
    "        prob_index = [\"c_same\", \"f_same\", \"task\", \"mouse_id\"]\n",
    "        data_tmp = tdata_ci[(tdata_ci.mouse_id == mouse_id)].reset_index(drop=True)\n",
    "\n",
    "        for data, index in zip([data_tmp[:range_lim], data_tmp[-range_lim:]], [0, 1]):\n",
    "            \"確率を出す\"\n",
    "            after_prob_df = pd.DataFrame(columns=prob_index)\n",
    "            data = data.reset_index(drop=True)\n",
    "            after_correct_all = data.is_correct[data.is_correct == 1].count()\n",
    "            after_incorrect_all = data.is_incorrect[data.is_incorrect == 1].count()\n",
    "            correct_index = data[data.is_correct == 1].index\n",
    "            incorrect_index = data[data.is_incorrect == 1].index\n",
    "            df = pd.DataFrame(columns=range(forward_trace))\n",
    "            same_correct = df.append(\n",
    "                [(data[idx + 1:idx + min(forward_trace + 1, len(data[idx + 1:]))].hole_no == data.hole_no[\n",
    "                    idx]).reset_index(drop=True).T for idx in correct_index]) * 1 if len(correct_index) else df\n",
    "            same_incorrect = df.append(\n",
    "                [(data[idx + 1:idx + min(forward_trace + 1, len(data[idx + 1:]))].hole_no == data.hole_no[\n",
    "                    idx]).reset_index(drop=True).T for idx in incorrect_index]) * 1 if len(incorrect_index) else df\n",
    "            same_correct.columns = same_correct.columns + 1\n",
    "            same_incorrect.columns = same_incorrect.columns + 1\n",
    "            after_prob_df = after_prob_df.append(pd.DataFrame({\"c_same\": same_correct.sum() / after_correct_all,\n",
    "                                                               \"f_same\": same_incorrect.sum() / after_incorrect_all,\n",
    "                                                               \"task\": task, \"mouse_id\": mouse_id}).fillna(0.0))\n",
    "\n",
    "            c_same = after_prob_df[\n",
    "                (after_prob_df['task'].isin([task])) &\n",
    "                (after_prob_df[\"mouse_id\"] == mouse_id)\n",
    "                ]['c_same']\n",
    "\n",
    "            f_same = after_prob_df[\n",
    "                (after_prob_df['task'].isin([task])) &\n",
    "                (after_prob_df[\"mouse_id\"] == mouse_id)\n",
    "                ]['f_same']\n",
    "\n",
    "            # ここから描画\n",
    "            xlen = c_same.size\n",
    "            xax = np.array(range(1, forward_trace + 1))\n",
    "            ax[index].plot(c_same, color=\"orange\", label=\"rewarded start\")\n",
    "            ax[index].plot(f_same, color=\"skyblue\", label=\"no-rewarded start\")\n",
    "            ax[index].set_xticks(xax)\n",
    "            ax[index].set_xlim(0.5, xlen + 0.5)\n",
    "            ax[index].set_ylim(0, 1.05)\n",
    "            ax[index].set_xlabel('Trial')\n",
    "        # label\n",
    "        ax[0].set_ylabel('P (same choice)')\n",
    "        ax[0].set_title('no{:03d}_{}_first{}step'.format(mouse_id, task, range_lim))\n",
    "        ax[1].set_title('no{:03d}_{}_last{}step'.format(mouse_id, task, range_lim))\n",
    "        plt.subplots_adjust(top=0.8)\n",
    "        plt.legend()\n",
    "        plt.savefig(\"no{:03d}_prob5_{}.png\".format(mouse_id, task))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    list(map(calc, mice))\n",
    "\n",
    "\n",
    "def view_pattern_entropy_summary(tdata, mice, task=None):\n",
    "    data = tdata.mice_entropy\n",
    "    average_all = None\n",
    "    for mouse_id in mice:\n",
    "        data_tmp = data[mouse_id].groupby(\n",
    "            [\"task\", \"correctnum_{}bit\".format(tdata.bit)])\n",
    "        mean = data_tmp.mean().reset_index()\n",
    "        sd = data_tmp.std().reset_index()\n",
    "        data_tmp = pd.merge(mean, sd, on=[\"task\", \"correctnum_{}bit\".format(tdata.bit)], suffixes=[\"_mean\", \"_sd\"])\n",
    "        data_tmp = data_tmp.loc[:, data_tmp.columns.str.startswith((\"task\", \"correctnum\", \"entropy\"))].assign(\n",
    "            mouse_id=mouse_id)\n",
    "        average_all = data_tmp if isinstance(average_all, type(None)) else average_all.append(data_tmp)\n",
    "    for group_info, data_tmp in average_all.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]):\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        # error bar\n",
    "        # ax.errorbar(data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].columns.to_numpy().reshape(2),\n",
    "        #             data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].to_numpy(),\n",
    "        #             yerr=data_tmp.loc[:, data_tmp.columns.str.endswith(\"sd\")].to_numpy(),\n",
    "        #             ecolor=\"black\")\n",
    "        # ax.errorbar(data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].columns,\n",
    "        #             data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].to_numpy().reshape(2, )[1],\n",
    "        #             yerr=data_tmp.loc[:, data_tmp.columns.str.endswith(\"sd\")].to_numpy().reshape(2, )[1],\n",
    "        #             ecolor=\"black\")\n",
    "        ax.errorbar(data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].columns.to_numpy(),\n",
    "                    data_tmp.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]).mean().loc[:,\n",
    "                    data_tmp.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]).mean().columns.str.endswith(\n",
    "                        \"mean\")].to_numpy().T,\n",
    "                    yerr=np.mean(data_tmp.loc[:, data_tmp.columns.str.endswith(\"sd\")].to_numpy(), axis=0),\n",
    "                    ecolor=\"blue\")\n",
    "        # ax.errorbar(data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].columns[1],\n",
    "        #             data_tmp.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]).mean().loc[:,\n",
    "        #             data_tmp.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]).mean().columns.str.endswith(\n",
    "        #                 \"mean\")].to_numpy().reshape(2, )[1],\n",
    "        #             yerr=data_tmp.loc[:, data_tmp.columns.str.endswith(\"sd\")].to_numpy().reshape(2, )[1],\n",
    "        #             ecolor=\"black\")\n",
    "        # mean\n",
    "        ax.plot(data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].columns,\n",
    "                data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].to_numpy().T,\n",
    "                marker=\"o\", color=\"black\")\n",
    "        # all average\n",
    "        ax.plot(data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].columns,\n",
    "                data_tmp.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]).mean().loc[:,\n",
    "                data_tmp.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]).mean().columns.str.endswith(\n",
    "                    \"mean\")].to_numpy().T,\n",
    "                marker=\"x\", color=\"blue\")\n",
    "        # plt.show(block=True)\n",
    "        plt.savefig(os.path.join(os.getcwd(), 'fig', 'pattern_ent',\n",
    "                                 'pattern_ent_average_{}_correct{}.png'.format(group_info[0], group_info[1])))\n",
    "\n",
    "\n",
    "def export_2bit_analyze(tdata, mice, tasks, bit=2, burst_len=10):\n",
    "    \"\"\" task ごとにパターンの確率を算出して csv 出力 \"\"\"\n",
    "    pattern_range = range(pow(bit, 2))\n",
    "    tmp = pd.DataFrame(columns=[\"{:02b}\".format(i) for i in pattern_range]).fillna(0.0)\n",
    "    prob_all = dict(zip(tasks, [tmp.copy() for _ in range(len(tasks))]))\n",
    "    count_all = dict(zip(tasks,\n",
    "                         [pd.DataFrame(columns=[\"{:02b}\".format(pattern) for pattern in pattern_range]).copy() for _ in\n",
    "                          range(len(tasks))]))\n",
    "    # row_data = dict(\n",
    "    #     zip(tasks, [pd.DataFrame(columns=[\"{:02b}\".format(pattern) for pattern in pattern_range]) for _ in tasks]))\n",
    "    for mouse_no in mice:\n",
    "        data = tdata.mice_task[tdata.mice_task.mouse_id == mouse_no]\n",
    "        data_ci = data[data.event_type.isin([\"reward\", \"failure\"])].reset_index(drop=True)\n",
    "        f_same_prev = lambda x: data_ci.at[data_ci[data_ci.session_id == x].index[0] + 1, \"hole_no\"] == \\\n",
    "                                data_ci.at[data_ci[data_ci.session_id == x].index[0], \"hole_no\"]\n",
    "        functions = lambda x: f_same_prev(x)\n",
    "        data_bursts = data_ci[data_ci.burst.isin(\n",
    "            data_ci.burst.unique()[data_ci.groupby(\"burst\").burst.count() > burst_len])]\n",
    "        bit_prob = dict(zip(tasks, [dict(zip([\"f_same_prev\"], [tmp.copy()])) for _ in range(len(tasks))]))\n",
    "        for task in tasks:\n",
    "            data_tmp = data_bursts[(data_ci.task.isin([task]))]\n",
    "            tmp_df = []\n",
    "            tmp_count = []\n",
    "            for pat_tmp in pattern_range:\n",
    "                # pattern count -> probability\n",
    "                f_p = pd.DataFrame(list(data_tmp[data_tmp.pattern_2bit == pat_tmp].session_id[1:].map(functions)),\n",
    "                                   columns=[\"f_same_prev\"]).fillna(0.0)\n",
    "                # row_data[task][\"{:02b}\".format(pat_tmp)] = row_data[task][\"{:02b}\".format(pat_tmp)].append((f_p * 1))\n",
    "                # f_p.count().to_csv(os.path.join(\"data\", \"2bit\",\n",
    "                #                                 \"no{:03d}_{}_pat{:02b}_burst_2bit.csv\".format(mouse_no, task, pat_tmp)))\n",
    "                tmp_count.append(len(data_tmp[data_tmp.pattern_2bit == pat_tmp]))\n",
    "                if len(f_p):\n",
    "                    \"\"\" 一例以上あった場合確率として計算 \"\"\"\n",
    "                    # Series\n",
    "                    tmp_df.append((pd.DataFrame(list(f_p.f_same_prev)).sum().fillna(0.0) /\n",
    "                                   len(data_tmp[data_tmp.pattern_2bit == pat_tmp])).values[0])\n",
    "\n",
    "                else:\n",
    "                    \"\"\" 一回もパターンが出ていない場合 \"\"\"\n",
    "                    tmp_df.append(np.nan)\n",
    "            bit_prob[task][\"f_same_prev\"] = bit_prob[task][\"f_same_prev\"].append(\n",
    "                pd.Series(tmp_df, index=bit_prob[task][\"f_same_prev\"].columns), ignore_index=True)\n",
    "            prob_all[task] = prob_all[task].append(pd.Series(tmp_df, index=bit_prob[task][\"f_same_prev\"].columns),\n",
    "                                                   ignore_index=True)\n",
    "            count_all[task].loc[\"no{}\".format(mouse_no)] = tmp_count\n",
    "            # export\n",
    "            bit_prob[task][\"f_same_prev\"].to_csv(\n",
    "                os.path.join(\"data\", \"2bit_prob_task-{}_no{}.csv\".format(task, mouse_no)), index=False, header=False)\n",
    "            # graph\n",
    "            fig = bit_prob[task][\"f_same_prev\"].T.plot.line(title=\"2bit no{:03d} task:{}\".format(mouse_no, task),\n",
    "                                                            style=\"bo-\", ylim=(0.0, 1.0), ms=10)\n",
    "            plt.savefig(os.path.join(\"fig\", \"2bit\", \"no{:03d}_{}_2bit.png\".format(mouse_no, task)))\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "    for task in tasks:\n",
    "        fig = prob_all[task].mean().T.plot.line(title=\"2bit {} task:{}\".format(\"all\", task),\n",
    "                                                style=\"ro-\", ylim=(0.0, 1.0), ms=10)\n",
    "        plt.savefig(os.path.join(\"fig\", \"2bit\", \"{}_{}_2bit.png\".format(\"all\", task)))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        prob_all[task].to_csv(os.path.join(\"data\", \"2bit\", \"allmice_{}_burst_2bit_prob.csv\".format(task)), index=False)\n",
    "        count_all[task].to_csv(os.path.join(\"data\", \"2bit\", \"n_{}_burst_2bit_count.csv\".format(task)))\n",
    "        count_all[task].count()\n",
    "        # [row_data[task][\"{:02b}\".format(pat)].to_csv(\n",
    "        #     os.path.join(\"data\", \"2bit\", \"{}_{}_pat{:02b}_burst_2bit.csv\".format(\"allmice\", task, pat))) for pat in\n",
    "        #     pattern_range]\n",
    "        # return bit_prob\n",
    "\n",
    "\n",
    "def export_all_entropy(tdata, mice, tasks=[\"All5_90\", \"All5_30\", \"All5_30_drug\"]):\n",
    "    target_task = tasks\n",
    "    ret_val = pd.DataFrame()\n",
    "    datas = tdata if isinstance(tdata, list) else [tdata]\n",
    "\n",
    "    def min_max(x, axis=None):\n",
    "        np.array(x)\n",
    "        min = np.array(x).min(axis=axis)\n",
    "        max = np.array(x).max(axis=axis)\n",
    "        result = (x - min) / (max - min)\n",
    "        return result\n",
    "\n",
    "    for d in datas:\n",
    "        for mouse_id in mice:\n",
    "            tmp = [mouse_id]\n",
    "            for task in target_task:\n",
    "                data = d.mice_task[\n",
    "                    (d.mice_task.event_type.isin([\"reward\", \"failure\"]))\n",
    "                    & (d.mice_task.task.isin([task]))\n",
    "                    & (d.mice_task.mouse_id.isin([str(mouse_id)]))]\n",
    "                if not len(data):\n",
    "                    continue\n",
    "                current_entropy = min_max([data[\"is_hole{}\".format(str(hole_no))].sum() /\n",
    "                                           len(data) for hole_no in [1, 3, 5, 7, 9]])\n",
    "                tmp += [task, entropy(current_entropy, base=2)]\n",
    "            if len(tmp) == 1:\n",
    "                continue\n",
    "            ret_val = ret_val.append([tmp])\n",
    "\n",
    "    ret_val.to_csv(os.path.join(\"data\", \"entropy\", \"entropy_tasks_{}.csv\".format(\"_\".join(target_task))), index=False,\n",
    "                   header=False)\n",
    "    # [ret_val[ret_val.task.isin([task])].to_csv(os.path.join(\"data\", \"entropy\", \"entropy_task_{}.csv\".format(task)),\n",
    "    #                                            index=False, header=False) for task in target_task]\n",
    "\n",
    "\n",
    "def view_converse_reaction_time(tdata, mice, tasks):\n",
    "    tasks = tasks if isinstance(tasks, list) else [tasks]\n",
    "    tdata = tdata if isinstance(tdata, list) else [tdata]\n",
    "    for task in tasks:\n",
    "        for data in tdata:\n",
    "            data = data.mice_delta[task][\n",
    "                (data.mice_delta[task].type.isin([\"reaction_time\"]))  # &\n",
    "                # (tdata.mice_delta.mouse_id == mice)\n",
    "            ].reaction_time_sec\n",
    "            fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "            data.plot.hist(bins=100)\n",
    "            plt.title(\"reaction time task:{}\".format(task))\n",
    "            plt.xlabel(\"reaction time(s)\")\n",
    "            plt.rcParams[\"font.size\"] = 18\n",
    "            plt.savefig(os.path.join(\"fig\", \"task-{}_reaction_time.png\".format(task)))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def view_converse_reward_latency(tdata, mice, tasks, bin=100):\n",
    "    tasks = tasks if isinstance(tasks, list) else [tasks]\n",
    "    tdata = tdata if isinstance(tdata, list) else [tdata]\n",
    "    for task in tasks:\n",
    "        for data in tdata:\n",
    "            data = data.mice_delta[task][\n",
    "                (data.mice_delta[task].type.isin([\"reward_latency\"]))  # &\n",
    "                # (tdata.mice_delta.mouse_id == mice)\n",
    "            ].noreward_duration_sec\n",
    "            fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "            data.plot.hist(bins=bin)\n",
    "            plt.xlabel(\"reward latency(s)\")\n",
    "            plt.title(\"reward latency task:{}\".format(task))\n",
    "            plt.rcParams[\"font.size\"] = 18\n",
    "            plt.savefig(os.path.join(\"fig\", \"task-{}_reward_latency.png\".format(task)))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            # under 100\n",
    "            fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "            data[data <= 300].plot.hist(bins=bin)\n",
    "            plt.xlabel(\"reward latency(s)\")\n",
    "            plt.title(\"reward latency task:{}\".format(task))\n",
    "            plt.rcParams[\"font.size\"] = 18\n",
    "            plt.savefig(os.path.join(\"fig\", \"task-{}_reward_latency_u300.png\".format(task)))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            # under 30000\n",
    "            fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "            data[data <= 10000].plot.hist(bins=bin)\n",
    "            plt.xlabel(\"reward latency(s)\")\n",
    "            plt.title(\"reward latency task:{}\".format(task))\n",
    "            plt.rcParams[\"font.size\"] = 18\n",
    "            plt.savefig(os.path.join(\"fig\", \"task-{}_reward_latency_u10000.png\".format(task)))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def view_50step_entropy(tdata, mice, tasks):\n",
    "    data = tdata\n",
    "    if isinstance(tdata, list):\n",
    "        data = pd.concat([d.mice_task for d in tdata])\n",
    "    else:\n",
    "        data = tdata.mice_task\n",
    "    # entropy\n",
    "    data = data[\n",
    "        (data.task.isin(tasks)) &\n",
    "        (data.event_type.isin([\"reward\"]))]\n",
    "    for task in tasks:\n",
    "        df = data[(data.task == task)].groupby([\"cumsum_correct_taskreset\"]).mean().head(150)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 8), dpi=100)\n",
    "        fig.suptitle('50step entropy task:{}'.format(task))\n",
    "        ax.plot(df.index, df['entropy_50'])\n",
    "        ax.set_ylabel('Entropy (bit)')\n",
    "        plt.rcParams[\"font.size\"] = 18\n",
    "        plt.savefig(os.path.join(\"fig\", \"task-{}_entropy_upto150.png\".format(task)))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    for task in tasks:\n",
    "        for mouse_id in mice:\n",
    "            df = data[(data.task == task) & (data.mouse_id == mouse_id)].set_index(\"cumsum_correct_taskreset\")\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(15, 8), dpi=100)\n",
    "            fig.suptitle('50step entropy task:{}'.format(task))\n",
    "            ax.plot(df.index, df['entropy_50'])\n",
    "            ax.set_ylabel('Entropy (bit)')\n",
    "            plt.savefig(os.path.join(\"fig\", \"no{}_task-{}_entropy.png\".format(mouse_id, task)))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def export_previeous_entropy(tdata, mice, tasks):\n",
    "    # entropy\n",
    "    data = tdata.mice_task[\n",
    "        (tdata.mice_task.task.isin(tasks)) &\n",
    "        (tdata.mice_task.event_type.isin([\"reward\", \"failure\"]))]\n",
    "    ret_val = dict(zip(tasks, [pd.DataFrame() for _ in tasks]))\n",
    "    for task in tasks:\n",
    "        for mouse_id in mice:\n",
    "            df = data[(data.task == task) & (data.mouse_id == mouse_id)].tail(100).head(50).reset_index()\n",
    "            ret_val[task] = ret_val[task].append(\n",
    "                df[\"entropy_50\"].to_frame().assign(mouse_id=mouse_id))\n",
    "            data[(data.task == task) & (data.mouse_id == mouse_id)].head(100).reset_index().tail(50)[\n",
    "                \"entropy_50\"].to_frame().to_csv(\n",
    "                os.path.join(\"data\", \"pre_entropy_no{}_task_{}.csv\".format(mouse_id, task)))\n",
    "            data[(data.task == task) & (data.mouse_id == mouse_id)].reset_index().tail(50)[\n",
    "                \"entropy_50\"].to_frame().to_csv(\n",
    "                os.path.join(\"data\", \"post_entropy_no{}_task_{}.csv\".format(mouse_id, task)))\n",
    "        ret_val[task].to_csv(os.path.join(\"data\", \"allmice_{}_previous_100step_entropy.csv\".format(task)))\n",
    "        ret_val[task].groupby(level=0).mean().entropy_50.to_csv(\n",
    "            os.path.join(\"data\", \"mean_{}_previous_100step_entropy.csv\".format(task)), index=False)\n",
    "\n",
    "\n",
    "def export_prepost_entropy(tdata, mice, tasks):\n",
    "    # entropy\n",
    "    cidata = tdata.mice_task[\n",
    "        (tdata.mice_task.task.isin(tasks)) &\n",
    "        (tdata.mice_task.event_type.isin([\"reward\",\"failure\"]))]\n",
    "    for task in tasks:\n",
    "        with open('data/prepost_entropy_task_{}.csv'.format(task), 'w', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for mouse_id in mice:\n",
    "                sz = cidata[(cidata.task == task) & (cidata.mouse_id == mouse_id)].reset_index()[\"entropy_50\"].size\n",
    "                print(\"size = {}\".format(sz))\n",
    "                if sz >= 300:\n",
    "                    pre  = cidata[(cidata.task == task) & (cidata.mouse_id == mouse_id)].reset_index()[\"entropy_50\"].iat[50]\n",
    "                    post = cidata[(cidata.task == task) & (cidata.mouse_id == mouse_id)].reset_index()[\"entropy_50\"].iat[300]\n",
    "                    writer.writerow([mouse_id, pre, post])\n",
    "                else:\n",
    "                    print(\"error\")\n",
    "\n",
    "\n",
    "def view_averaged_prob_same_prev(tdata, mice, tasks):\n",
    "    m = []\n",
    "    t = []\n",
    "    csame = []\n",
    "    fsame = []\n",
    "\n",
    "    for mouse_id in mice:\n",
    "        for task in tasks:\n",
    "            m += [mouse_id]\n",
    "            t += [task]\n",
    "            csame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['c_same']]\n",
    "            fsame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['f_same']]\n",
    "\n",
    "    after_prob_df = pd.DataFrame(\n",
    "        data={'mouse_id': m, 'task': t, 'c_same': csame, 'f_same': fsame},\n",
    "        columns=['mouse_id', 'task', 'c_same', 'f_same']\n",
    "    )\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(8, 4), dpi=100)\n",
    "    for task in tasks:\n",
    "        plt.subplot(1, len(tasks), tasks.index(task) + 1)\n",
    "\n",
    "        c_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['c_same'].to_list())\n",
    "        c_same_avg = np.mean(c_same, axis=0)\n",
    "        c_same_std = np.std(c_same, axis=0)\n",
    "        c_same_var = np.var(c_same, axis=0)\n",
    "\n",
    "        f_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['f_same'].to_list())\n",
    "        f_same_avg = np.mean(f_same, axis=0)\n",
    "        f_same_var = np.var(f_same, axis=0)\n",
    "\n",
    "        # 各個体ごと平均確率からの差\n",
    "        c_same_indiv_avg = c_same.groupby(\"mouse_id\").mean()\n",
    "        f_same_indiv_avg = f_same.groupby(\"mouse_id\").mean()\n",
    "        \n",
    "        task_prob_df = after_prob_df[after_prob_df['task'].isin([task])]\n",
    "        c_same_norm = ((task_prob_df['c_same']) - (pd.merge(task_prob_df['mouse_id'],c_same_indiv_avg,how='left',on=\"mouse_id\")[\"c_same\"])).to_numpy()\n",
    "        f_same_norm =((task_prob_df['f_same']) - (pd.merge(task_prob_df['mouse_id'],c_same_indiv_avg,how='left',on=\"mouse_id\")[\"f_same\"])).to_numpy()\n",
    "        after_prob_df = after_prob_df.assign(c_same_norm=c_same_norm)\n",
    "        after_prob_df = after_prob_df.assign(f_same_norm=f_same_norm)\n",
    "        \n",
    "        xlen = len(c_same_avg)\n",
    "        xax = np.array(range(1, xlen + 1))\n",
    "        plt.plot(xax, c_same_avg, label=\"rewarded start\")\n",
    "        plt.errorbar(xax, c_same_avg, yerr=c_same_norm, capsize=2, fmt='o', markersize=1, ecolor='black',\n",
    "                     markeredgecolor=\"black\", color='w', lolims=True)\n",
    "\n",
    "        plt.plot(np.array(range(1, xlen + 1)), f_same_avg, label=\"no-rewarded start\")\n",
    "        plt.errorbar(xax, f_same_avg, yerr=f_same_norm, capsize=2, fmt='o', markersize=1, ecolor='black',\n",
    "                     markeredgecolor=\"black\", color='w', uplims=True)\n",
    "\n",
    "        # plt.ion()\n",
    "        plt.xticks(np.arange(1, xlen + 1, 1))\n",
    "        plt.xlim(0.5, xlen + 0.5)\n",
    "        plt.ylim(0.25, 0.6)\n",
    "        if tasks.index(task) == 0:\n",
    "            plt.ylabel('P (same choice)')\n",
    "            plt.legend()\n",
    "        plt.xlabel('Trial')\n",
    "        plt.title('{}'.format(task))\n",
    "    # plt.savefig('fig/{}_prob_all4.png'.format(graph_ins.exportpath))\n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "    plt.savefig('fig/prob_all4_{}.png'.format(tasks[0]))\n",
    "    plt.show()\n",
    "    \n",
    "#    for mouse_id in mice:\n",
    "#        for task in tasks:\n",
    "#            after_prob_df[after_prob_df['mouse_id'].isin([mouse_id])][after_prob_df['task'].isin([task])][c_same].to_csv('data/{0}_{1}_correct.csv'.format(mouse_id, task))\n",
    "#           after_prob_df[after_prob_df['mouse_id'].isin([mouse_id])][after_prob_df['task'].isin([task])][f_same].to_csv('data/{0}_{1}_correct.csv'.format(mouse_id, task))\n",
    "\n",
    "#     after_prob_df[after_prob_df['task'].isin([task])].to_csv('data/{0}_ciprob.csv'.format(task))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mice90 = [35,36,38,39,40,41,42,43]\n",
    "tasks90 = [\"All5_90\"]\n",
    "\n",
    "logpath = '~/PycharmProjects/RaspSkinnerBox/MiceAnalysis'\n",
    "tdata90 = task_data(mice90, tasks90, logpath)\n",
    "\n",
    "#view_averaged_prob_same_prev(tdata90, mice90, tasks90)\n",
    "#view_summary(tdata, mice, tasks)\n",
    "\n",
    "# 45(体重無し?) ファイルが無い・要探索!\n",
    "# 27(体重無し)\n",
    "# 33: 317まで\n",
    "mice50 = [27,30,31,33,47,49,50]\n",
    "tasks50 = [\"All5_50\"]\n",
    "tdata50 = task_data(mice50, tasks50, logpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tdata30' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1b816a766be0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexport_P_20\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtdata30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmice30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasks30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}_aaaaa.csv\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks30\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mexport_P_20\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtdata50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmice50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasks50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}_aaaaa.csv\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks50\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mexport_P_20\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtdata90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmice90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasks90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}_aaaaa.csv\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks90\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tdata30' is not defined"
     ]
    }
   ],
   "source": [
    "export_P_20(tdata30, mice30, tasks30).to_csv(\"{}_aaaaa.csv\".format(tasks30[0]))\n",
    "export_P_20(tdata50, mice50, tasks50).to_csv(\"{}_aaaaa.csv\".format(tasks50[0]))\n",
    "export_P_20(tdata90, mice90, tasks90).to_csv(\"{}_aaaaa.csv\".format(tasks90[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-3e3adaa37cd9>, line 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-3e3adaa37cd9>\"\u001b[1;36m, line \u001b[1;32m64\u001b[0m\n\u001b[1;33m    f_same_norm = f_same - after_prob_df[]\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def view_averaged_prob_same_prev_avg(tdata, mice, tasks):\n",
    "    m = []\n",
    "    t = []\n",
    "    csame = []\n",
    "    fsame = []\n",
    "    m_average = []\n",
    "\n",
    "    for mouse_id in mice:\n",
    "        for task in tasks:\n",
    "            m += [mouse_id]\n",
    "            t += [task]\n",
    "            csame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['c_same']]\n",
    "            fsame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['f_same']]\n",
    "            m_average += [export_P_20(tdata, mouse_id, task).AVG]\n",
    "\n",
    "    after_prob_df = pd.DataFrame(\n",
    "        data={'mouse_id': m, 'task': t, 'c_same': csame, 'f_same': fsame, 'm_average': m_average},\n",
    "        columns=['mouse_id', 'task', 'c_same', 'f_same', 'm_average']\n",
    "    )\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(8, 4), dpi=100)\n",
    "    for task in tasks:\n",
    "        plt.subplot(1, len(tasks), tasks.index(task) + 1)\n",
    "\n",
    "        c_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['c_same'].to_list())\n",
    "        c_same_avg = np.mean(c_same, axis=0)\n",
    "        c_same_std = np.std(c_same, axis=0)\n",
    "        c_same_var = np.var(c_same, axis=0)\n",
    "        c_same_norm = c_same - np.repeat(np.mean(c_same,axis=1),9).reshape(-1,9)\n",
    "        c_same_norm_var = np.var(c_same_norm,axis=0)\n",
    "        c_same_norm_avg = np.mean(c_same_norm, axis=0)\n",
    "        \n",
    "\n",
    "        f_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['f_same'].to_list())\n",
    "        f_same_avg = np.mean(f_same, axis=0)\n",
    "        f_same_var = np.var(f_same, axis=0)\n",
    "#        f_same_norm = f_same - np.repeat(np.mean(f_same,axis=1),9).reshape(-1,9)\n",
    "        f_same_norm = f_same - after_prob_df[]\n",
    "\n",
    "        f_same_norm_var = np.var(f_same_norm,axis=0)\n",
    "        f_same_norm_avg = np.mean(f_same_norm, axis=0)\n",
    "\n",
    "        xlen = len(c_same_avg)\n",
    "        xax = np.array(range(1, xlen + 1))\n",
    "        plt.plot(xax, c_same_norm_avg, label=\"rewarded start\")\n",
    "        plt.errorbar(xax, c_same_norm_avg, yerr=c_same_norm_var, capsize=2, fmt='o', markersize=1, ecolor='black',\n",
    "                     markeredgecolor=\"black\", color='w', lolims=True)\n",
    "\n",
    "        plt.plot(np.array(range(1, xlen + 1)), f_same_norm_avg, label=\"no-rewarded start\")\n",
    "        plt.errorbar(xax, f_same_norm_avg, yerr=f_same_norm_var, capsize=2, fmt='o', markersize=1, ecolor='black',\n",
    "                     markeredgecolor=\"black\", color='w', uplims=True)\n",
    "\n",
    "        # plt.ion()\n",
    "        plt.xticks(np.arange(1, xlen + 1, 1))\n",
    "        plt.xlim(0.5, xlen + 0.5)\n",
    "#        plt.ylim(0.25, 0.6)\n",
    "        plt.ylim(-0.1, 0.1)\n",
    "        if tasks.index(task) == 0:\n",
    "            plt.ylabel('P (same choice)')\n",
    "            plt.legend()\n",
    "        plt.xlabel('Trial')\n",
    "        plt.title('{}'.format(task))\n",
    "#        plt.hlines(average,-11,11)\n",
    "        plt.hlines(0,-11,11)\n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "    plt.savefig('fig/WSLS_{}.png'.format(tasks[0]))\n",
    "    plt.show()\n",
    "    \n",
    "    #TODO: 1. c_same, f_sameに加えて，各マウスのaverageを各マウスのc_sameから引いて正規化したc_same_norm, f_same_normを計算\n",
    "    #TODO: 2. mice_id, c_same, f_same, c_same_norm, f_same_norm CSV出力 (グラフは両方)\n",
    "    \n",
    "def export_entropy300(tdata, mice, tasks):\n",
    "    # entropy\n",
    "    cidata = tdata.mice_task[\n",
    "        (tdata.mice_task.task.isin(tasks)) &\n",
    "        (tdata.mice_task.event_type.isin([\"reward\",\"failure\"]))]\n",
    "    for task in tasks:\n",
    "        with open('data/entropy_300_task_{}.csv'.format(task), 'w', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for mouse_id in mice:\n",
    "                sz = cidata[(cidata.task == task) & (cidata.mouse_id == mouse_id)][\"entropy_50\"].size\n",
    "#                 print(cidata[(cidata.task == task) & (cidata.mouse_id == mouse_id)])\n",
    "#                 print(\"size = {}\".format(sz))\n",
    "                if sz >= 300:\n",
    "                    post = cidata[(cidata.task == task) & (cidata.mouse_id == mouse_id)].reset_index()[\"entropy_150\"].iat[300]\n",
    "                    writer.writerow([mouse_id, post])\n",
    "                else:\n",
    "                    print(\"error\")\n",
    "\n",
    "\n",
    "# logpath = '~/PycharmProjects/RaspSkinnerBox/MiceAnalysis'\n",
    "logpath = \"./\"\n",
    "#45(体重無し?) ファイルが無い・要探索!\n",
    "# 27(体重無し)\n",
    "# 33: 317まで\n",
    "#mice50 = [27,30,31,33,47,49,50]\n",
    "#mice50 = [52, 64, 67, 68, 70, 71,  27,30,31,33,47,49,50]\n",
    "mice50 = [64, 67, 68, 70, 71,  27,30,31,33,47,49,50]\n",
    "\n",
    "tasks50 = [\"All5_50\"]\n",
    "tdata50 = task_data(mice50, tasks50, logpath)\n",
    "\n",
    "export_entropy300(tdata50, mice50, tasks50)\n",
    "view_averaged_prob_same_prev_avg(tdata50, mice50, tasks50)\n",
    "#view_averaged_prob_same_prev(tdata50, mice50, tasks50)\n",
    "\n",
    "# logpath = '~/PycharmProjects/RaspSkinnerBox/MiceAnalysis'\n",
    "mice30 = [6, 7, 8, 11, 12, 13, 14, 17, 18, 19, 21, 22, 23, 24]\n",
    "tasks30 = [\"All5_30\"]\n",
    "tdata30 = task_data(mice30, tasks30, logpath)\n",
    "export_entropy300(tdata30, mice30, tasks30)\n",
    "view_averaged_prob_same_prev_avg(tdata30, mice30, tasks30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 10先~19先で基準と同じ穴を選ぶ確率を平均する\n",
    "def export_P_20(tdata, mice, tasks, indiv=False):\n",
    "    data = tdata.mice_task[(tdata.mice_task.mouse_id.isin(mice))&(tdata.mice_task.event_type.isin([\"reward\",\"failure\"]))]\n",
    "    avg = []\n",
    "    avg_c = dict(zip(mice,[[] for _ in mice]))\n",
    "    avg_i = dict(zip(mice,[[] for _ in mice]))\n",
    "    for task in tasks:\n",
    "        task_data = data[data.task.isin([task])]\n",
    "        # 全mice平均\n",
    "        for no in mice:\n",
    "            mice_data = task_data[task_data.mouse_id.isin([no])]\n",
    "            prob = []\n",
    "            prob_co = []\n",
    "            prob_in = []\n",
    "            all_data = mice_data.reset_index()\n",
    "            correct_data = mice_data[mice_data.event_type.isin([\"reward\"])].reset_index()\n",
    "            incorrect_data = mice_data[mice_data.event_type.isin([\"failure\"])].reset_index()\n",
    "            length = len(mice_data)- 20\n",
    "            if length <= 0:\n",
    "                continue\n",
    "            for idx, dat in all_data.drop(range(len(all_data)-20,len(all_data))).iterrows():\n",
    "                tmp = 0\n",
    "                for j in range(10,20):\n",
    "                    if dat[\"hole_no\"] == all_data[\"hole_no\"][idx + j]:\n",
    "                        tmp += 1\n",
    "                prob.append(tmp/10)\n",
    "            avg.append(np.average(np.array(prob)))\n",
    "            # correctデータのみを対象， taskぶち抜き\n",
    "            for idx, dat in correct_data.drop(range(len(correct_data)-20,len(correct_data))).iterrows():\n",
    "                tmp = 0\n",
    "                for j in range(10,20):\n",
    "                    if dat[\"hole_no\"] == correct_data[\"hole_no\"][idx + j]:\n",
    "                        tmp += 1\n",
    "                prob_co.append(tmp/10)\n",
    "            avg_c[no].append(np.array(np.average(np.array(prob_co))))\n",
    "            # incorrect\n",
    "            for idx, dat in incorrect_data.drop(range(len(incorrect_data)-20,len(incorrect_data))).iterrows():\n",
    "                tmp = 0\n",
    "                for j in range(10,20):\n",
    "                    if dat[\"hole_no\"] == incorrect_data[\"hole_no\"][idx + j]:\n",
    "                        tmp += 1\n",
    "                prob_in.append(tmp/10)\n",
    "            avg_i[no].append(np.average(np.array(prob_in)))\n",
    "    for mouse_id in mice:\n",
    "        avg_c[mouse_id] = np.average(avg_c[mouse_id])\n",
    "        avg_i[mouse_id] = np.average(avg_i[mouse_id])\n",
    "    if indiv:\n",
    "        return {\"c_same\":[np.average(np.array(list(avg_c.values())))],\"f_same\":[np.average(np.array(list(avg_i.values())))],\n",
    "               \"c_same_mice\":avg_c,\"f_same_mice\":avg_i}\n",
    "    return pd.DataFrame(data={\"task\":tasks,\"AVG\":[np.average(avg)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...mouse_id=64\n",
      "max_id_col:6514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:133: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-09 10:28:25.095794 ; rehash_session_id done\n",
      "2020-06-09 10:28:25.455154 ; add_hot_vector done\n",
      "2020-06-09 10:28:26.453192 ; count_task done\n",
      "mouse_id=67\n",
      "max_id_col:6741\n",
      "2020-06-09 10:28:30.309488 ; rehash_session_id done\n",
      "2020-06-09 10:28:30.608066 ; add_hot_vector done\n",
      "2020-06-09 10:28:31.484585 ; count_task done\n",
      "mouse_id=68\n",
      "max_id_col:40449\n",
      "2020-06-09 10:29:02.730225 ; rehash_session_id done\n",
      "2020-06-09 10:29:05.857238 ; add_hot_vector done\n",
      "2020-06-09 10:29:07.232185 ; count_task done\n",
      "mouse_id=70\n",
      "max_id_col:18679\n",
      "2020-06-09 10:29:21.211747 ; rehash_session_id done\n",
      "2020-06-09 10:29:22.021288 ; add_hot_vector done\n",
      "2020-06-09 10:29:22.610812 ; count_task done\n",
      "mouse_id=71\n",
      "max_id_col:13437\n",
      "2020-06-09 10:29:30.788002 ; rehash_session_id done\n",
      "2020-06-09 10:29:31.904454 ; add_hot_vector done\n",
      "2020-06-09 10:29:33.552449 ; count_task done\n",
      "mouse_id=27\n",
      "max_id_col:25533\n",
      "2020-06-09 10:29:54.533104 ; rehash_session_id done\n",
      "2020-06-09 10:29:56.692962 ; add_hot_vector done\n",
      "2020-06-09 10:29:57.830731 ; count_task done\n",
      "mouse_id=30\n",
      "max_id_col:7274\n",
      "2020-06-09 10:30:03.139343 ; rehash_session_id done\n",
      "2020-06-09 10:30:03.540556 ; add_hot_vector done\n",
      "2020-06-09 10:30:04.425620 ; count_task done\n",
      "mouse_id=31\n",
      "max_id_col:14298\n",
      "2020-06-09 10:30:21.859666 ; rehash_session_id done\n",
      "2020-06-09 10:30:22.804677 ; add_hot_vector done\n",
      "2020-06-09 10:30:23.575327 ; count_task done\n",
      "mouse_id=33\n",
      "max_id_col:40005\n",
      "2020-06-09 10:30:48.173229 ; rehash_session_id done\n",
      "2020-06-09 10:30:49.776319 ; add_hot_vector done\n",
      "2020-06-09 10:30:50.350191 ; count_task done\n",
      "mouse_id=47\n",
      "max_id_col:16372\n",
      "2020-06-09 10:31:00.712220 ; rehash_session_id done\n",
      "2020-06-09 10:31:01.465230 ; add_hot_vector done\n",
      "2020-06-09 10:31:02.472656 ; count_task done\n",
      "mouse_id=49\n",
      "max_id_col:14751\n",
      "2020-06-09 10:31:11.893979 ; rehash_session_id done\n",
      "2020-06-09 10:31:12.495982 ; add_hot_vector done\n",
      "2020-06-09 10:31:13.741521 ; count_task done\n",
      "mouse_id=50\n",
      "max_id_col:8914\n",
      "2020-06-09 10:31:19.358929 ; rehash_session_id done\n",
      "2020-06-09 10:31:19.798040 ; add_hot_vector done\n",
      "2020-06-09 10:31:21.024736 ; count_task done\n",
      "2020-06-09 10:31:25.756917 ; export_csv done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "logpath = \"./\"\n",
    "mice50 = [64, 67, 68, 70, 71,  27,30,31,33,47,49,50]\n",
    "tasks50 = [\"All5_50\"]\n",
    "tdata50 = task_data(mice50, tasks50, logpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 選択確率(マウスごとの選択偏りを引いた)グラフ\n",
    "mice = mice50\n",
    "tdata = tdata50\n",
    "tasks = tasks50\n",
    "\n",
    "m = []\n",
    "t = []\n",
    "csame = []\n",
    "fsame = []\n",
    "m_average = []\n",
    "\n",
    "for mouse_id in mice:\n",
    "    for task in tasks:\n",
    "        m += [mouse_id]\n",
    "        t += [task]\n",
    "        csame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['c_same']]\n",
    "        fsame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['f_same']]\n",
    "        m_average += [export_P_20(tdata, [mouse_id], [task]).AVG]\n",
    "\n",
    "after_prob_df = pd.DataFrame(\n",
    "    data={'mouse_id': m, 'task': t, 'c_same': csame, 'f_same': fsame, 'm_average': m_average},\n",
    "    columns=['mouse_id', 'task', 'c_same', 'f_same', 'm_average']\n",
    ")\n",
    "\n",
    "for task in tasks:\n",
    "\n",
    "    c_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['c_same'].to_list())\n",
    "    c_same_avg = np.mean(c_same, axis=0)\n",
    "    c_same_std = np.std(c_same, axis=0)\n",
    "    c_same_var = np.var(c_same, axis=0)\n",
    "\n",
    "    c_same_norm = c_same - np.repeat(np.mean(c_same,axis=1),9).reshape(-1,9)\n",
    "    print(c_same_norm)\n",
    "    av = after_prob_df[after_prob_df['task'].isin([task])]['m_average'].to_numpy()\n",
    "    print(after_prob_df[after_prob_df['task'].isin([task])]['m_average'])\n",
    "    avm = np.repeat(av,9).reshape(-1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "x = 0.5\n",
    "p = np.linspace(0,1,100)\n",
    "q = p*x / (p*x + (1-p))\n",
    "\n",
    "plt.style.use('default')\n",
    "#fig = plt.figure(figsize=(8, 4), dpi=100)\n",
    "plt.plot(p, q, label=\"rewarded start\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位相性\n",
    "def export_phase_data(tdata, mice, tasks):\n",
    "    for task in tasks:\n",
    "        data = tdata.mice_task[\n",
    "                (tdata.mice_task.task == task) &\n",
    "                (tdata.mice_task.event_type.isin([\"nose poke\"]))]\n",
    "        distance = np.array([\n",
    "            [0,1,2,3,4],\n",
    "            [1,0,1,2,3],\n",
    "            [2,1,0,1,2],\n",
    "            [3,2,1,0,1],\n",
    "            [4,3,2,1,0]])\n",
    "        mice_hole_after_times = {}\n",
    "        mice_hole_select_times = np.empty((0,6), int)\n",
    "        mice_distance = {}\n",
    "        mice_correct = {}\n",
    "        mice_incorrect = {}\n",
    "\n",
    "        for mouse_id in mice:\n",
    "            mice_data = data[data.mouse_id.isin([mouse_id])]\n",
    "            tmp = np.zeros(5*5).reshape(5,5)\n",
    "            if mice_data.empty:\n",
    "                continue\n",
    "\n",
    "            # 1a. 各マウス、各タスク毎に5x5の割合のマトリックスをcsvで出す。\n",
    "            all_data = mice_data.reset_index()\n",
    "            length = len(mice_data)- 1\n",
    "            if length <= 0:\n",
    "                continue\n",
    "            for idx, dat in all_data.drop(range(len(all_data)-1,len(all_data))).iterrows():\n",
    "                tmp[int((int(all_data[\"hole_no\"][idx])-1)/2)][int((int(all_data[\"hole_no\"][idx + 1])-1)/2)] += 1\n",
    "            mice_hole_after_times.update({mouse_id:tmp/len(all_data)})\n",
    "            # export\n",
    "            np.savetxt(\"data/1a_no{}_task{}_selection.csv\".format(mouse_id,task),mice_hole_after_times[mouse_id],fmt=\"%f\",delimiter=',')\n",
    "            \n",
    "            # 2a. 各選択がサブタスク内でどのような割合であるかを示す基礎データをcsvに出す（1列目マウスID, 2～6列目1,3,5,7,9の選択数)。\n",
    "            tmp_select_times = (all_data[\"hole_no\"].value_counts() + pd.Series([0,0,0,0,0],index=[\"1\",\"3\",\"5\",\"7\",\"9\"])).fillna(0)\n",
    "            mouse_hole_select_times = np.concatenate((np.array([mouse_id]),tmp_select_times.to_numpy()/np.sum(tmp_select_times.to_numpy())))\n",
    "            mice_hole_select_times = np.append(mice_hole_select_times,[mouse_hole_select_times],axis=0)\n",
    "\n",
    "            # 3a. 横軸距離(1→1なら0, 1→3なら1, 1→5なら2, 1→7なら3, 1→9なら4, 3→5なら1 ...)、縦軸度数の割合（0～4の距離） のcsvを各マウス、各タスク毎に出力\n",
    "            mouse_dist = [0,0,0,0,0]\n",
    "            for dist,idx in zip(distance,range(len(distance))):\n",
    "                for num,col in zip(dist,range(len(dist))):\n",
    "                    mouse_dist[num] += tmp[idx,col]\n",
    "            mouse_dist /= sum(mouse_dist)\n",
    "            mice_distance.update({mouse_id:mouse_dist})\n",
    "            # export \n",
    "            np.savetxt(\"data/3a_no{}_task{}_selection_distance.csv\".format(mouse_id,task),mouse_dist,delimiter=',',fmt=\"%f\")\n",
    "            \n",
    "            # 4a correctの次の移動距離とincorrectの次の移動距離\n",
    "            dt = tdata.mice_task[(tdata.mice_task.task.isin([task]))]\n",
    "            correct = [0,0,0,0,0]\n",
    "            incorrect = [0,0,0,0,0]\n",
    "            correct_session = dt[dt.is_correct.isin([1])].session_id.values.tolist()\n",
    "            incorrect_session = dt[dt.is_failure.isin([1])].session_id.values.tolist()\n",
    "            for idx, dat in all_data[all_data.session_id.isin(correct_session)].iterrows():\n",
    "                try:\n",
    "                    correct[int(abs((int(all_data[\"hole_no\"][idx])-1)/2-\n",
    "                            (int(all_data[\"hole_no\"][idx+1])-1)/2))] += 1 \n",
    "                except KeyError:\n",
    "                    continue\n",
    "            for idx, dat in all_data[all_data.session_id.isin(incorrect_session)].iterrows():\n",
    "                try:\n",
    "                    incorrect[int(abs((int(all_data[\"hole_no\"][idx])-1)/2-\n",
    "                            (int(all_data[\"hole_no\"][idx+1])-1)/2))] += 1 \n",
    "                except KeyError:\n",
    "                    continue\n",
    "            mice_correct.update({mouse_id:correct})\n",
    "            mice_incorrect.update({mouse_id:incorrect})\n",
    "            np.savetxt(\"data/4a_no{}_task{}_correct.csv\".format(mouse_id,task),correct,delimiter=',',fmt=\"%f\")\n",
    "            np.savetxt(\"data/4a_no{}_task{}_incorrect.csv\".format(mouse_id,task),incorrect,delimiter=',',fmt=\"%f\")\n",
    "          \n",
    "        # 2a export\n",
    "        np.savetxt(\"data/2a_task{}_selection_rate.csv\".format(task),mice_hole_select_times,delimiter=',',fmt=\"%f\")\n",
    "        # 4a \n",
    "        np.savetxt(\"data/4a_no{}_task{}_correct.csv\".format(mouse_id,task),correct,delimiter=',',fmt=\"%f\")\n",
    "          \n",
    "        # 1b. 1aを基に各タスク毎、全マウス分の平均をcsvで出す。\n",
    "        all_mean_1 = np.mean(np.array(list(mice_hole_after_times.values())),axis=0)\n",
    "        np.savetxt(\"data/1b_allmice_task{}_selection_mean.csv\".format(task),all_mean_1,delimiter=',',fmt=\"%f\")\n",
    "        # 2b. 2aを基に各タスク毎、全マウス分の平均をcsvで出す。\n",
    "        all_mean_2 = np.delete(np.mean(mice_hole_select_times,axis=0),0,0)\n",
    "        np.savetxt(\"data/2b_allmice_task{}_selection_rate_mean.csv\".format(task),all_mean_2,delimiter=',',fmt=\"%f\")\n",
    "        # 3b. 3aを基に各タスク毎、全マウス分の平均をcsvで出力。            \n",
    "        all_mean_3 = np.mean(np.array(list(mice_distance.values())), axis=0)\n",
    "        np.savetxt(\"data/3b_allmice_task{}_selection_distance_mean.csv\".format(task),all_mean_3,delimiter=',',fmt=\"%f\")\n",
    "        # 4b\n",
    "        all_mean_4_co = np.mean(np.array(list(mice_correct.values())),axis=0)\n",
    "        all_mean_4_in = np.mean(np.array(list(mice_incorrect.values())),axis=0)\n",
    "        np.savetxt(\"data/4b_allmice_task{}_correct_mean.csv\".format(task),all_mean_4_co,delimiter=',',fmt=\"%f\")\n",
    "        np.savetxt(\"data/4b_allmice_task{}_incorrect_mean.csv\".format(task),all_mean_4_in,delimiter=',',fmt=\"%f\")\n",
    "\n",
    "        \n",
    "    # 出力\n",
    "    # 1a. 各マウス、各タスク毎に5x5の割合のマトリックスをcsvで出す。\n",
    "    # 1b. 1aを基に各タスク毎、全マウス分の平均をcsvで出す。\n",
    "    # 2a. 各選択がサブタスク内でどのような割合であるかを示す基礎データをcsvに出す（1列目マウスID, 2～6列目1,3,5,7,9の選択数)。\n",
    "    # 2b. 2aを基に各タスク毎、全マウス分の平均をcsvで出す。\n",
    "    # 3a. 横軸距離(1→1なら0, 1→3なら1, 1→5なら2, 1→7なら3, 1→9なら4, 3→5なら1 ...)、縦軸度数の割合（0～4の距離） のcsvを各マウス、各タスク毎に出力\n",
    "    # 3b. 3aを基に各タスク毎、全マウス分の平均をcsvで出力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_phase_data(tdata50,mice50,tasks50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{64: array([[0.8372093 , 0.77891156],\n",
      "       [0.81395349, 0.80272109],\n",
      "       [0.79401993, 0.81292517],\n",
      "       [0.79401993, 0.81632653],\n",
      "       [0.79734219, 0.82312925],\n",
      "       [0.79069767, 0.79931973],\n",
      "       [0.81395349, 0.78571429],\n",
      "       [0.79401993, 0.79251701],\n",
      "       [0.80066445, 0.79591837]]), 67: array([[0.54135338, 0.46551724],\n",
      "       [0.5112782 , 0.52068966],\n",
      "       [0.57142857, 0.50344828],\n",
      "       [0.51503759, 0.49655172],\n",
      "       [0.55263158, 0.50344828],\n",
      "       [0.5075188 , 0.54137931],\n",
      "       [0.48496241, 0.54482759],\n",
      "       [0.51503759, 0.46551724],\n",
      "       [0.51503759, 0.54137931]]), 68: array([[0.54817276, 0.4516129 ],\n",
      "       [0.52491694, 0.41290323],\n",
      "       [0.52491694, 0.45483871],\n",
      "       [0.50830565, 0.51935484],\n",
      "       [0.44850498, 0.51612903],\n",
      "       [0.48504983, 0.46774194],\n",
      "       [0.46179402, 0.48709677],\n",
      "       [0.46511628, 0.4483871 ],\n",
      "       [0.52159468, 0.46774194]]), 70: array([[0.94472362, 0.95402299],\n",
      "       [0.95477387, 0.94252874],\n",
      "       [0.93467337, 0.93678161],\n",
      "       [0.92964824, 0.93103448],\n",
      "       [0.93969849, 0.93103448],\n",
      "       [0.92462312, 0.94252874],\n",
      "       [0.92462312, 0.93103448],\n",
      "       [0.91959799, 0.94252874],\n",
      "       [0.91959799, 0.92528736]]), 71: array([[0.56146179, 0.60655738],\n",
      "       [0.63122924, 0.60983607],\n",
      "       [0.60465116, 0.60327869],\n",
      "       [0.56478405, 0.61639344],\n",
      "       [0.61129568, 0.61639344],\n",
      "       [0.56146179, 0.63606557],\n",
      "       [0.62126246, 0.62295082],\n",
      "       [0.59800664, 0.63606557],\n",
      "       [0.6013289 , 0.65901639]]), 27: array([[0.61589404, 0.71584699],\n",
      "       [0.67549669, 0.62295082],\n",
      "       [0.68874172, 0.64480874],\n",
      "       [0.68211921, 0.67213115],\n",
      "       [0.69536424, 0.69398907],\n",
      "       [0.61589404, 0.66666667],\n",
      "       [0.66887417, 0.66666667],\n",
      "       [0.66887417, 0.67213115],\n",
      "       [0.71523179, 0.6557377 ]]), 30: array([[0.22516556, 0.21084337],\n",
      "       [0.32450331, 0.19277108],\n",
      "       [0.28476821, 0.21686747],\n",
      "       [0.25165563, 0.22891566],\n",
      "       [0.24503311, 0.21084337],\n",
      "       [0.25165563, 0.26506024],\n",
      "       [0.25165563, 0.23493976],\n",
      "       [0.23178808, 0.27108434],\n",
      "       [0.31788079, 0.25903614]]), 31: array([[0.29411765, 0.18918919],\n",
      "       [0.30065359, 0.25405405],\n",
      "       [0.18954248, 0.24864865],\n",
      "       [0.2875817 , 0.24864865],\n",
      "       [0.26797386, 0.2972973 ],\n",
      "       [0.30718954, 0.23243243],\n",
      "       [0.21568627, 0.27567568],\n",
      "       [0.24183007, 0.23243243],\n",
      "       [0.32026144, 0.29189189]]), 33: array([[0.40397351, 0.28333333],\n",
      "       [0.29801325, 0.37777778],\n",
      "       [0.31125828, 0.37222222],\n",
      "       [0.33112583, 0.29444444],\n",
      "       [0.34437086, 0.38333333],\n",
      "       [0.36423841, 0.38333333],\n",
      "       [0.39735099, 0.31666667],\n",
      "       [0.37748344, 0.30555556],\n",
      "       [0.37748344, 0.36111111]]), 47: array([[0.67109635, 0.57827476],\n",
      "       [0.66445183, 0.61341853],\n",
      "       [0.65780731, 0.64217252],\n",
      "       [0.65780731, 0.60383387],\n",
      "       [0.67109635, 0.62619808],\n",
      "       [0.67109635, 0.61980831],\n",
      "       [0.64451827, 0.61661342],\n",
      "       [0.66112957, 0.60702875],\n",
      "       [0.63787375, 0.61341853]]), 49: array([[0.31229236, 0.32058824],\n",
      "       [0.35880399, 0.34411765],\n",
      "       [0.33222591, 0.31470588],\n",
      "       [0.34219269, 0.33823529],\n",
      "       [0.33222591, 0.36176471],\n",
      "       [0.37209302, 0.35588235],\n",
      "       [0.36877076, 0.29411765],\n",
      "       [0.34551495, 0.34411765],\n",
      "       [0.34219269, 0.30588235]]), 50: array([[0.23920266, 0.21604938],\n",
      "       [0.24252492, 0.15740741],\n",
      "       [0.24584718, 0.22839506],\n",
      "       [0.22259136, 0.22222222],\n",
      "       [0.2358804 , 0.19753086],\n",
      "       [0.20930233, 0.22222222],\n",
      "       [0.23920266, 0.25617284],\n",
      "       [0.24916944, 0.26234568],\n",
      "       [0.24584718, 0.2345679 ]])}\n"
     ]
    }
   ],
   "source": [
    "print(count_task(tdata50,mice50,tasks50,[1,3,7,9]))\n",
    "# print(count_task(tdata50,mice50,tasks50,5))\n",
    "# print(count_task(tdata50,mice50,tasks50),[1,3,5,7,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def export_P_20_filter(dc,mice):\n",
    "    hole_range = range(1,10)\n",
    "    avg_c = np.empty((0,len(hole_range)))\n",
    "    avg_f = np.empty((0,len(hole_range)))\n",
    "    avg_c_base = np.empty(0)\n",
    "    avg_i_base = np.empty(0)\n",
    "    \n",
    "    for no in mice:\n",
    "        mice_data = dc[dc.mouse_id.isin([no])]\n",
    "        prob_co = []\n",
    "        prob_in = []\n",
    "        prob_co_base = []\n",
    "        prob_in_base = []\n",
    "        correct_data = mice_data[mice_data.event_type.isin([\"reward\"])].reset_index()\n",
    "        incorrect_data = mice_data[mice_data.event_type.isin([\"failure\"])].reset_index()\n",
    "        length = len(mice_data)- 20\n",
    "        if length <= 0:\n",
    "            continue\n",
    "        \n",
    "        # c_same\n",
    "        # export: mice * hole prob\n",
    "        for idx, dat in correct_data.drop(range(len(correct_data)-10,len(correct_data))).iterrows():\n",
    "            tmp = np.zeros((len(hole_range),))\n",
    "            for j in hole_range:\n",
    "                if dat[\"hole_no\"] == correct_data[\"hole_no\"][idx + j]:\n",
    "                    tmp[j-1] += 1\n",
    "            prob_co.append(tmp)\n",
    "        avg_c = np.append(avg_c, np.array([np.sum(np.array(prob_co),axis=0)/(len(correct_data)-10)]),axis=0)\n",
    "        \n",
    "        # f_same\n",
    "        # export: mice * hole prob\n",
    "        for idx, dat in incorrect_data.drop(range(len(incorrect_data)-10,len(incorrect_data))).iterrows():\n",
    "            tmp = np.zeros((len(hole_range),))\n",
    "            for j in hole_range:\n",
    "                if dat[\"hole_no\"] == incorrect_data[\"hole_no\"][idx + j]:\n",
    "                    tmp[j-1] += 1\n",
    "            prob_in.append(tmp)\n",
    "        avg_f = np.append(avg_f, np.array([np.sum(np.array(prob_in),axis=0)/(len(incorrect_data)-10)]),axis=0)\n",
    "\n",
    "        # base correctデータのみを対象， taskぶち抜き \n",
    "        # export: mice \n",
    "        for idx, dat in correct_data.drop(range(len(correct_data)-20,len(correct_data))).iterrows():\n",
    "            tmp = 0\n",
    "            for j in range(10,20):\n",
    "                if dat[\"hole_no\"] == correct_data[\"hole_no\"][idx + j]:\n",
    "                    tmp += 1\n",
    "            prob_co_base.append(tmp/10)\n",
    "        avg_c_base = np.append(avg_c_base, np.average(np.array(prob_co_base)))\n",
    "        \n",
    "        # incorrect base\n",
    "        # export: mice\n",
    "        for idx, dat in incorrect_data.drop(range(len(incorrect_data)-20,len(incorrect_data))).iterrows():\n",
    "            tmp = 0\n",
    "            for j in range(10,20):\n",
    "                if dat[\"hole_no\"] == incorrect_data[\"hole_no\"][idx + j]:\n",
    "                    tmp += 1\n",
    "            prob_in_base.append(tmp/10)\n",
    "        avg_i_base = np.append(avg_i_base, np.average(np.array(prob_in_base)))\n",
    "        \n",
    "    return {\"c_same\":avg_c,\"f_same\":avg_f,\"c_same_base\":avg_c_base,\"f_same_base\":avg_i_base}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_task(tdata, mice, tasks,selection=[1,3,5,7,9]) -> dict:\n",
    "    # count_taskをclass task_dataの外で下記の仕様で再実装（classから消す必要はない）ßß\n",
    "        if isinstance(selection,str):\n",
    "            selection = [selection]\n",
    "        elif isinstance(selection,int):\n",
    "            selection = [str(selection)]\n",
    "        selection = [str(num)for num in selection]\n",
    "        dc = tdata.mice_task[tdata.mice_task[\"event_type\"].isin([\"reward\", \"failure\"]) & tdata.mice_task.task.isin(tasks)]\n",
    "        dc = dc.reset_index()\n",
    "\n",
    "        after_c_all_task = {}\n",
    "        after_f_all_task = {}\n",
    "\n",
    "        after_c_starts_task = {}\n",
    "        after_f_starts_task = {}\n",
    "\n",
    "        prob_index = [\"c_same\", \"f_same\"]\n",
    "        forward_trace = 10\n",
    "        task_prob = {}\n",
    "        task_prob_hole = dict(zip(selection,[[] for _ in selection]))\n",
    "        tmp_dt = dc[dc[\"hole_no\"].isin(selection)]\n",
    "        prob = export_P_20_filter(tmp_dt,mice)\n",
    "        prob[\"c_same\"] = np.hstack((prob[\"c_same_base\"].reshape((len(mice),1)), prob[\"c_same\"]))\n",
    "        prob[\"f_same\"] = np.hstack((prob[\"f_same_base\"].reshape((len(mice),1)), prob[\"f_same\"]))\n",
    "        \n",
    "        #         prob[\"c_same\"].to_csv(\"./data/mices_task{}_cistart.csv\".format(,\"\".join(task))\n",
    "        \n",
    "\n",
    "        correct_data = pd.DataFrame(prob[\"c_same\"],index=mice)\n",
    "        incorrect_data = pd.DataFrame(prob[\"f_same\"],index=mice)\n",
    "        # 3のタスク毎の任意の複数の選択肢毎の全マウス平均をcsv出力 \n",
    "        correct_data.to_csv(\"./data/mice_task{}_hole{}_cstart.csv\".format(\"-\".join(tasks),\"\".join(selection)))\n",
    "        incorrect_data.to_csv(\"./data/mice_task{}_hole{}_fstart.csv\".format(\"-\".join(tasks),\"\".join(selection)))\n",
    "\n",
    "        return task_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_task(tdata50,mice50,tasks50,[1,3,7,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...mouse_id=6\n",
      "max_id_col:17052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:133: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30 17:32:43.399883 ; rehash_session_id done\n",
      "2020-06-30 17:32:44.042737 ; add_hot_vector done\n",
      "2020-06-30 17:32:44.945038 ; count_task done\n",
      "mouse_id=7\n",
      "max_id_col:20087\n",
      "2020-06-30 17:32:54.433442 ; rehash_session_id done\n",
      "2020-06-30 17:32:55.636386 ; add_hot_vector done\n",
      "2020-06-30 17:32:56.546124 ; count_task done\n",
      "mouse_id=8\n",
      "max_id_col:16222\n",
      "2020-06-30 17:33:06.167472 ; rehash_session_id done\n",
      "2020-06-30 17:33:06.981175 ; add_hot_vector done\n",
      "2020-06-30 17:33:08.154463 ; count_task done\n",
      "mouse_id=11\n",
      "max_id_col:17458\n",
      "2020-06-30 17:33:17.141532 ; rehash_session_id done\n",
      "2020-06-30 17:33:17.866652 ; add_hot_vector done\n",
      "2020-06-30 17:33:18.622965 ; count_task done\n",
      "mouse_id=12\n",
      "max_id_col:17487\n",
      "2020-06-30 17:33:27.925505 ; rehash_session_id done\n",
      "2020-06-30 17:33:28.639022 ; add_hot_vector done\n",
      "2020-06-30 17:33:29.557331 ; count_task done\n",
      "mouse_id=13\n",
      "max_id_col:16723\n",
      "2020-06-30 17:33:41.647065 ; rehash_session_id done\n",
      "2020-06-30 17:33:42.370063 ; add_hot_vector done\n",
      "2020-06-30 17:33:43.350280 ; count_task done\n",
      "mouse_id=14\n",
      "max_id_col:26626\n",
      "2020-06-30 17:33:59.124234 ; rehash_session_id done\n",
      "2020-06-30 17:34:00.185095 ; add_hot_vector done\n",
      "2020-06-30 17:34:01.148394 ; count_task done\n",
      "mouse_id=17\n",
      "max_id_col:19525\n",
      "2020-06-30 17:34:11.555209 ; rehash_session_id done\n",
      "2020-06-30 17:34:12.272816 ; add_hot_vector done\n",
      "2020-06-30 17:34:13.246321 ; count_task done\n",
      "mouse_id=18\n",
      "max_id_col:15280\n",
      "2020-06-30 17:34:21.845124 ; rehash_session_id done\n",
      "2020-06-30 17:34:22.455191 ; add_hot_vector done\n",
      "2020-06-30 17:34:23.537034 ; count_task done\n",
      "mouse_id=19\n",
      "max_id_col:22119\n",
      "2020-06-30 17:34:36.481182 ; rehash_session_id done\n",
      "2020-06-30 17:34:37.737699 ; add_hot_vector done\n",
      "2020-06-30 17:34:39.318387 ; count_task done\n",
      "mouse_id=21\n",
      "max_id_col:19414\n",
      "2020-06-30 17:34:54.285510 ; rehash_session_id done\n",
      "2020-06-30 17:34:55.664248 ; add_hot_vector done\n",
      "2020-06-30 17:34:56.685720 ; count_task done\n",
      "mouse_id=22\n",
      "max_id_col:20753\n",
      "2020-06-30 17:35:08.768882 ; rehash_session_id done\n",
      "2020-06-30 17:35:09.506246 ; add_hot_vector done\n",
      "2020-06-30 17:35:10.322567 ; count_task done\n",
      "mouse_id=23\n",
      "max_id_col:27818\n",
      "2020-06-30 17:35:23.788272 ; rehash_session_id done\n",
      "2020-06-30 17:35:24.802366 ; add_hot_vector done\n",
      "2020-06-30 17:35:25.625155 ; count_task done\n",
      "mouse_id=24\n",
      "max_id_col:31241\n",
      "2020-06-30 17:35:40.794352 ; rehash_session_id done\n",
      "2020-06-30 17:35:41.910051 ; add_hot_vector done\n",
      "2020-06-30 17:35:42.821923 ; count_task done\n",
      "2020-06-30 17:35:49.632388 ; export_csv done\n",
      "done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'export_entropy300' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-76aee3e8d4f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtasks30\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"All5_30\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtdata30\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmice30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mexport_entropy300\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtdata30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmice30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mview_averaged_prob_same_prev_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtdata30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmice30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'export_entropy300' is not defined"
     ]
    }
   ],
   "source": [
    "logpath = \"~/Downloads/takarada\"\n",
    "mice30 = [6, 7, 8, 11, 12, 13, 14, 17, 18, 19, 21, 22, 23, 24]\n",
    "tasks30 = [\"All5_30\"]\n",
    "tdata30 = task_data(mice30, tasks30, logpath)\n",
    "export_entropy300(tdata30, mice30, tasks30)\n",
    "view_averaged_prob_same_prev_avg(tdata30, mice30, tasks30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyentrp import entropy as pent\n",
    "def calc_permutation_entropy(tdata,mice,tasks):\n",
    "    for tsk in tasks:\n",
    "        en_list1 = []\n",
    "        en_list2 = []\n",
    "        len_list = []\n",
    "        for mouse_no in mice:\n",
    "            data = tdata.mice_task[\n",
    "                (tdata.mice_task.mouse_id == mouse_no)\n",
    "                &(tdata.mice_task.task == tsk)\n",
    "                &(tdata.mice_task.hole_no.isin([\"1\",\"3\",\"5\",\"7\",\"9\"]))\n",
    "                &(tdata.mice_task.event_type.isin([\"reward\",\"failure\"]))\n",
    "            ][[\"mouse_id\",\"task\",\"hole_no\",\"event_type\",\"correct_times\"]]\n",
    "            data = data.reset_index()\n",
    "            leng = len(data)\n",
    "            choice_data1 = data.head(300).hole_no.astype(int).values\n",
    "            choice_data2 = data.head(150).hole_no.astype(int).values\n",
    "            en1 = pent.shannon_entropy(choice_data1)\n",
    "            en2 = pent.shannon_entropy(choice_data2)\n",
    "            en_list1.append(en1)\n",
    "            en_list2.append(en2)\n",
    "        en_list_vert = np.stack([mice, en_list1, en_list2])\n",
    "        np.savetxt(f\"./data/entropy/allmice_{tsk}.csv\",en_list_vert,delimiter=',',fmt=\"%f\")\n",
    "    return en_list1, en_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_daily_feeds(tdata,mice):\n",
    "    feeds_data = pd.DataFrame(columns=[\"id\",\"timestamp\",\"day\",\"feed\",\"tasks\"])\n",
    "    base_time = datetime.time(8)\n",
    "    # mouseid, 日付, 何日目, 粒数, そのときにやっていたタスクのリストをハイフン繋ぎ\n",
    "    for mouse_id in mice:\n",
    "        data = tdata.mice_task[tdata.mice_task.mouse_id.isin([mouse_id]) & \n",
    "                               tdata.mice_task.event_type.isin([\"reward\"])]\n",
    "        if data.empty:\n",
    "            continue\n",
    "        # timestamp(start)\n",
    "        start_timestamp = data.timestamps.iloc[0]\n",
    "        start_date = start_timestamp.date() \n",
    "#         + datetime.timedelta(\n",
    "#             int(start_timestamp.time() > datetime.time(8,0,0)))\n",
    "        finish_date = data.timestamps.iat[-1].date() - datetime.timedelta(days=1)\n",
    "        # task\n",
    "        task = \"-\".join(list(data.task.unique()))\n",
    "        # day\n",
    "        for d in pd.date_range(start_date,finish_date,freq=\"D\"):\n",
    "            # index:d\n",
    "            # range\n",
    "            rew = data.event_type[(data.timestamps > datetime.datetime.combine(d,base_time)) & \n",
    "                 (data.timestamps < datetime.datetime.combine(d+datetime.timedelta(days=1),base_time))].count()\n",
    "            feeds_data = feeds_data.append(\n",
    "                pd.Series([mouse_id,d,(d.date()-start_timestamp.date() + datetime.timedelta(days=1)).days,rew,task],index=[\"id\",\"timestamp\",\"day\",\"feed\",\"tasks\"]\n",
    "                                                                         ),ignore_index=True)\n",
    "    feeds_data.to_csv(\"./data/allmice_feeds_summary_{}.csv\".format(\"-\".join(list(data.task.unique()))))\n",
    "    return feeds_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = export_daily_feeds(tdata30,mice30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>day</th>\n",
       "      <th>feed</th>\n",
       "      <th>tasks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Other30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>2019-05-11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Other30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>2019-05-25</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Other30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>2019-05-23</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>2019-05-24</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>2019-05-26</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>2019-05-28</td>\n",
       "      <td>7</td>\n",
       "      <td>61</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>2019-05-29</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>19</td>\n",
       "      <td>2019-06-05</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>2019-06-26</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_70-Not5_Othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>22</td>\n",
       "      <td>2019-06-26</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_70-Not5_Othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>24</td>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50-Not5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  timestamp day feed                                              tasks\n",
       "5    7 2019-05-06   2   42  T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Other30\n",
       "10   8 2019-05-11   1    6  T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Other30\n",
       "27  13 2019-05-25   4   52  T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Other30\n",
       "29  14 2019-05-23   2   43  T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...\n",
       "30  14 2019-05-24   3   58  T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...\n",
       "32  14 2019-05-26   5   33  T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...\n",
       "33  14 2019-05-27   6   46  T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...\n",
       "34  14 2019-05-28   7   61  T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...\n",
       "35  14 2019-05-29   8   12  T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...\n",
       "47  19 2019-06-05   2   50  T0-No TC-TC100-TC70-All5_30-Only5_50-Not5_Othe...\n",
       "53  21 2019-06-26   2   14  T0-No TC-TC100-TC70-All5_30-Only5_70-Not5_Othe...\n",
       "59  22 2019-06-26   2   17  T0-No TC-TC100-TC70-All5_30-Only5_70-Not5_Othe...\n",
       "71  24 2019-07-03   2   13  T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50-Not5..."
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a.feed < 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = export_daily_feeds(tdata50,mice50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>day</th>\n",
       "      <th>feed</th>\n",
       "      <th>tasks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>68</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-All5_30-egg1_Other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>68</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-All5_30-egg1_Other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>68</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-All5_30-egg1_Other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>2019-07-11</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-Only5_50-Not5_Othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>2019-07-13</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-Only5_50-Not5_Othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>2019-08-06</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-All5_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>31</td>\n",
       "      <td>2019-08-06</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>All5_50-T0-No TC-TC100-TC70-All5_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>33</td>\n",
       "      <td>2019-08-13</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>33</td>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>33</td>\n",
       "      <td>2019-08-15</td>\n",
       "      <td>7</td>\n",
       "      <td>52</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>2019-08-16</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>2019-08-17</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>33</td>\n",
       "      <td>2019-08-18</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>33</td>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>47</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>No TC-TC100-TC70-All5_50-All5_30-All5_30_drug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>47</td>\n",
       "      <td>2019-11-06</td>\n",
       "      <td>7</td>\n",
       "      <td>66</td>\n",
       "      <td>No TC-TC100-TC70-All5_50-All5_30-All5_30_drug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>49</td>\n",
       "      <td>2019-11-08</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>No TC-TC100-TC70-All5_50-All5_30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  timestamp day feed                                              tasks\n",
       "8   68 2019-12-29   3    0  T0-No TC-TC100-TC70-All5_50-All5_30-egg1_Other...\n",
       "13  68 2020-01-03   8   51  T0-No TC-TC100-TC70-All5_50-All5_30-egg1_Other...\n",
       "14  68 2020-01-04   9   15  T0-No TC-TC100-TC70-All5_50-All5_30-egg1_Other...\n",
       "24  27 2019-07-11   2   14  T0-No TC-TC100-TC70-All5_50-Only5_50-Not5_Othe...\n",
       "26  27 2019-07-13   4   45  T0-No TC-TC100-TC70-All5_50-Only5_50-Not5_Othe...\n",
       "32  30 2019-08-06   2    1                T0-No TC-TC100-TC70-All5_50-All5_30\n",
       "36  31 2019-08-06   2    0                All5_50-T0-No TC-TC100-TC70-All5_30\n",
       "43  33 2019-08-13   5   29  T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...\n",
       "44  33 2019-08-14   6   26  T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...\n",
       "45  33 2019-08-15   7   52  T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...\n",
       "46  33 2019-08-16   8   40  T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...\n",
       "47  33 2019-08-17   9   23  T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...\n",
       "48  33 2019-08-18  10   19  T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...\n",
       "49  33 2019-08-19  11   10  T0-No TC-TC100-TC70-All5_50-All5_30-All5_10-Al...\n",
       "51  47 2019-11-01   2   58      No TC-TC100-TC70-All5_50-All5_30-All5_30_drug\n",
       "56  47 2019-11-06   7   66      No TC-TC100-TC70-All5_50-All5_30-All5_30_drug\n",
       "58  49 2019-11-08   2   37                   No TC-TC100-TC70-All5_50-All5_30"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[b.feed<70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def export_daily_feeds(tdata,mice,end_task):\n",
    "    feeds_data = pd.DataFrame(columns=[\"id\",\"timestamp\",\"day\",\"feed\",\"tasks\"])\n",
    "    # mouseid, 日付, 何日目, 粒数, そのときにやっていたタスクのリストをハイフン繋ぎ\n",
    "    for mouse_id in mice:\n",
    "        data = tdata.mice_task[tdata.mice_task.mouse_id.isin([mouse_id]) & \n",
    "                               tdata.mice_task.event_type.isin([\"reward\"])].reset_index()\n",
    "        if data.empty:\n",
    "            continue\n",
    "        start_timestamp = data.timestamps.iloc[0]\n",
    "        start_date = start_timestamp.date() \n",
    "        base_time = start_timestamp.time()\n",
    "        last_taskday = data[:data[data.task.isin([end_task])].index[-1]].timestamps.iat[-1]\n",
    "        \n",
    "        finish_date = last_taskday.date() - dt.timedelta(days=1)*(last_taskday.time() < base_time)\n",
    "        data = data[data.timestamps < dt.datetime.combine(finish_date+dt.timedelta(days=1),base_time)]\n",
    "        # task\n",
    "        task = \"-\".join(list(data.task.unique()))\n",
    "        # day\n",
    "        for d in pd.date_range(start_date,finish_date,freq=\"D\"):\n",
    "            # index:d\n",
    "            # range\n",
    "            rew = data.event_type[(data.timestamps > dt.datetime.combine(d,base_time)) & \n",
    "                 (data.timestamps < dt.datetime.combine(d+dt.timedelta(days=1),base_time))].count()\n",
    "            feeds_data = feeds_data.append(\n",
    "                pd.Series([mouse_id,d,(d.date()-start_timestamp.date() + dt.timedelta(days=1)).days,rew,task],index=[\"id\",\"timestamp\",\"day\",\"feed\",\"tasks\"]\n",
    "                                                                         ),ignore_index=True)\n",
    "    feeds_data.to_csv(\"./data/allmice_feeds_summary_{}.csv\".format(\"-\".join(list(data.task.unique()))))\n",
    "    return feeds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>day</th>\n",
       "      <th>feed</th>\n",
       "      <th>tasks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2019-05-09</td>\n",
       "      <td>2</td>\n",
       "      <td>158</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2019-05-05</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2019-05-07</td>\n",
       "      <td>3</td>\n",
       "      <td>111</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2019-05-11</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2019-05-12</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>3</td>\n",
       "      <td>148</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>2019-05-15</td>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2019-05-17</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>2019-05-22</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>2019-05-23</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>2019-05-22</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>2019-05-23</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>2019-05-24</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>2019-05-25</td>\n",
       "      <td>4</td>\n",
       "      <td>91</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>2019-06-04</td>\n",
       "      <td>2</td>\n",
       "      <td>138</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>2019-06-05</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>2019-06-04</td>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>2019-06-05</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>3</td>\n",
       "      <td>129</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>2019-06-25</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>2019-06-26</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>2019-06-27</td>\n",
       "      <td>3</td>\n",
       "      <td>141</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "      <td>2019-06-25</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>2019-06-26</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>2019-06-27</td>\n",
       "      <td>3</td>\n",
       "      <td>177</td>\n",
       "      <td>T0-No TC-TC100-TC70-All5_30-Only5_70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>1</td>\n",
       "      <td>167</td>\n",
       "      <td>T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>23</td>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  timestamp day feed                                      tasks\n",
       "0    6 2019-05-08   1  172       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "1    6 2019-05-09   2  158       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "2    7 2019-05-05   1  142       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "3    7 2019-05-06   2   44       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "4    7 2019-05-07   3  111       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "5    7 2019-05-08   4   81       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "6    8 2019-05-11   1  161       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "7    8 2019-05-12   2  101       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "8    8 2019-05-13   3  148       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "9   11 2019-05-14   1  161       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "10  11 2019-05-15   2  146       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "11  12 2019-05-16   1  181       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "12  12 2019-05-17   2  129       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "13  13 2019-05-22   1  175       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "14  13 2019-05-23   2  150       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "15  14 2019-05-22   1  122       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "16  14 2019-05-23   2   45       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "17  14 2019-05-24   3   58       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "18  14 2019-05-25   4   91       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "19  17 2019-05-31   1  192       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "20  17 2019-06-01   2  130       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "21  18 2019-06-03   1  164       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "22  18 2019-06-04   2  138       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "23  18 2019-06-05   3  159       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "24  19 2019-06-04   1  146       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "25  19 2019-06-05   2   77       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "26  19 2019-06-06   3  129       T0-No TC-TC100-TC70-All5_30-Only5_50\n",
       "27  21 2019-06-25   1  164       T0-No TC-TC100-TC70-All5_30-Only5_70\n",
       "28  21 2019-06-26   2   21       T0-No TC-TC100-TC70-All5_30-Only5_70\n",
       "29  21 2019-06-27   3  141       T0-No TC-TC100-TC70-All5_30-Only5_70\n",
       "30  22 2019-06-25   1  160       T0-No TC-TC100-TC70-All5_30-Only5_70\n",
       "31  22 2019-06-26   2   41       T0-No TC-TC100-TC70-All5_30-Only5_70\n",
       "32  22 2019-06-27   3  177       T0-No TC-TC100-TC70-All5_30-Only5_70\n",
       "33  23 2019-07-02   1  167  T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50\n",
       "34  23 2019-07-03   2  159  T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50\n",
       "35  24 2019-07-02   1  152  T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50\n",
       "36  24 2019-07-03   2    9  T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50\n",
       "37  24 2019-07-04   3  105  T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50\n",
       "38  24 2019-07-05   4   67  T0-No TC-NoTC-TC100-TC70-All5_30-Only5_50"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_daily_feeds(tdata30,mice30,\"All5_30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as collections\n",
    "import matplotlib.markers as markers\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# サブタスクの実施時間をCSV出力\n",
    "def export_subtask_duration_csv(mouse_id):\n",
    "    # TODO failure記録回数が一回少ない？\n",
    "    if not isinstance(mouse_id,list):\n",
    "        mouse_id = [mouse_id]\n",
    "    for no in mouse_id:\n",
    "        file = \"./no{:03d}_action.csv\".format(no)\n",
    "        data = pd.read_csv(file,names=[\"timestamps\", \"task\", \"session_id\", \"correct_times\", \"event_type\", \"hole_no\"],parse_dates=[0])\n",
    "        if isinstance(data.iloc[0].timestamps, str):\n",
    "            data = pd.read_csv(file,parse_dates=[0])\n",
    "            data.columns = [\"timestamps\", \"task\", \"session_id\", \"correct_times\", \"event_type\", \"hole_no\"]\n",
    "        data = data[[\"timestamps\",\"event_type\",\"task\",\"hole_no\"]]\n",
    "        tasks = data.task.unique()\n",
    "        print(tasks)\n",
    "        event_times = pd.pivot_table(data[data.event_type.isin([\"reward\",\"failure\",\"time over\"])],index=\"event_type\",columns=\"task\",aggfunc=\"count\").timestamps\n",
    "        task_duration = pd.DataFrame(data.groupby(\"task\").timestamps.max()-data.groupby(\"task\").timestamps.min())\n",
    "        task_duration.timestamps = task_duration.timestamps / np.timedelta64(1, 'h')\n",
    "        ret_val = event_times.append(task_duration.T).fillna(0)\n",
    "        ret_val = ret_val.rename(index={\"timestamps\":\"duration in hours\"})\n",
    "        # 列の順番をtaskをやった順でソート\n",
    "        ret_val = ret_val.loc[:,tasks]\n",
    "        ret_val.to_csv(\"./data/no{:03d}_summary.csv\".format(no))\n",
    "        # 各タスクの各選択肢毎のerror数, correct数, total trial数を出したい\n",
    "        tasks_df = []\n",
    "        for task in tasks: \n",
    "            tasks_data = data[data.task.isin([task])]\n",
    "            tasks_df.append(tasks_data[tasks_data.event_type.isin([\"failure\"])])\n",
    "            tasks_data = pd.pivot_table(tasks_data[tasks_data.event_type.isin([\"reward\",\"failure\",\"time over\"])],index=\"event_type\",columns=\"hole_no\",aggfunc=\"count\").timestamps.fillna(0)\n",
    "            tasks_data.loc[\"total_trials\"] = tasks_data.sum()\n",
    "            tasks_data.to_csv(\"./data/no{:03d}_{}_selection_trials.csv\".format(no,task))\n",
    "        # return tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['T0' 'left' 'right' 'L30-R60']\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "                     timestamps event_type  task hole_no\n1275 2020-08-23 18:44:47.089154    failure  left       2\n1639 2020-08-23 19:51:40.802861    failure  left       2\n1656 2020-08-23 20:03:24.033623    failure  left       2\n1696 2020-08-23 20:45:31.557445    failure  left       1\n1707 2020-08-23 20:46:21.379405    failure  left       2\n1745 2020-08-23 21:19:32.305930    failure  left       2\n2138 2020-08-23 21:36:00.740733    failure  left       1\n2161 2020-08-23 21:36:56.687071    failure  left       1\n2217 2020-08-23 21:43:38.664535    failure  left       2\n2244 2020-08-23 21:44:19.868667    failure  left       2\n2454 2020-08-23 21:48:10.220334    failure  left       2\n2530 2020-08-23 21:53:25.280216    failure  left       2\n2574 2020-08-23 21:55:51.768969    failure  left       1\n2591 2020-08-23 21:57:08.383146    failure  left       1\n2608 2020-08-23 21:57:36.875620    failure  left       2\n2708 2020-08-23 22:02:34.179425    failure  left       2\n2721 2020-08-23 22:03:06.706207    failure  left       2\n2736 2020-08-23 22:03:49.508320    failure  left       2\n2764 2020-08-23 22:04:38.539944    failure  left       2\n2780 2020-08-23 22:05:13.566335    failure  left       2\n2863 2020-08-23 22:09:13.971166    failure  left       1\n2882 2020-08-23 22:09:41.370306    failure  left       1\n2923 2020-08-23 22:12:04.881527    failure  left       2\n2947 2020-08-23 22:12:31.647912    failure  left       2\n2965 2020-08-23 22:12:59.117657    failure  left       2\n3009 2020-08-23 22:13:40.329935    failure  left       2\n3045 2020-08-23 22:14:12.120322    failure  left       2\n3129 2020-08-23 22:19:45.053680    failure  left       1\n3149 2020-08-23 22:20:21.600369    failure  left       2\n3198 2020-08-23 22:24:17.909070    failure  left       1\n...                         ...        ...   ...     ...\n6771 2020-08-24 20:25:58.867988    failure  left       1\n6797 2020-08-24 20:29:47.360845    failure  left       1\n6837 2020-08-24 20:37:55.437689    failure  left       1\n6960 2020-08-24 20:57:02.724401    failure  left       1\n6971 2020-08-24 21:02:18.699421    failure  left       1\n6997 2020-08-24 21:03:27.305995    failure  left       1\n7010 2020-08-24 21:04:44.669917    failure  left       1\n7039 2020-08-24 21:08:30.167455    failure  left       1\n7110 2020-08-24 21:17:03.702901    failure  left       1\n7166 2020-08-24 21:22:02.580291    failure  left       1\n7195 2020-08-24 21:22:53.293146    failure  left       1\n7206 2020-08-24 21:27:26.805723    failure  left       1\n7218 2020-08-24 21:27:56.329198    failure  left       1\n7230 2020-08-24 21:28:42.990318    failure  left       1\n7262 2020-08-24 21:48:32.548886    failure  left       1\n7273 2020-08-24 21:54:20.442010    failure  left       1\n7286 2020-08-24 21:54:50.068903    failure  left       1\n7298 2020-08-24 21:57:07.623264    failure  left       1\n8129 2020-08-25 07:00:11.356597    failure  left       1\n8167 2020-08-25 07:01:37.996570    failure  left       1\n8198 2020-08-25 07:03:17.216902    failure  left       1\n8212 2020-08-25 07:04:17.657289    failure  left       1\n8223 2020-08-25 07:05:07.682212    failure  left       1\n8233 2020-08-25 07:08:54.630268    failure  left       1\n8246 2020-08-25 07:09:32.741486    failure  left       1\n8260 2020-08-25 07:14:16.596007    failure  left       1\n8287 2020-08-25 07:17:22.734430    failure  left       1\n8302 2020-08-25 07:17:50.105365    failure  left       1\n8328 2020-08-25 07:19:02.281319    failure  left       1\n8339 2020-08-25 07:20:09.941601    failure  left       1\n\n[168 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamps</th>\n      <th>event_type</th>\n      <th>task</th>\n      <th>hole_no</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1275</th>\n      <td>2020-08-23 18:44:47.089154</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1639</th>\n      <td>2020-08-23 19:51:40.802861</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1656</th>\n      <td>2020-08-23 20:03:24.033623</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1696</th>\n      <td>2020-08-23 20:45:31.557445</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1707</th>\n      <td>2020-08-23 20:46:21.379405</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1745</th>\n      <td>2020-08-23 21:19:32.305930</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2138</th>\n      <td>2020-08-23 21:36:00.740733</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2161</th>\n      <td>2020-08-23 21:36:56.687071</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2217</th>\n      <td>2020-08-23 21:43:38.664535</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2244</th>\n      <td>2020-08-23 21:44:19.868667</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2454</th>\n      <td>2020-08-23 21:48:10.220334</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2530</th>\n      <td>2020-08-23 21:53:25.280216</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2574</th>\n      <td>2020-08-23 21:55:51.768969</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2591</th>\n      <td>2020-08-23 21:57:08.383146</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2608</th>\n      <td>2020-08-23 21:57:36.875620</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2708</th>\n      <td>2020-08-23 22:02:34.179425</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2721</th>\n      <td>2020-08-23 22:03:06.706207</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2736</th>\n      <td>2020-08-23 22:03:49.508320</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2764</th>\n      <td>2020-08-23 22:04:38.539944</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2780</th>\n      <td>2020-08-23 22:05:13.566335</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2863</th>\n      <td>2020-08-23 22:09:13.971166</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2882</th>\n      <td>2020-08-23 22:09:41.370306</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2923</th>\n      <td>2020-08-23 22:12:04.881527</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2947</th>\n      <td>2020-08-23 22:12:31.647912</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2965</th>\n      <td>2020-08-23 22:12:59.117657</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3009</th>\n      <td>2020-08-23 22:13:40.329935</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3045</th>\n      <td>2020-08-23 22:14:12.120322</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3129</th>\n      <td>2020-08-23 22:19:45.053680</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3149</th>\n      <td>2020-08-23 22:20:21.600369</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3198</th>\n      <td>2020-08-23 22:24:17.909070</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6771</th>\n      <td>2020-08-24 20:25:58.867988</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6797</th>\n      <td>2020-08-24 20:29:47.360845</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6837</th>\n      <td>2020-08-24 20:37:55.437689</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6960</th>\n      <td>2020-08-24 20:57:02.724401</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6971</th>\n      <td>2020-08-24 21:02:18.699421</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6997</th>\n      <td>2020-08-24 21:03:27.305995</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7010</th>\n      <td>2020-08-24 21:04:44.669917</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7039</th>\n      <td>2020-08-24 21:08:30.167455</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7110</th>\n      <td>2020-08-24 21:17:03.702901</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7166</th>\n      <td>2020-08-24 21:22:02.580291</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7195</th>\n      <td>2020-08-24 21:22:53.293146</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7206</th>\n      <td>2020-08-24 21:27:26.805723</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7218</th>\n      <td>2020-08-24 21:27:56.329198</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7230</th>\n      <td>2020-08-24 21:28:42.990318</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7262</th>\n      <td>2020-08-24 21:48:32.548886</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7273</th>\n      <td>2020-08-24 21:54:20.442010</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7286</th>\n      <td>2020-08-24 21:54:50.068903</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7298</th>\n      <td>2020-08-24 21:57:07.623264</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8129</th>\n      <td>2020-08-25 07:00:11.356597</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8167</th>\n      <td>2020-08-25 07:01:37.996570</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8198</th>\n      <td>2020-08-25 07:03:17.216902</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8212</th>\n      <td>2020-08-25 07:04:17.657289</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8223</th>\n      <td>2020-08-25 07:05:07.682212</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8233</th>\n      <td>2020-08-25 07:08:54.630268</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8246</th>\n      <td>2020-08-25 07:09:32.741486</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8260</th>\n      <td>2020-08-25 07:14:16.596007</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8287</th>\n      <td>2020-08-25 07:17:22.734430</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8302</th>\n      <td>2020-08-25 07:17:50.105365</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8328</th>\n      <td>2020-08-25 07:19:02.281319</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8339</th>\n      <td>2020-08-25 07:20:09.941601</td>\n      <td>failure</td>\n      <td>left</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>168 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 27
    }
   ],
   "source": [
    "a = export_subtask_duration_csv(145)\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a[1].to_csv(\"./data/no145_left_fail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}