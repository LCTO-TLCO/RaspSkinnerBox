{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as collections\n",
    "import matplotlib.markers as markers\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# debug = True\n",
    "debug = False\n",
    "\n",
    "\n",
    "class task_data:\n",
    "    def __init__(self, mice: list, tasks, logpath):\n",
    "        global debug\n",
    "        self.data_file = \"\"\n",
    "        self.data = None\n",
    "        self.data_ci = None\n",
    "        self.delta = None\n",
    "        self.mouse_no = mice\n",
    "        self.tasks = tasks\n",
    "        self.pattern_prob = {}\n",
    "        self.probability = None\n",
    "        self.mice_task = None\n",
    "        self.task_prob = {}\n",
    "        self.mice_delta = {}\n",
    "        self.entropy_analyze = None\n",
    "        self.mice_entropy = None\n",
    "        self.logpath = logpath\n",
    "        self.session_id = 0\n",
    "        self.burst_id = 0\n",
    "        self.data_not_omission = None\n",
    "        self.fig_prob_tmp = None\n",
    "        self.fig_prob = {}\n",
    "        self.bit = 4\n",
    "\n",
    "        print('reading data...', end='')\n",
    "\n",
    "        # TODO debug\n",
    "        # 1.self.data, 2.probability, 3.task_prob, 4.self.delta, 5.self.fig_prob_tmp, 6.pattern, 7.self.entropy_analyze\n",
    "        # to:dict, add:dict[task]\n",
    "        # to:dict, add:dict(fig) or DF\n",
    "        def append_dataframe(to: Union[pd.DataFrame, dict, None], add: Union[pd.DataFrame, dict, None], mouse_id: int,\n",
    "                             task=None, fig_num=None):\n",
    "            if isinstance(add, dict):\n",
    "\n",
    "                # 二回目の場合Fig確定\n",
    "                if not isinstance(task, type(None)):\n",
    "                    # print(add)\n",
    "                    ret_val = {}\n",
    "                    [ret_val.update(append_dataframe(to.get(task, {}), add[fig], mouse_id, task=task, fig_num=fig)) for\n",
    "                     fig in [\"fig1\", \"fig2\", \"fig3\"]]\n",
    "                    return {task: ret_val}\n",
    "                # taskごと\n",
    "                # to;dict, add:dict[task]\n",
    "                for add_task, add_dict in add.items():\n",
    "                    # append_dataframe(to, append_dataframe(to, add_dict, mouse_id, task=add_task), mouse_id)\n",
    "                    to.update(append_dataframe(to, add_dict, mouse_id, task=add_task))\n",
    "                return to\n",
    "            if isinstance(to, dict):\n",
    "                # Fig二回目入力\n",
    "                if not isinstance(fig_num, type(None)):\n",
    "                    return {fig_num: append_dataframe(to.get(fig_num, None), add, mouse_id)}\n",
    "                return {\n",
    "                    task: append_dataframe(to.get(task, None), add, mouse_id)}\n",
    "            if isinstance(to, type(None)):\n",
    "                return add.assign(mouse_id=mouse_id)\n",
    "            else:\n",
    "                return to.append(add.assign(mouse_id=mouse_id), ignore_index=True)\n",
    "\n",
    "        if debug:\n",
    "            for mouse_id in self.mouse_no:\n",
    "                print('mouse_id={}'.format(mouse_id))\n",
    "                self.mice_task[mouse_id], self.probability[mouse_id], self.task_prob[mouse_id], self.mice_delta[\n",
    "                    mouse_id], self.fig_prob[mouse_id] = self.dev_read_data(mouse_id)\n",
    "                # tmp = self.dev_read_data(mouse_id)\n",
    "                # self.mice_task = append_dataframe(self.mice_task, tmp[0], mouse_id)\n",
    "                # self.probability = append_dataframe(self.probability, tmp[1], mouse_id)\n",
    "                # self.task_prob = append_dataframe(self.task_prob, tmp[2], mouse_id)\n",
    "                # self.mice_delta = append_dataframe(self.mice_delta, tmp[3], mouse_id)\n",
    "                # # append_dataframe(self.fig_prob, tmp[4], mouse_id)\n",
    "                # self.fig_prob[mouse_id] = self.fig_prob[mouse_id].append(tmp[4])\n",
    "                # self.pattern_prob = append_dataframe(self.pattern_prob, tmp[5], mouse_id)\n",
    "                # TODO entropy_analyze\n",
    "        else:\n",
    "            # all 0: 2.probability\n",
    "            # 3.task_prob, 4.self.delta, 5.self.fig_prob_tmp, 6.pattern\n",
    "            for mouse_id in self.mouse_no:\n",
    "                try:\n",
    "                    print('mouse_id={}'.format(mouse_id))\n",
    "                    # self.data_file = \"{}no{:03d}_action.csv\".format(self.logpath, mouse_id)\n",
    "                    # self.mice_task[mouse_id], self.probability[mouse_id], self.task_prob[mouse_id], self.mice_delta[\n",
    "                    #     mouse_id], self.fig_prob[mouse_id], self.pattern_prob[mouse_id] = self.read_data()\n",
    "                    self.data_file = os.path.join(self.logpath, \"no{:03d}_action.csv\".format(mouse_id))\n",
    "                    tmp = self.read_data()\n",
    "                    self.mice_task = append_dataframe(self.mice_task, tmp[0], mouse_id)\n",
    "                    # 0\n",
    "#                     self.probability = append_dataframe(self.probability, tmp[1], mouse_id)\n",
    "                    self.task_prob = append_dataframe(self.task_prob, tmp[2], mouse_id)\n",
    "#                     self.mice_delta = append_dataframe(self.mice_delta, tmp[3], mouse_id)\n",
    "                    # 単体\n",
    "#                     self.fig_prob = append_dataframe(self.fig_prob, tmp[4], mouse_id)\n",
    "#                     self.pattern_prob = append_dataframe(self.pattern_prob, tmp[5], mouse_id)\n",
    "#                     self.mice_entropy = append_dataframe(self.mice_entropy, tmp[6], mouse_id)\n",
    "                except Exception as e:\n",
    "                    print(\"error! no {}\".format(mouse_id))\n",
    "                    print(e)\n",
    "                    continue\n",
    "            self.export_csv()\n",
    "        print('done')\n",
    "\n",
    "    def read_data(self):\n",
    "\n",
    "        def rehash_session_id():\n",
    "            data = pd.read_csv(self.data_file, names=header, parse_dates=[0], dtype={'hole_no': 'str'})\n",
    "            self.session_id = 0\n",
    "            print(\"max_id_col:{}\".format(len(data)))\n",
    "\n",
    "            def remove_terminate(index):\n",
    "                if data.at[index, \"event_type\"] == data.at[index + 1, \"event_type\"] and data.at[\n",
    "                    index, \"event_type\"] == \"start\":\n",
    "                    data.drop(index, inplace=True)\n",
    "\n",
    "            def rehash(x_index):\n",
    "                start_task = data.head(1).task.values[0]\n",
    "                if data.at[data.index[x_index], \"task\"] == start_task:\n",
    "                    if (x_index == 0 or data.shift(1).at[data.index[x_index], \"event_type\"] == \"start\") and \\\n",
    "                            len(data[:x_index][data.session_id == 0]) == 0:\n",
    "                        self.session_id = 0\n",
    "                        return 0\n",
    "                    self.session_id = self.session_id + 1\n",
    "                    return self.session_id\n",
    "                if data.at[data.index[x_index], \"event_type\"] == \"start\":\n",
    "                    self.session_id = self.session_id + 1\n",
    "                    return self.session_id\n",
    "                else:\n",
    "                    return self.session_id\n",
    "\n",
    "            list(map(remove_terminate, data.index[:-1]))\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            data[\"session_id\"] = list(map(rehash, data.index))\n",
    "            data = data[\n",
    "                data.session_id.isin(data.session_id[data.event_type.isin([\"reward\", \"failure\", \"time over\"])])]\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            self.session_id = 0\n",
    "            data[\"session_id\"] = list(map(rehash, data.index))\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "            return data\n",
    "\n",
    "        def add_timedelta():\n",
    "            data = self.data\n",
    "            data = data[data.session_id.isin(data[data.event_type.isin(['reward', 'failure'])][\"session_id\"])]\n",
    "            deltas = {}\n",
    "            for task in self.tasks:\n",
    "                def calculate(session):\n",
    "                    delta_df = pd.DataFrame()\n",
    "                    # reaction time\n",
    "                    current_target = data[data.session_id.isin([session])]\n",
    "                    if bool(sum(current_target[\"event_type\"].isin([\"task called\"]))):\n",
    "                        task_call = current_target[current_target[\"event_type\"] == \"task called\"]\n",
    "                        task_end = current_target[current_target[\"event_type\"].isin([\"nose poke\", \"failure\"])]\n",
    "                        reaction_time = task_end.at[task_end.index[0], \"timestamps\"] - task_call.at[\n",
    "                            task_call.index[0], \"timestamps\"]\n",
    "                        # 連続無報酬期間\n",
    "                        previous_reward = data[\n",
    "                            (data[\"event_type\"] == \"reward\") & (\n",
    "                                    data[\"timestamps\"] < task_call.at[task_call.index[0], \"timestamps\"])].tail(1)\n",
    "                        norewarded_time = task_call.at[task_call.index[0], \"timestamps\"] - previous_reward.at[\n",
    "                            previous_reward.index[0], \"timestamps\"]\n",
    "                        correct_failure = \"correct\" if bool(\n",
    "                            sum(current_target[\"event_type\"].isin([\"reward\"]))) else \"failure\"\n",
    "                        # df 追加\n",
    "                        delta_df = delta_df.append(\n",
    "                            {'session_id': session,\n",
    "                             'type': 'reaction_time',\n",
    "                             'noreward_duration_sec': pd.to_timedelta(norewarded_time) / np.timedelta64(1, 's'),\n",
    "                             'reaction_time_sec': pd.to_timedelta(reaction_time) / np.timedelta64(1, 's'),\n",
    "                             'correct_failure': correct_failure},\n",
    "                            ignore_index=True)\n",
    "                    # reward latency\n",
    "                    if bool(sum(current_target[\"event_type\"].isin([\"reward\"]))) and bool(\n",
    "                            sum(current_target[\"event_type\"].isin([\"task called\"]))):\n",
    "                        nose_poke = current_target[current_target[\"event_type\"] == \"nose poke\"]\n",
    "                        reward_latency = current_target[current_target[\"event_type\"] == \"magazine nose poked\"]\n",
    "                        reward_latency = reward_latency.at[reward_latency.index[0], \"timestamps\"] - \\\n",
    "                                         nose_poke.at[nose_poke.index[0], \"timestamps\"]\n",
    "                        previous_reward = data[\n",
    "                            (data[\"event_type\"] == \"reward\") & (\n",
    "                                    data[\"timestamps\"] < nose_poke.at[nose_poke.index[0], \"timestamps\"])].tail(1)\n",
    "                        norewarded_time = nose_poke.at[nose_poke.index[0], \"timestamps\"] - previous_reward.at[\n",
    "                            previous_reward.index[0], \"timestamps\"]\n",
    "                        delta_df = delta_df.append(\n",
    "                            {'session_id': session,\n",
    "                             'type': 'reward_latency',\n",
    "                             'noreward_duration_sec': pd.to_timedelta(norewarded_time) / np.timedelta64(1, 's'),\n",
    "                             'reward_latency_sec': pd.to_timedelta(reward_latency) / np.timedelta64(1, 's')\n",
    "                             }, ignore_index=True)\n",
    "                    return delta_df\n",
    "\n",
    "                delta_df = data[data.task == task].session_id.drop_duplicates().map(calculate)\n",
    "                deltas[task] = pd.concat(list(delta_df), sort=False) if len(delta_df) else delta_df\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "            return deltas\n",
    "\n",
    "        def add_hot_vector():\n",
    "            #    data = data[data[\"event_type\"].isin([\"reward\", \"failure\", \"time over\"])]\n",
    "            data = self.data\n",
    "            # data = data[data[\".seevent_type\"].isin([\"reward\", \"failure\", \"time over\"])]\n",
    "            # data = data[data[\"task\"].isin(self.tasks)]\n",
    "\n",
    "            data = data.reset_index(drop=True)\n",
    "            # task interval\n",
    "            task_start_index = [0]\n",
    "            for i in range(1, len(data)):\n",
    "                if not data[\"task\"][i] == data[\"task\"][i - 1]:\n",
    "                    task_start_index.append(i)\n",
    "\n",
    "            data[\"hole_correct\"] = -1\n",
    "            data[\"hole_failure\"] = -1\n",
    "            data[\"is_correct\"] = -1\n",
    "            data[\"is_failure\"] = -1\n",
    "            data[\"is_omission\"] = -1\n",
    "            data[\"cumsum_correct\"] = -1\n",
    "            data[\"cumsum_failure\"] = -1\n",
    "            data[\"cumsum_omission\"] = -1\n",
    "            data[\"cumsum_correct_taskreset\"] = -1\n",
    "            data[\"cumsum_failure_taskreset\"] = -1\n",
    "            data[\"cumsum_omission_taskreset\"] = -1\n",
    "            for hole_no in range(1, 9 + 1, 2):\n",
    "                data[\"is_hole{}\".format(str(hole_no))] = -1\n",
    "\n",
    "            # print(data)\n",
    "\n",
    "            # hole_information\n",
    "            # warning mettyaderu zone\n",
    "            # SettingWithCopyWarning\n",
    "            data.loc[data[\"event_type\"].isin([\"reward\"]), 'hole_correct'] = data[\"hole_no\"]\n",
    "            data.loc[~data[\"event_type\"].isin([\"reward\"]), 'hole_correct'] = np.nan\n",
    "            data.loc[data[\"event_type\"].isin([\"failure\"]), 'hole_failure'] = data[\"hole_no\"]\n",
    "            data.loc[~data[\"event_type\"].isin([\"failure\"]), 'hole_failure'] = np.nan\n",
    "\n",
    "            data.loc[data[\"event_type\"].isin([\"reward\"]), 'is_correct'] = 1\n",
    "            data.loc[~data[\"event_type\"].isin([\"reward\"]), 'is_correct'] = np.nan\n",
    "            data.loc[data[\"event_type\"].isin([\"failure\"]), 'is_failure'] = 1\n",
    "            data.loc[~data[\"event_type\"].isin([\"failure\"]), 'is_failure'] = np.nan\n",
    "            data.loc[data[\"event_type\"].isin([\"time over\"]), 'is_omission'] = 1\n",
    "            data.loc[~data[\"event_type\"].isin([\"time over\"]), 'is_omission'] = np.nan\n",
    "\n",
    "            data[\"cumsum_correct\"] = data[\"is_correct\"].cumsum(axis=0)\n",
    "            data[\"cumsum_failure\"] = data[\"is_failure\"].cumsum(axis=0)\n",
    "            data[\"cumsum_omission\"] = data[\"is_omission\"].cumsum(axis=0)\n",
    "\n",
    "            for hole_no in range(1, 9 + 1, 2):\n",
    "                data.loc[data['hole_no'] == str(hole_no), \"is_hole{}\".format(hole_no)] = 1\n",
    "                data.loc[~(data['hole_no'] == str(hole_no)), \"is_hole{}\".format(hole_no)] = None\n",
    "\n",
    "            # cumsum\n",
    "            # for i in range(0, len(task_start_index)):\n",
    "            #     index_start = task_start_index[i]\n",
    "            #     index_end = len(data)\n",
    "            #     if i < len(task_start_index) - 1:\n",
    "            #         index_end = task_start_index[i + 1]\n",
    "            #     pre_correct = data[\"cumsum_correct\"][index_start] if not i == 0 else 0\n",
    "            #     pre_incorrect = data[\"cumsum_incorrect\"][index_start] if not i == 0 else 0\n",
    "            #     pre_omission = data[\"cumsum_omission\"][index_start] if not i == 0 else 0\n",
    "            #     # warning mettyaderu zone\n",
    "            #     data[\"cumsum_correct_taskreset\"][index_start:index_end] = data[\"cumsum_correct\"][\n",
    "            #                                                               index_start:index_end] - \\\n",
    "            #                                                               pre_correct\n",
    "            #     data[\"cumsum_incorrect_taskreset\"][index_start:index_end] = data[\"cumsum_incorrect\"][\n",
    "            #                                                                 index_start:index_end] - \\\n",
    "            #                                                                 pre_incorrect\n",
    "            #     data[\"cumsum_omission_taskreset\"][index_start:index_end] = data[\"cumsum_omission\"][\n",
    "            #                                                                index_start:index_end] - \\\n",
    "            #                                                                pre_omission\n",
    "            def add_cumsum():\n",
    "                data[\"cumsum_correct_taskreset\"] = data[\"is_correct\"].fillna(0)\n",
    "                data[\"cumsum_failure_taskreset\"] = data[\"is_failure\"].fillna(0)\n",
    "                data[\"cumsum_omission_taskreset\"] = data[\"is_omission\"].fillna(0)\n",
    "                data[\"cumsum_correct_taskreset\"] = data.groupby(\"task\")[\"cumsum_correct_taskreset\"].cumsum()\n",
    "                data[\"cumsum_failure_taskreset\"] = data.groupby(\"task\")[\"cumsum_failure_taskreset\"].cumsum()\n",
    "                data[\"cumsum_omission_taskreset\"] = data.groupby(\"task\")[\"cumsum_omission_taskreset\"].cumsum()\n",
    "\n",
    "#             add_cumsum()\n",
    "\n",
    "            # burst\n",
    "            # data[\"burst_group\"] = 1\n",
    "            # for i in range(1, len(data)):\n",
    "            #     if data[\"timestamps\"][i] - data[\"timestamps\"][i - 1] <= datetime.timedelta(seconds=60):\n",
    "            #         data[\"burst_group\"][i] = data[\"burst_group\"][i - 1]\n",
    "            #         continue\n",
    "            #     data[\"burst_group\"][i] = data[\"burst_group\"][i - 1] + 1\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "            return data\n",
    "\n",
    "        def calc_entropy(section=150):\n",
    "            data = self.data[self.data.event_type.isin(['reward', 'failure'])]\n",
    "\n",
    "            def min_max(x, axis=None):\n",
    "                np.array(x)\n",
    "                min = np.array(x).min(axis=axis)\n",
    "                max = np.array(x).max(axis=axis)\n",
    "                result = (x - min) / (max - min)\n",
    "                return result\n",
    "\n",
    "            # entropy\n",
    "            ent = [np.nan] * section\n",
    "            for i in range(0, len(data) - section):\n",
    "                denominator = float(section)\n",
    "                # sum([data[\"is_hole{}\".format(str(hole_no))][i:i + 150].sum() for hole_no in range(1, 9 + 1, 2)])\n",
    "                current_entropy = min_max(\n",
    "                    [data[\"is_hole{}\".format(str(hole_no))][i:i + section].sum() /\n",
    "                     denominator for hole_no in [1, 3, 5, 7, 9]])\n",
    "                ent.append(entropy(current_entropy, base=2))\n",
    "            # region Description\n",
    "            # data[data.event_type.isin(['reward', 'failure'])][\"hole_choice_entropy\"] = ent\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "            return pd.DataFrame(ent).fillna(0.0).values.tolist()\n",
    "            # endregion\n",
    "\n",
    "        def count_task() -> dict:\n",
    "            dc = self.data[self.data[\"event_type\"].isin([\"reward\", \"failure\"])]\n",
    "            # dc = self.data[self.data[\"event_type\"].isin([\"reward\", \"failure\", \"time over\"])]\n",
    "\n",
    "            dc = dc.reset_index()\n",
    "\n",
    "            after_c_all_task = {}\n",
    "            after_f_all_task = {}\n",
    "\n",
    "            after_c_starts_task = {}\n",
    "            after_f_starts_task = {}\n",
    "\n",
    "            prob_index = [\"c_same\", \"c_diff\", \"c_omit\", \"c_checksum\", \"f_same\", \"f_diff\", \"f_omit\", \"f_checksum\",\n",
    "                          \"c_NotMax\",\n",
    "                          \"f_NotMax\", \"o_NotMax\"]\n",
    "            forward_trace = 10\n",
    "\n",
    "            for task in self.tasks:\n",
    "                after_c_starts_task[task] = dc[(dc[\"is_correct\"] == 1) & (dc[\"task\"] == task)]\n",
    "                after_f_starts_task[task] = dc[(dc[\"is_failure\"] == 1) & (dc[\"task\"] == task)]\n",
    "                after_c_all_task[task] = float(len(after_c_starts_task[task]))\n",
    "                after_f_all_task[task] = float(len(after_f_starts_task[task]))\n",
    "\n",
    "                prob = pd.DataFrame(columns=prob_index, index=range(1, forward_trace)).fillna(0.0)\n",
    "                # correctスタート\n",
    "                for idx, dt in after_c_starts_task[task].iterrows():\n",
    "                    for j in range(1, min(forward_trace, len(dc) - idx)):\n",
    "                        #                    for j in range(1, min(forward_trace, len(self.data_cio) - idx)):\n",
    "                        # 報酬を得たときと同じ選択(CF両方)をしたときの処理\n",
    "                        if dt[\"hole_no\"] == dc[\"hole_no\"][idx + j]:\n",
    "                            prob[\"c_same\"][j] = prob[\"c_same\"][j] + 1\n",
    "                        # omissionの場合\n",
    "                        elif dc[\"is_omission\"][idx + j] == 1:\n",
    "                            prob[\"c_omit\"][j] = prob[\"c_omit\"][j] + 1\n",
    "                        elif dt[\"hole_no\"] != dc[\"hole_no\"][idx + j]:\n",
    "                            prob[\"c_diff\"][j] = prob[\"c_diff\"][j] + 1\n",
    "\n",
    "                # incorrectスタート\n",
    "                for idx, dt in after_f_starts_task[task].iterrows():\n",
    "                    for j in range(1, min(forward_trace, len(dc) - idx)):\n",
    "                        #                    for j in range(1, min(forward_trace, len(self.data_cio) - idx)):\n",
    "                        if dt[\"hole_no\"] == dc[\"hole_no\"][idx + j]:\n",
    "                            prob[\"f_same\"][j] = prob[\"f_same\"][j] + 1\n",
    "                        elif dc[\"is_omission\"][idx + j] == 1:\n",
    "                            prob[\"f_omit\"][j] = prob[\"f_omit\"][j] + 1\n",
    "                        elif dt[\"hole_no\"] != dc[\"hole_no\"][idx + j]:\n",
    "                            prob[\"f_diff\"][j] = prob[\"f_diff\"][j] + 1\n",
    "\n",
    "                # calculate\n",
    "                prob[\"c_same\"] = prob[\"c_same\"] / after_c_all_task[task] if not after_c_all_task[task] == 0 else 0.0\n",
    "                prob[\"c_diff\"] = prob[\"c_diff\"] / after_c_all_task[task] if not after_c_all_task[task] == 0 else 0.0\n",
    "                prob[\"c_omit\"] = prob[\"c_omit\"] / after_c_all_task[task] if not after_c_all_task[task] == 0 else 0.0\n",
    "                prob[\"c_checksum\"] = prob[\"c_same\"] + prob[\"c_diff\"] + prob[\"c_omit\"]\n",
    "                prob[\"f_same\"] = prob[\"f_same\"] / after_f_all_task[task] if not after_f_all_task[task] == 0 else 0.0\n",
    "                prob[\"f_diff\"] = prob[\"f_diff\"] / after_f_all_task[task] if not after_f_all_task[task] == 0 else 0.0\n",
    "                prob[\"f_omit\"] = prob[\"f_omit\"] / after_f_all_task[task] if not after_f_all_task[task] == 0 else 0.0\n",
    "                prob[\"f_checksum\"] = prob[\"f_same\"] + prob[\"f_diff\"] + prob[\"f_omit\"]\n",
    "\n",
    "                task_prob[task] = prob\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "\n",
    "        # TODO 結構な確率でエラー吐く\n",
    "        def analyze_pattern(bit=self.bit):\n",
    "            fig_prob = {}\n",
    "            pattern_range = range(0, pow(2, bit))\n",
    "            for task in self.tasks:\n",
    "                pattern[task] = {}\n",
    "                fig_prob[task] = {\"fig1\": pd.DataFrame(columns=[\"{:b}\".format(i).zfill(bit) for i in pattern_range]\n",
    "                                                       ).fillna(0.0),\n",
    "                                  \"fig2\": pd.DataFrame(columns=[\"{:b}\".format(i).zfill(bit) for i in pattern_range]\n",
    "                                                       ).fillna(0.0),\n",
    "                                  \"fig3\": pd.DataFrame(columns=[\"{:b}\".format(i).zfill(bit) for i in pattern_range],\n",
    "                                                       ).fillna(0.0)}\n",
    "                data = self.data[\n",
    "                    (self.data.task == task) & (\n",
    "                        self.data.event_type.isin([\"reward\", \"failure\", \"time over\"]))].reset_index(drop=True)\n",
    "                data_ci = data[data.event_type.isin([\"reward\", \"failure\"])].reset_index(drop=True)\n",
    "                # search pattern\n",
    "\n",
    "                f_pattern_matching = lambda x: sum([\n",
    "                    (not np.isnan(data_ci.at[x + (bit - i - 1), \"is_correct\"])) * pow(2, i)\n",
    "                    for i in range(0, bit)])\n",
    "                pattern[task] = data_ci[:-(bit - 1)].assign(pattern=data_ci[:-(bit - 1)].index.map(f_pattern_matching))\n",
    "                # count\n",
    "\n",
    "                f_same_base = lambda x: [data_ci.at[data_ci[data_ci.session_id == x].index[0], \"hole_no\"] == \\\n",
    "                                         data_ci.at[data_ci[data_ci.session_id == x].index[0] + idx, \"hole_no\"] for idx\n",
    "                                         in range(1, bit)]\n",
    "                f_same_prev = lambda x: [data_ci.at[data_ci[data_ci.session_id == x].index[0] + idx - 1, \"hole_no\"] == \\\n",
    "                                         data_ci.at[data_ci[data_ci.session_id == x].index[0] + idx, \"hole_no\"] for idx\n",
    "                                         in range(1, bit)]\n",
    "                f_omit = lambda x: [bool(data.at[data[data.session_id == x].index[0] + idx, \"is_omission\"]) for idx in\n",
    "                                    range(1, bit)]\n",
    "                functions = lambda x: [f_same_base(x), f_same_prev(x), f_omit(x)]\n",
    "                # pattern count -> probability\n",
    "                for pat_tmp in pattern_range:\n",
    "                    f_p = pd.DataFrame(list(pattern[task][pattern[task].pattern == pat_tmp].session_id.map(functions)),\n",
    "                                       columns=[\"fig1\", \"fig2\", \"fig3\"]).fillna(0.0)\n",
    "                    if len(f_p):\n",
    "                        fig_prob[task][\"fig1\"][\"{:b}\".format(pat_tmp).zfill(bit)] = pd.DataFrame(\n",
    "                            list(f_p.fig1)).sum().fillna(0.0) / len(pattern[task][pattern[task].pattern == pat_tmp])\n",
    "                        fig_prob[task][\"fig2\"][\"{:b}\".format(pat_tmp).zfill(bit)] = pd.DataFrame(\n",
    "                            list(f_p.fig2)).sum().fillna(0.0) / len(pattern[task][pattern[task].pattern == pat_tmp])\n",
    "                        fig_prob[task][\"fig3\"][\"{:b}\".format(pat_tmp).zfill(bit)] = pd.DataFrame(\n",
    "                            list(f_p.fig3)).sum().fillna(0.0) / len(pattern[task][pattern[task].pattern == pat_tmp])\n",
    "                    else:\n",
    "                        for figure in list(f_p.columns):\n",
    "                            fig_prob[task][figure][\"{:b}\".format(pat_tmp).zfill(bit)] = fig_prob[task][figure][\n",
    "                                \"{:b}\".format(pat_tmp).zfill(bit)].fillna(0.0)\n",
    "                    for figure in list(f_p.columns):\n",
    "                        fig_prob[task][figure].at[\"n\", \"{:b}\".format(pat_tmp).zfill(bit)] = len(\n",
    "                            pattern[task][pattern[task].pattern == pat_tmp])\n",
    "            # save\n",
    "            self.fig_prob_tmp = fig_prob\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "            return pattern\n",
    "\n",
    "        def burst():\n",
    "            def calc_burst(session):\n",
    "                if session == 0:\n",
    "                    self.burst_id = 0\n",
    "                    return self.burst_id\n",
    "                if data.at[data.index[data.session_id == session][0], \"timestamps\"] - \\\n",
    "                        data.at[data.index[data.session_id == session - 1][0], \"timestamps\"] >= timedelta(\n",
    "                    seconds=60):\n",
    "                    self.burst_id = self.burst_id + 1\n",
    "                return self.burst_id\n",
    "\n",
    "            data = self.data[self.data.event_type.isin([\"reward\", \"failure\", \"time over\"])]\n",
    "            self.data = self.data.merge(\n",
    "                pd.DataFrame({\"session_id\": self.data.session_id.unique(),\n",
    "                              \"burst\": list(map(calc_burst, self.data.session_id.unique()))}),\n",
    "                on=\"session_id\", how=\"left\")\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "\n",
    "        def entropy_analyzing(section=10, bit=self.bit):\n",
    "            data = self.data[(self.data.event_type.isin([\"reward\", \"failure\"])) & (self.data.task.isin(self.tasks))]\n",
    "            entropy_df = data[\n",
    "                [\"session_id\", \"task\", \"entropy_{}\".format(section), \"entropy_after_{}\".format(section), \"pattern\"]]\n",
    "            count_correct = lambda pat: np.nan if np.isnan(pat) else \"{:b}\".format(int(pat)).zfill(bit).count(\"1\")\n",
    "            entropy_df[\"correctnum_{}bit\".format(bit)] = list(map(count_correct, entropy_df.pattern))\n",
    "            # entropy_df[\"mouse_no\"] = self.mouse_no\n",
    "            print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "            return entropy_df\n",
    "\n",
    "        # main\n",
    "        header = [\"timestamps\", \"task\", \"session_id\", \"correct_times\", \"event_type\", \"hole_no\"]\n",
    "        pattern = {}\n",
    "        task_prob = {}\n",
    "        self.data = rehash_session_id()\n",
    "        self.data = add_hot_vector()\n",
    "        self.data_ci = self.data\n",
    "#         self.data.loc[\n",
    "#             self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"hole_choice_entropy\"] = calc_entropy()\n",
    "        # ent_section = 10\n",
    "        # self.data.loc[\n",
    "        #     self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"entropy_10\"] = calc_entropy(ent_section)\n",
    "        # self.data.loc[\n",
    "        #     self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"entropy_after_10\"] = \\\n",
    "        #     self.data.loc[self.data.index[self.data.event_type.isin(\n",
    "        #         ['reward', 'failure'])], \"entropy_10\"][(ent_section + self.bit - 1):].to_list() + \\\n",
    "        #     ([np.nan] * (ent_section + self.bit - 1))\n",
    "#         ent_section3 = 150\n",
    "#         self.data.loc[\n",
    "#             self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"entropy_{}\".format(\n",
    "#                 ent_section3)] = calc_entropy(ent_section3)\n",
    "#         ent_section2 = 300\n",
    "#         self.data.loc[\n",
    "#             self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"entropy_{}\".format(\n",
    "#                 ent_section2)] = calc_entropy(ent_section2)\n",
    "#         ent_section = 50\n",
    "#         self.data.loc[\n",
    "#             self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"entropy_{}\".format(\n",
    "#                 ent_section)] = calc_entropy(ent_section)\n",
    "#         self.data.loc[\n",
    "#             self.data.index[self.data.event_type.isin(['reward', 'failure'])], \"entropy_after_{}\".format(\n",
    "#                 ent_section)] = \\\n",
    "#             self.data.loc[self.data.index[self.data.event_type.isin(\n",
    "#                 ['reward', 'failure'])], \"entropy_{}\".format(ent_section)][(ent_section + self.bit - 1):].tolist() + \\\n",
    "#             ([np.nan] * (ent_section + self.bit - 1))\n",
    "#         self.delta = add_timedelta()\n",
    "        self.data_not_omission = self.data[\n",
    "            ~self.data.session_id.isin(self.data.session_id[self.data.event_type.isin([\"time over\"])])]\n",
    "\n",
    "        # action Probability\n",
    "        after_c_all = float(len(self.data[self.data[\"is_correct\"] == 1]))\n",
    "        after_f_all = float(len(self.data[self.data[\"is_failure\"] == 1]))\n",
    "        after_c_starts = self.data[self.data[\"is_correct\"] == 1]\n",
    "        after_f_starts = self.data[self.data[\"is_failure\"] == 1]\n",
    "        after_c_all_task = {}\n",
    "        after_f_all_task = {}\n",
    "        after_c_starts_task = {}\n",
    "        after_f_starts_task = {}\n",
    "        for task in self.tasks:\n",
    "            after_c_starts_task[task] = self.data[(self.data[\"is_correct\"] == 1) & (self.data[\"task\"] == task)]\n",
    "            after_f_starts_task[task] = self.data[(self.data[\"is_failure\"] == 1) & (self.data[\"task\"] == task)]\n",
    "            after_c_all_task[task] = float(len(after_c_starts_task[task]))\n",
    "            after_f_all_task[task] = float(len(after_f_starts_task[task]))\n",
    "\n",
    "        # after_o_all = len(data[data[\"event_type\"] == \"time over\"])\n",
    "        forward_trace = 5\n",
    "        prob_index = [\"c_same\", \"c_diff\", \"c_omit\", \"c_checksum\", \"f_same\", \"f_diff\", \"f_omit\", \"f_checksum\",\n",
    "                      \"c_NotMax\",\n",
    "                      \"f_NotMax\", \"o_NotMax\"]\n",
    "        probability = pd.DataFrame(columns=prob_index, index=range(1, forward_trace + 1)).fillna(0.0)\n",
    "\n",
    "        #        count_all()\n",
    "        count_task()\n",
    "        # bit analyze\n",
    "#         pp = analyze_pattern(self.bit)\n",
    "#         pp = pd.concat([pp[task].loc[:, pp[task].columns.isin([\"session_id\", \"pattern\"])] for task in self.tasks])\n",
    "#         self.data = pd.merge(self.data, pp, how='left')\n",
    "#         # 2 bit analyze\n",
    "#         pp = analyze_pattern(2)\n",
    "#         pp = pd.concat([pp[task].loc[:, pp[task].columns.isin([\"session_id\", \"pattern\"])] for task in self.tasks])\n",
    "#         pp = pp.rename(columns={\"pattern\": \"pattern_2bit\"})\n",
    "#         self.data = pd.merge(self.data, pp, how='left')\n",
    "#         burst()\n",
    "        # entropy analyzing\n",
    "#         self.entropy_analyze = entropy_analyzing(section=50)\n",
    "        # self.entropy_analyze.concat(entropy_analyzing(section=50))\n",
    "        return self.data, probability, task_prob, self.delta, self.fig_prob_tmp, pattern, self.entropy_analyze\n",
    "\n",
    "    def dev_read_data(self, mouse_no):\n",
    "        task_prob = {}\n",
    "        delta = {}\n",
    "        fig_prob = {}\n",
    "        pattern_prob = {}\n",
    "        data = pd.read_csv(os.path.join(self.logpath, 'data/no{:03d}_{}_data.csv'.format(mouse_no, \"all\")))\n",
    "        probability = pd.read_csv(os.path.join(self.logpath, 'data/no{:03d}_{}_prob.csv'.format(mouse_no, \"all\")))\n",
    "\n",
    "        for task in self.tasks:\n",
    "            delta[task] = pd.read_csv(os.path.join(self.logpath, 'data/no{:03d}_{}_time.csv'.format(mouse_no, task)))\n",
    "            task_prob[task] = pd.read_csv(\n",
    "                os.path.join(self.logpath, 'data/no{:03d}_{}_prob.csv'.format(mouse_no, task)))\n",
    "            fig_prob[task] = {}\n",
    "            for fig_num in [\"fig1\", \"fig2\", \"fig3\"]:\n",
    "                fig_prob[task][fig_num] = pd.read_csv(\n",
    "                    os.path.join(self.logpath, 'data/no{:03d}_{}_{}_prob_fig.csv'.format(mouse_no, task, fig_num)),\n",
    "                    index_col=0)\n",
    "            pattern_prob[task] = pd.read_csv(\n",
    "                os.path.join(self.logpath, 'data/no{:03d}_{}_pattern.csv'.format(mouse_no, task)))\n",
    "        return data, probability, task_prob, delta, fig_prob, pattern_prob\n",
    "\n",
    "    def export_csv(self, mouse_no=None):\n",
    "        self.mice_task.to_csv(os.path.join(self.logpath, 'data/{}_data.csv'.format(\"all\")))\n",
    "#         self.probability.to_csv(\n",
    "#             os.path.join(self.logpath, 'data/{}_prob.csv'.format(\"all\")))\n",
    "        for task in self.tasks:\n",
    "#             self.mice_delta[task].to_csv(os.path.join(self.logpath, 'data/{}_time.csv'.format(task)))\n",
    "            # AttributeError: 'Series' object has no attribute 'type'\n",
    "#             reward_latency_data = self.mice_delta[task][self.mice_delta[task].type == \"reward_latency\"]\n",
    "#             reward_latency_data.to_csv(os.path.join(self.logpath, 'data/{}_rewardlatency.csv'.format(task)))\n",
    "            self.task_prob[task].to_csv(os.path.join(self.logpath, 'data/{}_prob.csv'.format(task)))\n",
    "#             self.pattern_prob[task].to_csv(os.path.join(self.logpath, 'data/{}_pattern.csv'.format(task)))\n",
    "#             [self.fig_prob[task][fig_num].to_csv(\n",
    "#                 os.path.join(self.logpath, 'data/prob_fig{}_{}.csv'.format(fig_num, task))) for\n",
    "#                 fig_num in [\"fig1\", \"fig2\", \"fig3\"]]\n",
    "            # pattern\n",
    "            # [self.entropy_analyze[\n",
    "            #      (self.entropy_analyze[\"correctnum_{}bit\".format(10,self.bit)] == count) &\n",
    "            #      (self.entropy_analyze[\"task\"] == task)  # & (\n",
    "            #      # self.entropy_analyze[\"mouse_no\"] == mouse_no)\n",
    "            #      ][10:-10].to_csv(\n",
    "            #     '{}data/pattern_entropy/summary/no{:03d}_{}_entropy_pattern_count_{}_summary.csv'.format(\n",
    "            #         self.logpath, mouse_no, task, int(count))) for count in\n",
    "            #     self.entropy_analyze[\"correctnum_{}bit\".format(10,self.bit)][\n",
    "            #         ~np.isnan(self.entropy_analyze[\"correctnum_{}bit\".format(10,self.bit)])].unique()]\n",
    "            # [self.entropy_analyze[\n",
    "            #      (self.entropy_analyze[\"pattern\"] == pattern) &\n",
    "            #      (self.entropy_analyze[\"task\"] == task)  # & (\n",
    "            #      # self.entropy_analyze[\"mouse_no\"] == mouse_no)\n",
    "            #      ][10:-10].to_csv(\n",
    "            #     '{}data/pattern_entropy/no{:03d}_{}_entropy_pattern_{:04b}.csv'.format(\n",
    "            #         self.logpath, mouse_no, task, int(pattern))) for\n",
    "            #     pattern in self.data.pattern[~np.isnan(self.data.pattern)].unique()]\n",
    "#             [self.mice_entropy[(self.mice_entropy[\"task\"] == task)  # & (\n",
    "#                  # self.entropy_analyze[\"mouse_no\"] == mouse_no)\n",
    "#              ][50:-50][(self.mice_entropy[\"correctnum_{}bit\".format(self.bit)] == count)].to_csv(\n",
    "#                 '{}data/pattern_entropy/summary/{}_entropy_pattern{:d}_count_{}_summary.csv'.format(\n",
    "#                     self.logpath, task, 50, int(count))) for count in\n",
    "#                 # self.entropy_analyze[\"correctnum_{}bit\".format(self.bit)][\n",
    "#                 # ~np.isnan(self.entropy_analyze[\"correctnum_{}bit\".format(self.bit)])].unique()]\n",
    "#                 range(0, self.bit)]\n",
    "#             [self.mice_entropy[\n",
    "#                  (self.mice_entropy[\"task\"] == task)  # & (\n",
    "#                  # self.entropy_analyze[\"mouse_no\"] == mouse_no)\n",
    "#              ][50:-50][(self.mice_entropy[\"pattern\"] == pattern)].to_csv(\n",
    "#                 '{}/data/pattern_entropy/{}_entropy{:d}_pattern_{:04b}.csv'.format(\n",
    "#                     self.logpath, task, 50, int(pattern))) for\n",
    "#                 pattern in self.data.pattern[~np.isnan(self.data.pattern)].unique()]\n",
    "\n",
    "        print(\"{} ; {} done\".format(datetime.now(), sys._getframe().f_code.co_name))\n",
    "\n",
    "\n",
    "def export_onehole_csv(tdata, mice, tasks):\n",
    "    df = dict(zip(tasks, [pd.DataFrame() for _ in tasks]))\n",
    "    for task in tasks:\n",
    "        for mouse_id in mice:\n",
    "            data_timedelta = tdata.mice_delta[task][(tdata.mice_delta[task].mouse_id.isin([mouse_id]))]\n",
    "            data_action = tdata.mice_task[\n",
    "                (tdata.mice_task.task.isin([task])) & (tdata.mice_task.mouse_id.isin([mouse_id]))]\n",
    "            tmp_df = data_action.merge(\n",
    "                data_timedelta[(data_timedelta.type.isin([\"reward_latency\"]))][\n",
    "                    [\"reward_latency_sec\", \"session_id\"]],\n",
    "                on=\"session_id\", how='left')\n",
    "            tmp_df = tmp_df.merge(\n",
    "                data_timedelta[(data_timedelta.type.isin([\"reaction_time\"]))].drop(\n",
    "                    columns=\"reward_latency_sec\"),\n",
    "                on=\"session_id\", how='left')\n",
    "            tmp_df.loc[\n",
    "                tmp_df.index[tmp_df.event_type.isin([\"failure\", \"omission\"])].to_list(), \"reaction_time_sec\"] = \\\n",
    "                data_timedelta.reaction_time_sec[\n",
    "                    (data_timedelta.type.isin([\"reaction_time\"])) & (~data_timedelta.session_id.isin(\n",
    "                        (data_timedelta.session_id[\n",
    "                             data_timedelta.type.isin([\"reward_latency\"])].drop_duplicates().to_list())))].to_list()\n",
    "\n",
    "            # nosepoke after reward\n",
    "\n",
    "            def check_poke_after(correct_num):\n",
    "                data = data_action[data_action.cumsum_correct_taskreset.isin([correct_num])]\n",
    "                return bool(sum(data.event_type.isin([\"nose poke after rew\"])))\n",
    "\n",
    "            poke_after = list(map(check_poke_after, data_action[\n",
    "                data_action.event_type.isin([\"reward\", \"failure\", \"omission\"])].cumsum_correct_taskreset.unique()))\n",
    "            tmp_df = tmp_df.assign(poke_after=np.nan)[tmp_df.event_type.isin([\"reward\", \"failure\", \"time over\"])]\n",
    "            tmp_df.loc[tmp_df.index[tmp_df.event_type.isin([\"reward\"])], \"poke_after\"] = poke_after\n",
    "            tmp_df = tmp_df[\n",
    "                [\"timestamps\", \"task\", \"event_type\", \"reaction_time_sec\", \"reward_latency_sec\", \"poke_after\"]]\n",
    "            df[task] = pd.concat([df[task], tmp_df])\n",
    "        df[task].to_csv(os.path.join(\"data\", \"{}_task-{}_1holedata.csv\".format(\"all\", task)), index=False)\n",
    "\n",
    "\n",
    "def view_averaged_prob_same_prev(tdata, mice, tasks):\n",
    "    m = []\n",
    "    t = []\n",
    "    csame = []\n",
    "    fsame = []\n",
    "\n",
    "    for mouse_id in mice:\n",
    "        for task in tasks:\n",
    "            m += [mouse_id]\n",
    "            t += [task]\n",
    "            csame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['c_same']]\n",
    "            fsame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['f_same']]\n",
    "\n",
    "    after_prob_df = pd.DataFrame(\n",
    "        data={'mouse_id': m, 'task': t, 'c_same': csame, 'f_same': fsame},\n",
    "        columns=['mouse_id', 'task', 'c_same', 'f_same']\n",
    "    )\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(8, 4), dpi=100)\n",
    "    for task in tasks:\n",
    "        plt.subplot(1, len(tasks), tasks.index(task) + 1)\n",
    "\n",
    "        c_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['c_same'].to_list())\n",
    "        c_same_avg = np.mean(c_same, axis=0)\n",
    "        c_same_std = np.std(c_same, axis=0)\n",
    "        c_same_var = np.var(c_same, axis=0)\n",
    "\n",
    "        f_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['f_same'].to_list())\n",
    "        f_same_avg = np.mean(f_same, axis=0)\n",
    "        f_same_var = np.var(f_same, axis=0)\n",
    "\n",
    "        xlen = len(c_same_avg)\n",
    "        xax = np.array(range(1, xlen + 1))\n",
    "        plt.plot(xax, c_same_avg, label=\"rewarded start\")\n",
    "        plt.errorbar(xax, c_same_avg, yerr=c_same_var, capsize=2, fmt='o', markersize=1, ecolor='black',\n",
    "                     markeredgecolor=\"black\", color='w', lolims=True)\n",
    "\n",
    "        plt.plot(np.array(range(1, xlen + 1)), f_same_avg, label=\"no-rewarded start\")\n",
    "        plt.errorbar(xax, f_same_avg, yerr=f_same_var, capsize=2, fmt='o', markersize=1, ecolor='black',\n",
    "                     markeredgecolor=\"black\", color='w', uplims=True)\n",
    "\n",
    "        # plt.ion()\n",
    "        plt.xticks(np.arange(1, xlen + 1, 1))\n",
    "        plt.xlim(0.5, xlen + 0.5)\n",
    "        plt.ylim(0, 1.05)\n",
    "        if tasks.index(task) == 0:\n",
    "            plt.ylabel('P (same choice)')\n",
    "            plt.legend()\n",
    "        plt.xlabel('Trial')\n",
    "        plt.title('{}'.format(task))\n",
    "        \n",
    "    # plt.savefig('fig/{}_prob_all4.png'.format(graph_ins.exportpath))\n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "    plt.savefig('fig/prob_all4_{}.png'.format(tasks[0]))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def view_summary(tdata, mice, tasks, x=\"session_id\"):\n",
    "    for mouse_id in mice:\n",
    "        def plot(mdf, task=\"all\"):\n",
    "            labels = [\"failure\", \"correct\", \"omission\"]\n",
    "            df = mdf[mdf[\"event_type\"].isin([\"reward\", \"failure\", \"time over\"])]\n",
    "\n",
    "            # past time\n",
    "            df.timestamps = (df.timestamps - df.iat[0, 0]).apply(lambda time: time.total_seconds())\n",
    "\n",
    "            # entropy\n",
    "            fig, ax = plt.subplots(4, 1, sharex=\"all\", figsize=(15, 8), dpi=100)\n",
    "            fig.suptitle('no{:03} summary {}'.format(mouse_id, task), y=1.0)\n",
    "            plt.subplots_adjust(hspace=0, bottom=0)\n",
    "\n",
    "            ax[0].plot(df[df.event_type.isin([\"reward\", \"failure\"])][x],\n",
    "                       df[df.event_type.isin([\"reward\", \"failure\"])]['hole_choice_entropy'])\n",
    "            ax[0].set_ylabel('Entropy (bit)')\n",
    "            ax[0].set_xlim(df[x].min(), df[x].max())\n",
    "            if task == \"all\":\n",
    "                collection = collections.BrokenBarHCollection.span_where(df[x].to_numpy(), ymin=-100, ymax=100,\n",
    "                                                                         where=(df.task.isin(tasks[0::2])),\n",
    "                                                                         facecolor='lightblue', alpha=0.3)\n",
    "                ax[0].add_collection(collection)\n",
    "\n",
    "            # scatter\n",
    "            colors = [\"red\", \"blue\", \"black\"]\n",
    "            size = dict(zip(labels, [25, 50, 25]))\n",
    "            pos = dict(zip(labels, [\"bottom\", \"full\", \"bottom\"]))\n",
    "            datasets = ([df[df[\"is_{}\".format(flag)] == 1] for flag in labels])\n",
    "            leg = []\n",
    "            for dt, la, cl in zip(datasets, labels, colors):\n",
    "                marker = markers.MarkerStyle(\"|\", pos[la])\n",
    "                leg.append(ax[1].scatter(dt[x], dt['is_hole1'] * 1, s=size[la], color=cl, marker=marker, label=la))\n",
    "                ax[1].scatter(dt[x], dt['is_hole3'] * 2, s=size[la], color=cl, marker=marker)\n",
    "                ax[1].scatter(dt[x], dt['is_hole5'] * 3, s=size[la], color=cl, marker=marker)\n",
    "                ax[1].scatter(dt[x], dt['is_hole7'] * 4, s=size[la], color=cl, marker=marker)\n",
    "                ax[1].scatter(dt[x], dt['is_hole9'] * 5, s=size[la], color=cl, marker=marker)\n",
    "                ax[1].scatter(dt[x], dt['is_omission'] * 0, s=size[la], color=cl, marker=marker)\n",
    "            ax[1].set_ylabel(\"Hole\")\n",
    "            ax[1].set_yticks([1, 2, 3, 4, 5])\n",
    "            ax[1].legend()\n",
    "            if task == \"all\":\n",
    "                collection = collections.BrokenBarHCollection.span_where(df[x].to_numpy(), ymin=-2, ymax=6,\n",
    "                                                                         where=(df.task.isin(tasks[0::2])),\n",
    "                                                                         facecolor='lightblue', alpha=0.3)\n",
    "                ax[1].add_collection(collection)\n",
    "\n",
    "            # cumsum\n",
    "            ax[2].plot(df[x], df['cumsum_correct_taskreset'])\n",
    "            ax[2].plot(df[x], df['cumsum_incorrect_taskreset'])\n",
    "            ax[2].plot(df[x], df['cumsum_omission_taskreset'])\n",
    "            ax[2].set_ylabel('Cumulative')\n",
    "            # ax[2].set_xlabel('Trial')\n",
    "            ax[2].legend([\"correct\", \"incorrect\", \"omission\"])\n",
    "            if task == \"all\":\n",
    "                collection = collections.BrokenBarHCollection.span_where(df[x].to_numpy(), ymin=-20, ymax=1000,\n",
    "                                                                         where=(df.task.isin(tasks[0::2])),\n",
    "                                                                         facecolor='lightblue', alpha=0.3)\n",
    "                ax[2].add_collection(collection)\n",
    "\n",
    "            # 100 step move average\n",
    "            # make dataframe\n",
    "            df_o = df[df[\"event_type\"].isin([\"reward\", \"failure\"])]\n",
    "            # data = pd.DataFrame(columns=[\"is_hole{}\".format(i) for i in range(1, 10, 2)])\n",
    "            data_tmp = pd.DataFrame()\n",
    "            add_average = lambda idx: [df_o[max(0, idx - 100):idx][\"is_hole{}\".format(i)].sum() /\n",
    "                                       max(df_o[max(0, idx - 100):idx][\"is_hole{}\".format(i)].size, 1) for i in\n",
    "                                       range(1, 10, 2)]\n",
    "            data_tmp = data_tmp.append(list(map(add_average, list(range(0, len(df_o))))), ignore_index=True)\n",
    "            # plot\n",
    "            ax[3].plot(df_o[x], data_tmp)\n",
    "            ax[3].set_ylabel(\"moving average action rate\")\n",
    "            # legend\n",
    "            ax[3].legend([\"hole{}\".format(i) for i in range(1, 10, 2)])\n",
    "            if task == \"all\":\n",
    "                collection = collections.BrokenBarHCollection.span_where(df_o[x].to_numpy(), ymin=-20,\n",
    "                                                                         ymax=1000,\n",
    "                                                                         where=(df_o.task.isin(tasks[0::2])),\n",
    "                                                                         facecolor='lightblue', alpha=0.3)\n",
    "                ax[3].add_collection(collection)\n",
    "            # savefig\n",
    "            fig.savefig('fig/no{:03d}_{}_summary_{}.png'.format(mouse_id, task, x))\n",
    "            fig.show()\n",
    "\n",
    "        data = tdata.mice_task[tdata.mice_task.mouse_id == mouse_id]\n",
    "        plot(data)\n",
    "        list(map(plot, [data[data.task == task] for task in tdata.tasks], tdata.tasks))\n",
    "\n",
    "\n",
    "def view_trial_per_datetime(tdata, mice=[18], task=\"All5_30\"):\n",
    "    \"\"\" for debug \"\"\"\n",
    "    # for mouse_no in mice:\n",
    "    data = tdata.data[\n",
    "        (tdata.data.event_type.isin([\"reward\", \"failure\", \"time over\"]))\n",
    "        & (tdata.data.task == task)\n",
    "        # &(tdata.data.mouse_id == mouse_no)\n",
    "        ].set_index(\"timestamps\").resample(\"1H\").sum()\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "    data.plot.bar(y=[\"is_correct\", \"is_incorrect\", \"is_omission\"], stacked=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def view_scatter_vs_times_with_burst(tdata, mice=[18], task=\"All5_30\", burst=1):\n",
    "    \"\"\" fig1 B \"\"\"\n",
    "    for mouse_id in mice:\n",
    "\n",
    "        labels = [\"correct\", \"incorrect\", \"omission\"]\n",
    "\n",
    "        data = tdata.data.assign(\n",
    "            timestamps=(tdata.data.timestamps - tdata.data.timestamps[0]).dt.total_seconds())  # [mouse_id]\n",
    "        data = data[data[\"event_type\"].isin([\"reward\", \"failure\", \"time over\"])]\n",
    "        # data = data[data.burst.isin(data.burst.unique()[data.groupby(\"burst\").burst.count() > burst])]\n",
    "        burst_time = list(data.burst.unique()[data.groupby(\"burst\").burst.count() > burst])\n",
    "        fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "        fig_subplot = fig.add_subplot(1, 1, 1)\n",
    "        # plt.title('{:03} summary'.format(mouse_id))\n",
    "        #    nose_poke_raster(mouse_id, fig.add_subplot(3, 1, 2))\n",
    "\n",
    "        colors = [\"blue\", \"red\", \"black\"]\n",
    "        for single_burst in burst_time:\n",
    "            d = data[data.burst == single_burst]\n",
    "            datasets = [(d[d[\"is_{}\".format(flag)] == 1]) for flag in labels]\n",
    "            for dt, la, cl in zip(datasets, labels, colors):\n",
    "                plt.scatter(dt.timestamps, dt['is_hole1'] * 1, s=15, c=cl)\n",
    "                plt.scatter(dt.timestamps, dt['is_hole3'] * 2, s=15, c=cl)\n",
    "                plt.scatter(dt.timestamps, dt['is_hole5'] * 3, s=15, c=cl)\n",
    "                plt.scatter(dt.timestamps, dt['is_hole7'] * 4, s=15, c=cl)\n",
    "                plt.scatter(dt.timestamps, dt['is_hole9'] * 5, s=15, c=cl)\n",
    "                plt.scatter(dt.timestamps, dt['is_omission'] * 0, s=15, c=cl)\n",
    "            plt.ylabel(\"Hole\")\n",
    "            plt.xlim(d.timestamps.min() - 30, d.timestamps.max() + 30)\n",
    "            plt.ylim(0, 5)\n",
    "            #    plt.xlim(0, len(mdf))\n",
    "\n",
    "            collection = collections.BrokenBarHCollection.span_where(data.timestamps.to_numpy(), ymin=0, ymax=5,\n",
    "                                                                     where=(data.burst.isin(burst_time)),\n",
    "                                                                     facecolor='pink', alpha=0.3)\n",
    "            fig_subplot.add_collection(collection)\n",
    "            # save\n",
    "            # plt.show()\n",
    "            burst_len = d.timestamps.count()\n",
    "            if not os.path.isdir(os.path.join(os.getcwd(), \"fig\", \"burst\", \"len\" + str(burst_len))):\n",
    "                os.mkdir(os.path.join(os.getcwd(), \"fig\", \"burst\", \"len\" + str(burst_len)))\n",
    "            plt.savefig(os.path.join(os.getcwd(), 'fig', 'burst', \"len\" + str(burst_len),\n",
    "                                     'no{:03d}_burst{}_hole_pasttime_burst.png'.format(mouse_id, single_burst)))\n",
    "\n",
    "\n",
    "def view_trial_per_time(tdata, mice=[18], task=\"All5_30\"):\n",
    "    \"\"\" fig1 C \"\"\"\n",
    "    data = tdata.data[\n",
    "        (tdata.data.event_type.isin([\"reward\", \"failure\", \"time over\"])) &\n",
    "        (tdata.data.task == task)\n",
    "        ].set_index(\"timestamps\").resample(\"1H\").sum()\n",
    "    data = data.set_index(data.index.time).groupby(level=0).mean()\n",
    "    fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "    data.plot.bar(y=[\"is_correct\", \"is_incorrect\", \"is_omission\"], stacked=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def view_prob_same_choice_burst(tdata, mice, tasks, burst=1):\n",
    "    \"\"\" fig4 \"\"\"\n",
    "    tdata_ci = tdata[tdata.event_type.isin([\"reward\", \"failure\"])]\n",
    "    tdata_ci = tdata_ci[\n",
    "        tdata_ci.burst.isin(tdata_ci.burst.unique()[tdata_ci.groupby(\"burst\").burst.count() > burst])].reset_index()\n",
    "\n",
    "    # burst_len limit なし\n",
    "    # after_prob_df = pd.concat([tdata.task_prob[task].assign(task=task) for task in tasks])\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig, ax = plt.subplots(1, len(tasks), sharey=\"all\", sharex=\"all\", figsize=(8, 4), dpi=100)\n",
    "    forward_trace = 7\n",
    "\n",
    "    def calc(mouse_id):\n",
    "        prob_index = [\"c_same\", \"f_same\", \"task\", \"mouse_id\"]\n",
    "        after_prob_df = pd.DataFrame(columns=prob_index)\n",
    "        lgnd = None\n",
    "        for task in tasks:\n",
    "            data_tmp = tdata_ci[(tdata_ci.task.isin([task])) & (tdata_ci.mouse_id == mouse_id)]  # .groupby(\"burst\")\n",
    "            \"burst ごと確率を出す\"\n",
    "            for bst in data_tmp.burst.unique():\n",
    "                data = data_tmp[data_tmp.burst.isin([bst])].reset_index(drop=True)\n",
    "                after_correct_all = data.burst[:-forward_trace][data.is_correct == 1].count()\n",
    "                after_incorrect_all = data.burst[:-forward_trace][data.is_incorrect == 1].count()\n",
    "                correct_index = data[:-forward_trace][data.is_correct == 1].index\n",
    "                incorrect_index = data[:-forward_trace][data.is_incorrect == 1].index\n",
    "                df = pd.DataFrame(columns=range(forward_trace))\n",
    "                same_correct = \\\n",
    "                    df.append([data[idx:idx + min(forward_trace, len(data))].hole_no == data.hole_no[idx] for idx in\n",
    "                               correct_index]).sum() if len(correct_index) else df.sum()\n",
    "                same_incorrect = \\\n",
    "                    df.append([data[idx:idx + min(forward_trace, len(data))].hole_no == data.hole_no[idx] for idx in\n",
    "                               incorrect_index]).sum() if len(incorrect_index) else df.sum()\n",
    "                after_prob_df = after_prob_df.append(pd.DataFrame({\"c_same\": same_correct / after_correct_all,\n",
    "                                                                   \"f_same\": same_incorrect / after_incorrect_all,\n",
    "                                                                   \"task\": task, \"mouse_id\": mouse_id,\n",
    "                                                                   \"burst\": bst}).fillna(0.0))\n",
    "\n",
    "                # after_prob_df = after_prob_df.append(pd.DataFrame({\"c_same\": (same_correct / after_correct_all).mean(),\n",
    "                #                                                    \"f_same\": (same_incorrect / after_incorrect_all).mean(),\n",
    "                #                                                    \"task\": task, \"mouse_id\": mouse_id,\n",
    "                #                                                    \"burst\": bst}).fillna(0.0), ignore_index=True)\n",
    "            \"\"\" burstごと確率 を平均する\"\"\"\n",
    "            c_same = after_prob_df[\n",
    "                (after_prob_df['task'].isin([task])) &\n",
    "                (after_prob_df[\"mouse_id\"] == mouse_id)\n",
    "                ]['c_same'].groupby(level=0)\n",
    "            c_same_avg = c_same.mean()[:forward_trace + 1]\n",
    "            c_same_var = c_same.var()[:forward_trace + 1]\n",
    "\n",
    "            f_same = after_prob_df[\n",
    "                (after_prob_df['task'].isin([task])) &\n",
    "                (after_prob_df[\"mouse_id\"] == mouse_id)\n",
    "                ]['f_same'].groupby(level=0)\n",
    "            f_same_avg = f_same.mean()[:forward_trace + 1]\n",
    "            f_same_var = f_same.var()[:forward_trace + 1]\n",
    "\n",
    "            # ここから描画\n",
    "            xlen = c_same_avg.size\n",
    "            # xax = np.array(range(1, xlen + 1))\n",
    "            xax = np.array(range(forward_trace + 1))\n",
    "            ax.plot(xax, c_same_avg, color=\"orange\", label=\"rewarded start\")\n",
    "            ax.errorbar(xax, c_same_avg, yerr=c_same_var, capsize=2, fmt='o', markersize=1,\n",
    "                        ecolor='black',\n",
    "                        markeredgecolor=\"black\", color='w', lolims=True)\n",
    "\n",
    "            ax.plot(xax, f_same_avg, color=\"blue\", label=\"no-rewarded start\")\n",
    "            ax.errorbar(xax, f_same_avg, yerr=f_same_var, capsize=2, fmt='o', markersize=1,\n",
    "                        ecolor='black',\n",
    "                        markeredgecolor=\"black\", color='w', uplims=True)\n",
    "\n",
    "            # plt.ion()\n",
    "            ax.set_xticks(xax)\n",
    "            ax.set_xlim(-0.5, xlen + 0.5)\n",
    "            ax.set_ylim(0, 1.05)\n",
    "            if tasks.index(task) == 0:\n",
    "                ax.set_ylabel('P (same choice)')\n",
    "            if tasks.index(task) == int(len(tasks) / 2):\n",
    "                lgnd = ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.05), ncol=2,\n",
    "                                 mode=\"expand\")\n",
    "            if tasks.index(task) in [0, len(tasks) - 1]:\n",
    "                ax.set_xlabel('Trial')\n",
    "            ax.set_title('{}'.format(task))\n",
    "        # label\n",
    "        # plt.legend()\n",
    "        lgnd.get_frame().set_linewidth(0.0)\n",
    "        plt.savefig(\"no{:03d}_prob4.png\".format(mouse_id))\n",
    "        plt.show()\n",
    "\n",
    "    list(map(calc, mice))\n",
    "\n",
    "\n",
    "def view_sigletask_prob(tdata, mice, task):\n",
    "    \"\"\" fig5 A \"\"\"\n",
    "    tdata_ci = tdata.mice_task[tdata.mice_task.event_type.isin([\"reward\", \"failure\"])]\n",
    "    tdata_ci = tdata_ci[tdata_ci.task.isin([task])].reset_index(drop=True)\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig, ax = plt.subplots(1, 2, sharey=\"all\", sharex=\"all\", figsize=(8, 4), dpi=100)\n",
    "    forward_trace = 7\n",
    "    range_lim = 50\n",
    "\n",
    "    def calc(mouse_id):\n",
    "        prob_index = [\"c_same\", \"f_same\", \"task\", \"mouse_id\"]\n",
    "        data_tmp = tdata_ci[(tdata_ci.mouse_id == mouse_id)].reset_index(drop=True)\n",
    "\n",
    "        for data, index in zip([data_tmp[:range_lim], data_tmp[-range_lim:]], [0, 1]):\n",
    "            \"確率を出す\"\n",
    "            after_prob_df = pd.DataFrame(columns=prob_index)\n",
    "            data = data.reset_index(drop=True)\n",
    "            after_correct_all = data.is_correct[data.is_correct == 1].count()\n",
    "            after_incorrect_all = data.is_incorrect[data.is_incorrect == 1].count()\n",
    "            correct_index = data[data.is_correct == 1].index\n",
    "            incorrect_index = data[data.is_incorrect == 1].index\n",
    "            df = pd.DataFrame(columns=range(forward_trace))\n",
    "            same_correct = df.append(\n",
    "                [(data[idx + 1:idx + min(forward_trace + 1, len(data[idx + 1:]))].hole_no == data.hole_no[\n",
    "                    idx]).reset_index(drop=True).T for idx in correct_index]) * 1 if len(correct_index) else df\n",
    "            same_incorrect = df.append(\n",
    "                [(data[idx + 1:idx + min(forward_trace + 1, len(data[idx + 1:]))].hole_no == data.hole_no[\n",
    "                    idx]).reset_index(drop=True).T for idx in incorrect_index]) * 1 if len(incorrect_index) else df\n",
    "            same_correct.columns = same_correct.columns + 1\n",
    "            same_incorrect.columns = same_incorrect.columns + 1\n",
    "            after_prob_df = after_prob_df.append(pd.DataFrame({\"c_same\": same_correct.sum() / after_correct_all,\n",
    "                                                               \"f_same\": same_incorrect.sum() / after_incorrect_all,\n",
    "                                                               \"task\": task, \"mouse_id\": mouse_id}).fillna(0.0))\n",
    "\n",
    "            c_same = after_prob_df[\n",
    "                (after_prob_df['task'].isin([task])) &\n",
    "                (after_prob_df[\"mouse_id\"] == mouse_id)\n",
    "                ]['c_same']\n",
    "\n",
    "            f_same = after_prob_df[\n",
    "                (after_prob_df['task'].isin([task])) &\n",
    "                (after_prob_df[\"mouse_id\"] == mouse_id)\n",
    "                ]['f_same']\n",
    "\n",
    "            # ここから描画\n",
    "            xlen = c_same.size\n",
    "            xax = np.array(range(1, forward_trace + 1))\n",
    "            ax[index].plot(c_same, color=\"orange\", label=\"rewarded start\")\n",
    "            ax[index].plot(f_same, color=\"skyblue\", label=\"no-rewarded start\")\n",
    "            ax[index].set_xticks(xax)\n",
    "            ax[index].set_xlim(0.5, xlen + 0.5)\n",
    "            ax[index].set_ylim(0, 1.05)\n",
    "            ax[index].set_xlabel('Trial')\n",
    "        # label\n",
    "        ax[0].set_ylabel('P (same choice)')\n",
    "        ax[0].set_title('no{:03d}_{}_first{}step'.format(mouse_id, task, range_lim))\n",
    "        ax[1].set_title('no{:03d}_{}_last{}step'.format(mouse_id, task, range_lim))\n",
    "        plt.subplots_adjust(top=0.8)\n",
    "        plt.legend()\n",
    "        plt.savefig(\"no{:03d}_prob5_{}.png\".format(mouse_id, task))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    list(map(calc, mice))\n",
    "\n",
    "\n",
    "def view_pattern_entropy_summary(tdata, mice, task=None):\n",
    "    data = tdata.mice_entropy\n",
    "    average_all = None\n",
    "    for mouse_id in mice:\n",
    "        data_tmp = data[mouse_id].groupby(\n",
    "            [\"task\", \"correctnum_{}bit\".format(tdata.bit)])\n",
    "        mean = data_tmp.mean().reset_index()\n",
    "        sd = data_tmp.std().reset_index()\n",
    "        data_tmp = pd.merge(mean, sd, on=[\"task\", \"correctnum_{}bit\".format(tdata.bit)], suffixes=[\"_mean\", \"_sd\"])\n",
    "        data_tmp = data_tmp.loc[:, data_tmp.columns.str.startswith((\"task\", \"correctnum\", \"entropy\"))].assign(\n",
    "            mouse_id=mouse_id)\n",
    "        average_all = data_tmp if isinstance(average_all, type(None)) else average_all.append(data_tmp)\n",
    "    for group_info, data_tmp in average_all.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]):\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        # error bar\n",
    "        # ax.errorbar(data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].columns.to_numpy().reshape(2),\n",
    "        #             data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].to_numpy(),\n",
    "        #             yerr=data_tmp.loc[:, data_tmp.columns.str.endswith(\"sd\")].to_numpy(),\n",
    "        #             ecolor=\"black\")\n",
    "        # ax.errorbar(data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].columns,\n",
    "        #             data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].to_numpy().reshape(2, )[1],\n",
    "        #             yerr=data_tmp.loc[:, data_tmp.columns.str.endswith(\"sd\")].to_numpy().reshape(2, )[1],\n",
    "        #             ecolor=\"black\")\n",
    "        ax.errorbar(data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].columns.to_numpy(),\n",
    "                    data_tmp.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]).mean().loc[:,\n",
    "                    data_tmp.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]).mean().columns.str.endswith(\n",
    "                        \"mean\")].to_numpy().T,\n",
    "                    yerr=np.mean(data_tmp.loc[:, data_tmp.columns.str.endswith(\"sd\")].to_numpy(), axis=0),\n",
    "                    ecolor=\"blue\")\n",
    "        # ax.errorbar(data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].columns[1],\n",
    "        #             data_tmp.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]).mean().loc[:,\n",
    "        #             data_tmp.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]).mean().columns.str.endswith(\n",
    "        #                 \"mean\")].to_numpy().reshape(2, )[1],\n",
    "        #             yerr=data_tmp.loc[:, data_tmp.columns.str.endswith(\"sd\")].to_numpy().reshape(2, )[1],\n",
    "        #             ecolor=\"black\")\n",
    "        # mean\n",
    "        ax.plot(data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].columns,\n",
    "                data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].to_numpy().T,\n",
    "                marker=\"o\", color=\"black\")\n",
    "        # all average\n",
    "        ax.plot(data_tmp.loc[:, data_tmp.columns.str.endswith(\"mean\")].columns,\n",
    "                data_tmp.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]).mean().loc[:,\n",
    "                data_tmp.groupby([\"task\", \"correctnum_{}bit\".format(tdata.bit)]).mean().columns.str.endswith(\n",
    "                    \"mean\")].to_numpy().T,\n",
    "                marker=\"x\", color=\"blue\")\n",
    "        # plt.show(block=True)\n",
    "        plt.savefig(os.path.join(os.getcwd(), 'fig', 'pattern_ent',\n",
    "                                 'pattern_ent_average_{}_correct{}.png'.format(group_info[0], group_info[1])))\n",
    "\n",
    "\n",
    "def export_2bit_analyze(tdata, mice, tasks, bit=2, burst_len=10):\n",
    "    \"\"\" task ごとにパターンの確率を算出して csv 出力 \"\"\"\n",
    "    pattern_range = range(pow(bit, 2))\n",
    "    tmp = pd.DataFrame(columns=[\"{:02b}\".format(i) for i in pattern_range]).fillna(0.0)\n",
    "    prob_all = dict(zip(tasks, [tmp.copy() for _ in range(len(tasks))]))\n",
    "    count_all = dict(zip(tasks,\n",
    "                         [pd.DataFrame(columns=[\"{:02b}\".format(pattern) for pattern in pattern_range]).copy() for _ in\n",
    "                          range(len(tasks))]))\n",
    "    # row_data = dict(\n",
    "    #     zip(tasks, [pd.DataFrame(columns=[\"{:02b}\".format(pattern) for pattern in pattern_range]) for _ in tasks]))\n",
    "    for mouse_no in mice:\n",
    "        data = tdata.mice_task[tdata.mice_task.mouse_id == mouse_no]\n",
    "        data_ci = data[data.event_type.isin([\"reward\", \"failure\"])].reset_index(drop=True)\n",
    "        f_same_prev = lambda x: data_ci.at[data_ci[data_ci.session_id == x].index[0] + 1, \"hole_no\"] == \\\n",
    "                                data_ci.at[data_ci[data_ci.session_id == x].index[0], \"hole_no\"]\n",
    "        functions = lambda x: f_same_prev(x)\n",
    "        data_bursts = data_ci[data_ci.burst.isin(\n",
    "            data_ci.burst.unique()[data_ci.groupby(\"burst\").burst.count() > burst_len])]\n",
    "        bit_prob = dict(zip(tasks, [dict(zip([\"f_same_prev\"], [tmp.copy()])) for _ in range(len(tasks))]))\n",
    "        for task in tasks:\n",
    "            data_tmp = data_bursts[(data_ci.task.isin([task]))]\n",
    "            tmp_df = []\n",
    "            tmp_count = []\n",
    "            for pat_tmp in pattern_range:\n",
    "                # pattern count -> probability\n",
    "                f_p = pd.DataFrame(list(data_tmp[data_tmp.pattern_2bit == pat_tmp].session_id[1:].map(functions)),\n",
    "                                   columns=[\"f_same_prev\"]).fillna(0.0)\n",
    "                # row_data[task][\"{:02b}\".format(pat_tmp)] = row_data[task][\"{:02b}\".format(pat_tmp)].append((f_p * 1))\n",
    "                # f_p.count().to_csv(os.path.join(\"data\", \"2bit\",\n",
    "                #                                 \"no{:03d}_{}_pat{:02b}_burst_2bit.csv\".format(mouse_no, task, pat_tmp)))\n",
    "                tmp_count.append(len(data_tmp[data_tmp.pattern_2bit == pat_tmp]))\n",
    "                if len(f_p):\n",
    "                    \"\"\" 一例以上あった場合確率として計算 \"\"\"\n",
    "                    # Series\n",
    "                    tmp_df.append((pd.DataFrame(list(f_p.f_same_prev)).sum().fillna(0.0) /\n",
    "                                   len(data_tmp[data_tmp.pattern_2bit == pat_tmp])).values[0])\n",
    "\n",
    "                else:\n",
    "                    \"\"\" 一回もパターンが出ていない場合 \"\"\"\n",
    "                    tmp_df.append(np.nan)\n",
    "            bit_prob[task][\"f_same_prev\"] = bit_prob[task][\"f_same_prev\"].append(\n",
    "                pd.Series(tmp_df, index=bit_prob[task][\"f_same_prev\"].columns), ignore_index=True)\n",
    "            prob_all[task] = prob_all[task].append(pd.Series(tmp_df, index=bit_prob[task][\"f_same_prev\"].columns),\n",
    "                                                   ignore_index=True)\n",
    "            count_all[task].loc[\"no{}\".format(mouse_no)] = tmp_count\n",
    "            # export\n",
    "            bit_prob[task][\"f_same_prev\"].to_csv(\n",
    "                os.path.join(\"data\", \"2bit_prob_task-{}_no{}.csv\".format(task, mouse_no)), index=False, header=False)\n",
    "            # graph\n",
    "            fig = bit_prob[task][\"f_same_prev\"].T.plot.line(title=\"2bit no{:03d} task:{}\".format(mouse_no, task),\n",
    "                                                            style=\"bo-\", ylim=(0.0, 1.0), ms=10)\n",
    "            plt.savefig(os.path.join(\"fig\", \"2bit\", \"no{:03d}_{}_2bit.png\".format(mouse_no, task)))\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "    for task in tasks:\n",
    "        fig = prob_all[task].mean().T.plot.line(title=\"2bit {} task:{}\".format(\"all\", task),\n",
    "                                                style=\"ro-\", ylim=(0.0, 1.0), ms=10)\n",
    "        plt.savefig(os.path.join(\"fig\", \"2bit\", \"{}_{}_2bit.png\".format(\"all\", task)))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        prob_all[task].to_csv(os.path.join(\"data\", \"2bit\", \"allmice_{}_burst_2bit_prob.csv\".format(task)), index=False)\n",
    "        count_all[task].to_csv(os.path.join(\"data\", \"2bit\", \"n_{}_burst_2bit_count.csv\".format(task)))\n",
    "        count_all[task].count()\n",
    "        # [row_data[task][\"{:02b}\".format(pat)].to_csv(\n",
    "        #     os.path.join(\"data\", \"2bit\", \"{}_{}_pat{:02b}_burst_2bit.csv\".format(\"allmice\", task, pat))) for pat in\n",
    "        #     pattern_range]\n",
    "        # return bit_prob\n",
    "\n",
    "\n",
    "def export_all_entropy(tdata, mice, tasks=[\"All5_90\", \"All5_30\", \"All5_30_drug\"]):\n",
    "    target_task = tasks\n",
    "    ret_val = pd.DataFrame()\n",
    "    datas = tdata if isinstance(tdata, list) else [tdata]\n",
    "\n",
    "    def min_max(x, axis=None):\n",
    "        np.array(x)\n",
    "        min = np.array(x).min(axis=axis)\n",
    "        max = np.array(x).max(axis=axis)\n",
    "        result = (x - min) / (max - min)\n",
    "        return result\n",
    "\n",
    "    for d in datas:\n",
    "        for mouse_id in mice:\n",
    "            tmp = [mouse_id]\n",
    "            for task in target_task:\n",
    "                data = d.mice_task[\n",
    "                    (d.mice_task.event_type.isin([\"reward\", \"failure\"]))\n",
    "                    & (d.mice_task.task.isin([task]))\n",
    "                    & (d.mice_task.mouse_id.isin([str(mouse_id)]))]\n",
    "                if not len(data):\n",
    "                    continue\n",
    "                current_entropy = min_max([data[\"is_hole{}\".format(str(hole_no))].sum() /\n",
    "                                           len(data) for hole_no in [1, 3, 5, 7, 9]])\n",
    "                tmp += [task, entropy(current_entropy, base=2)]\n",
    "            if len(tmp) == 1:\n",
    "                continue\n",
    "            ret_val = ret_val.append([tmp])\n",
    "\n",
    "    ret_val.to_csv(os.path.join(\"data\", \"entropy\", \"entropy_tasks_{}.csv\".format(\"_\".join(target_task))), index=False,\n",
    "                   header=False)\n",
    "    # [ret_val[ret_val.task.isin([task])].to_csv(os.path.join(\"data\", \"entropy\", \"entropy_task_{}.csv\".format(task)),\n",
    "    #                                            index=False, header=False) for task in target_task]\n",
    "\n",
    "\n",
    "def view_converse_reaction_time(tdata, mice, tasks):\n",
    "    tasks = tasks if isinstance(tasks, list) else [tasks]\n",
    "    tdata = tdata if isinstance(tdata, list) else [tdata]\n",
    "    for task in tasks:\n",
    "        for data in tdata:\n",
    "            data = data.mice_delta[task][\n",
    "                (data.mice_delta[task].type.isin([\"reaction_time\"]))  # &\n",
    "                # (tdata.mice_delta.mouse_id == mice)\n",
    "            ].reaction_time_sec\n",
    "            fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "            data.plot.hist(bins=100)\n",
    "            plt.title(\"reaction time task:{}\".format(task))\n",
    "            plt.xlabel(\"reaction time(s)\")\n",
    "            plt.rcParams[\"font.size\"] = 18\n",
    "            plt.savefig(os.path.join(\"fig\", \"task-{}_reaction_time.png\".format(task)))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def view_converse_reward_latency(tdata, mice, tasks, bin=100):\n",
    "    tasks = tasks if isinstance(tasks, list) else [tasks]\n",
    "    tdata = tdata if isinstance(tdata, list) else [tdata]\n",
    "    for task in tasks:\n",
    "        for data in tdata:\n",
    "            data = data.mice_delta[task][\n",
    "                (data.mice_delta[task].type.isin([\"reward_latency\"]))  # &\n",
    "                # (tdata.mice_delta.mouse_id == mice)\n",
    "            ].noreward_duration_sec\n",
    "            fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "            data.plot.hist(bins=bin)\n",
    "            plt.xlabel(\"reward latency(s)\")\n",
    "            plt.title(\"reward latency task:{}\".format(task))\n",
    "            plt.rcParams[\"font.size\"] = 18\n",
    "            plt.savefig(os.path.join(\"fig\", \"task-{}_reward_latency.png\".format(task)))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            # under 100\n",
    "            fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "            data[data <= 300].plot.hist(bins=bin)\n",
    "            plt.xlabel(\"reward latency(s)\")\n",
    "            plt.title(\"reward latency task:{}\".format(task))\n",
    "            plt.rcParams[\"font.size\"] = 18\n",
    "            plt.savefig(os.path.join(\"fig\", \"task-{}_reward_latency_u300.png\".format(task)))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            # under 30000\n",
    "            fig = plt.figure(figsize=(15, 8), dpi=100)\n",
    "            data[data <= 10000].plot.hist(bins=bin)\n",
    "            plt.xlabel(\"reward latency(s)\")\n",
    "            plt.title(\"reward latency task:{}\".format(task))\n",
    "            plt.rcParams[\"font.size\"] = 18\n",
    "            plt.savefig(os.path.join(\"fig\", \"task-{}_reward_latency_u10000.png\".format(task)))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def view_50step_entropy(tdata, mice, tasks):\n",
    "    data = tdata\n",
    "    if isinstance(tdata, list):\n",
    "        data = pd.concat([d.mice_task for d in tdata])\n",
    "    else:\n",
    "        data = tdata.mice_task\n",
    "    # entropy\n",
    "    data = data[\n",
    "        (data.task.isin(tasks)) &\n",
    "        (data.event_type.isin([\"reward\"]))]\n",
    "    for task in tasks:\n",
    "        df = data[(data.task == task)].groupby([\"cumsum_correct_taskreset\"]).mean().head(150)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 8), dpi=100)\n",
    "        fig.suptitle('50step entropy task:{}'.format(task))\n",
    "        ax.plot(df.index, df['entropy_50'])\n",
    "        ax.set_ylabel('Entropy (bit)')\n",
    "        plt.rcParams[\"font.size\"] = 18\n",
    "        plt.savefig(os.path.join(\"fig\", \"task-{}_entropy_upto150.png\".format(task)))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    for task in tasks:\n",
    "        for mouse_id in mice:\n",
    "            df = data[(data.task == task) & (data.mouse_id == mouse_id)].set_index(\"cumsum_correct_taskreset\")\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(15, 8), dpi=100)\n",
    "            fig.suptitle('50step entropy task:{}'.format(task))\n",
    "            ax.plot(df.index, df['entropy_50'])\n",
    "            ax.set_ylabel('Entropy (bit)')\n",
    "            plt.savefig(os.path.join(\"fig\", \"no{}_task-{}_entropy.png\".format(mouse_id, task)))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def export_previeous_entropy(tdata, mice, tasks):\n",
    "    # entropy\n",
    "    data = tdata.mice_task[\n",
    "        (tdata.mice_task.task.isin(tasks)) &\n",
    "        (tdata.mice_task.event_type.isin([\"reward\", \"failure\"]))]\n",
    "    ret_val = dict(zip(tasks, [pd.DataFrame() for _ in tasks]))\n",
    "    for task in tasks:\n",
    "        for mouse_id in mice:\n",
    "            df = data[(data.task == task) & (data.mouse_id == mouse_id)].tail(100).head(50).reset_index()\n",
    "            ret_val[task] = ret_val[task].append(\n",
    "                df[\"entropy_50\"].to_frame().assign(mouse_id=mouse_id))\n",
    "            data[(data.task == task) & (data.mouse_id == mouse_id)].head(100).reset_index().tail(50)[\n",
    "                \"entropy_50\"].to_frame().to_csv(\n",
    "                os.path.join(\"data\", \"pre_entropy_no{}_task_{}.csv\".format(mouse_id, task)))\n",
    "            data[(data.task == task) & (data.mouse_id == mouse_id)].reset_index().tail(50)[\n",
    "                \"entropy_50\"].to_frame().to_csv(\n",
    "                os.path.join(\"data\", \"post_entropy_no{}_task_{}.csv\".format(mouse_id, task)))\n",
    "        ret_val[task].to_csv(os.path.join(\"data\", \"allmice_{}_previous_100step_entropy.csv\".format(task)))\n",
    "        ret_val[task].groupby(level=0).mean().entropy_50.to_csv(\n",
    "            os.path.join(\"data\", \"mean_{}_previous_100step_entropy.csv\".format(task)), index=False)\n",
    "\n",
    "\n",
    "def export_prepost_entropy(tdata, mice, tasks):\n",
    "    # entropy\n",
    "    cidata = tdata.mice_task[\n",
    "        (tdata.mice_task.task.isin(tasks)) &\n",
    "        (tdata.mice_task.event_type.isin([\"reward\",\"failure\"]))]\n",
    "    for task in tasks:\n",
    "        with open('data/prepost_entropy_task_{}.csv'.format(task), 'w', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for mouse_id in mice:\n",
    "                sz = cidata[(cidata.task == task) & (cidata.mouse_id == mouse_id)].reset_index()[\"entropy_50\"].size\n",
    "                print(\"size = {}\".format(sz))\n",
    "                if sz >= 300:\n",
    "                    pre  = cidata[(cidata.task == task) & (cidata.mouse_id == mouse_id)].reset_index()[\"entropy_50\"].iat[50]\n",
    "                    post = cidata[(cidata.task == task) & (cidata.mouse_id == mouse_id)].reset_index()[\"entropy_50\"].iat[300]\n",
    "                    writer.writerow([mouse_id, pre, post])\n",
    "                else:\n",
    "                    print(\"error\")\n",
    "\n",
    "\n",
    "def view_averaged_prob_same_prev(tdata, mice, tasks):\n",
    "    m = []\n",
    "    t = []\n",
    "    csame = []\n",
    "    fsame = []\n",
    "\n",
    "    for mouse_id in mice:\n",
    "        for task in tasks:\n",
    "            m += [mouse_id]\n",
    "            t += [task]\n",
    "            csame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['c_same']]\n",
    "            fsame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['f_same']]\n",
    "\n",
    "    after_prob_df = pd.DataFrame(\n",
    "        data={'mouse_id': m, 'task': t, 'c_same': csame, 'f_same': fsame},\n",
    "        columns=['mouse_id', 'task', 'c_same', 'f_same']\n",
    "    )\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(8, 4), dpi=100)\n",
    "    for task in tasks:\n",
    "        plt.subplot(1, len(tasks), tasks.index(task) + 1)\n",
    "\n",
    "        c_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['c_same'].to_list())\n",
    "        c_same_avg = np.mean(c_same, axis=0)\n",
    "        c_same_std = np.std(c_same, axis=0)\n",
    "        c_same_var = np.var(c_same, axis=0)\n",
    "\n",
    "        f_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['f_same'].to_list())\n",
    "        f_same_avg = np.mean(f_same, axis=0)\n",
    "        f_same_var = np.var(f_same, axis=0)\n",
    "\n",
    "        # 各個体ごと平均確率からの差\n",
    "        c_same_indiv_avg = c_same.groupby(\"mouse_id\").mean()\n",
    "        f_same_indiv_avg = f_same.groupby(\"mouse_id\").mean()\n",
    "        \n",
    "        task_prob_df = after_prob_df[after_prob_df['task'].isin([task])]\n",
    "        c_same_norm = ((task_prob_df['c_same']) - (pd.merge(task_prob_df['mouse_id'],c_same_indiv_avg,how='left',on=\"mouse_id\")[\"c_same\"])).to_numpy()\n",
    "        f_same_norm =((task_prob_df['f_same']) - (pd.merge(task_prob_df['mouse_id'],c_same_indiv_avg,how='left',on=\"mouse_id\")[\"f_same\"])).to_numpy()\n",
    "        after_prob_df = after_prob_df.assign(c_same_norm=c_same_norm)\n",
    "        after_prob_df = after_prob_df.assign(f_same_norm=f_same_norm)\n",
    "        \n",
    "        xlen = len(c_same_avg)\n",
    "        xax = np.array(range(1, xlen + 1))\n",
    "        plt.plot(xax, c_same_avg, label=\"rewarded start\")\n",
    "        plt.errorbar(xax, c_same_avg, yerr=c_same_norm, capsize=2, fmt='o', markersize=1, ecolor='black',\n",
    "                     markeredgecolor=\"black\", color='w', lolims=True)\n",
    "\n",
    "        plt.plot(np.array(range(1, xlen + 1)), f_same_avg, label=\"no-rewarded start\")\n",
    "        plt.errorbar(xax, f_same_avg, yerr=f_same_norm, capsize=2, fmt='o', markersize=1, ecolor='black',\n",
    "                     markeredgecolor=\"black\", color='w', uplims=True)\n",
    "\n",
    "        # plt.ion()\n",
    "        plt.xticks(np.arange(1, xlen + 1, 1))\n",
    "        plt.xlim(0.5, xlen + 0.5)\n",
    "        plt.ylim(0.25, 0.6)\n",
    "        if tasks.index(task) == 0:\n",
    "            plt.ylabel('P (same choice)')\n",
    "            plt.legend()\n",
    "        plt.xlabel('Trial')\n",
    "        plt.title('{}'.format(task))\n",
    "    # plt.savefig('fig/{}_prob_all4.png'.format(graph_ins.exportpath))\n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "    plt.savefig('fig/prob_all4_{}.png'.format(tasks[0]))\n",
    "    plt.show()\n",
    "    \n",
    "#    for mouse_id in mice:\n",
    "#        for task in tasks:\n",
    "#            after_prob_df[after_prob_df['mouse_id'].isin([mouse_id])][after_prob_df['task'].isin([task])][c_same].to_csv('data/{0}_{1}_correct.csv'.format(mouse_id, task))\n",
    "#           after_prob_df[after_prob_df['mouse_id'].isin([mouse_id])][after_prob_df['task'].isin([task])][f_same].to_csv('data/{0}_{1}_correct.csv'.format(mouse_id, task))\n",
    "\n",
    "#     after_prob_df[after_prob_df['task'].isin([task])].to_csv('data/{0}_ciprob.csv'.format(task))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mice90 = [35,36,38,39,40,41,42,43]\n",
    "tasks90 = [\"All5_90\"]\n",
    "\n",
    "logpath = '~/PycharmProjects/RaspSkinnerBox/MiceAnalysis'\n",
    "tdata90 = task_data(mice90, tasks90, logpath)\n",
    "\n",
    "#view_averaged_prob_same_prev(tdata90, mice90, tasks90)\n",
    "#view_summary(tdata, mice, tasks)\n",
    "\n",
    "# 45(体重無し?) ファイルが無い・要探索!\n",
    "# 27(体重無し)\n",
    "# 33: 317まで\n",
    "mice50 = [27,30,31,33,47,49,50]\n",
    "tasks50 = [\"All5_50\"]\n",
    "tdata50 = task_data(mice50, tasks50, logpath)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "export_P_20(tdata30, mice30, tasks30).to_csv(\"{}_aaaaa.csv\".format(tasks30[0]))\n",
    "export_P_20(tdata50, mice50, tasks50).to_csv(\"{}_aaaaa.csv\".format(tasks50[0]))\n",
    "export_P_20(tdata90, mice90, tasks90).to_csv(\"{}_aaaaa.csv\".format(tasks90[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def view_averaged_prob_same_prev_avg(tdata, mice, tasks):\n",
    "    m = []\n",
    "    t = []\n",
    "    csame = []\n",
    "    fsame = []\n",
    "    m_average = []\n",
    "\n",
    "    for mouse_id in mice:\n",
    "        for task in tasks:\n",
    "            m += [mouse_id]\n",
    "            t += [task]\n",
    "            csame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['c_same']]\n",
    "            fsame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['f_same']]\n",
    "            m_average += [export_P_20(tdata, mouse_id, task).AVG]\n",
    "\n",
    "    after_prob_df = pd.DataFrame(\n",
    "        data={'mouse_id': m, 'task': t, 'c_same': csame, 'f_same': fsame, 'm_average': m_average},\n",
    "        columns=['mouse_id', 'task', 'c_same', 'f_same', 'm_average']\n",
    "    )\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(8, 4), dpi=100)\n",
    "    for task in tasks:\n",
    "        plt.subplot(1, len(tasks), tasks.index(task) + 1)\n",
    "\n",
    "        c_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['c_same'].to_list())\n",
    "        c_same_avg = np.mean(c_same, axis=0)\n",
    "        c_same_std = np.std(c_same, axis=0)\n",
    "        c_same_var = np.var(c_same, axis=0)\n",
    "        c_same_norm = c_same - np.repeat(np.mean(c_same,axis=1),9).reshape(-1,9)\n",
    "        c_same_norm_var = np.var(c_same_norm,axis=0)\n",
    "        c_same_norm_avg = np.mean(c_same_norm, axis=0)\n",
    "        \n",
    "\n",
    "        f_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['f_same'].to_list())\n",
    "        f_same_avg = np.mean(f_same, axis=0)\n",
    "        f_same_var = np.var(f_same, axis=0)\n",
    "#        f_same_norm = f_same - np.repeat(np.mean(f_same,axis=1),9).reshape(-1,9)\n",
    "        f_same_norm = f_same - after_prob_df[]\n",
    "\n",
    "        f_same_norm_var = np.var(f_same_norm,axis=0)\n",
    "        f_same_norm_avg = np.mean(f_same_norm, axis=0)\n",
    "\n",
    "        xlen = len(c_same_avg)\n",
    "        xax = np.array(range(1, xlen + 1))\n",
    "        plt.plot(xax, c_same_norm_avg, label=\"rewarded start\")\n",
    "        plt.errorbar(xax, c_same_norm_avg, yerr=c_same_norm_var, capsize=2, fmt='o', markersize=1, ecolor='black',\n",
    "                     markeredgecolor=\"black\", color='w', lolims=True)\n",
    "\n",
    "        plt.plot(np.array(range(1, xlen + 1)), f_same_norm_avg, label=\"no-rewarded start\")\n",
    "        plt.errorbar(xax, f_same_norm_avg, yerr=f_same_norm_var, capsize=2, fmt='o', markersize=1, ecolor='black',\n",
    "                     markeredgecolor=\"black\", color='w', uplims=True)\n",
    "\n",
    "        # plt.ion()\n",
    "        plt.xticks(np.arange(1, xlen + 1, 1))\n",
    "        plt.xlim(0.5, xlen + 0.5)\n",
    "#        plt.ylim(0.25, 0.6)\n",
    "        plt.ylim(-0.1, 0.1)\n",
    "        if tasks.index(task) == 0:\n",
    "            plt.ylabel('P (same choice)')\n",
    "            plt.legend()\n",
    "        plt.xlabel('Trial')\n",
    "        plt.title('{}'.format(task))\n",
    "#        plt.hlines(average,-11,11)\n",
    "        plt.hlines(0,-11,11)\n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "    plt.savefig('fig/WSLS_{}.png'.format(tasks[0]))\n",
    "    plt.show()\n",
    "    \n",
    "    #TODO: 1. c_same, f_sameに加えて，各マウスのaverageを各マウスのc_sameから引いて正規化したc_same_norm, f_same_normを計算\n",
    "    #TODO: 2. mice_id, c_same, f_same, c_same_norm, f_same_norm CSV出力 (グラフは両方)\n",
    "    \n",
    "def export_entropy300(tdata, mice, tasks):\n",
    "    # entropy\n",
    "    cidata = tdata.mice_task[\n",
    "        (tdata.mice_task.task.isin(tasks)) &\n",
    "        (tdata.mice_task.event_type.isin([\"reward\",\"failure\"]))]\n",
    "    for task in tasks:\n",
    "        with open('data/entropy_300_task_{}.csv'.format(task), 'w', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for mouse_id in mice:\n",
    "                sz = cidata[(cidata.task == task) & (cidata.mouse_id == mouse_id)][\"entropy_50\"].size\n",
    "#                 print(cidata[(cidata.task == task) & (cidata.mouse_id == mouse_id)])\n",
    "#                 print(\"size = {}\".format(sz))\n",
    "                if sz >= 300:\n",
    "                    post = cidata[(cidata.task == task) & (cidata.mouse_id == mouse_id)].reset_index()[\"entropy_150\"].iat[300]\n",
    "                    writer.writerow([mouse_id, post])\n",
    "                else:\n",
    "                    print(\"error\")\n",
    "\n",
    "\n",
    "# logpath = '~/PycharmProjects/RaspSkinnerBox/MiceAnalysis'\n",
    "logpath = \"./\"\n",
    "#45(体重無し?) ファイルが無い・要探索!\n",
    "# 27(体重無し)\n",
    "# 33: 317まで\n",
    "#mice50 = [27,30,31,33,47,49,50]\n",
    "#mice50 = [52, 64, 67, 68, 70, 71,  27,30,31,33,47,49,50]\n",
    "mice50 = [64, 67, 68, 70, 71,  27,30,31,33,47,49,50]\n",
    "\n",
    "tasks50 = [\"All5_50\"]\n",
    "tdata50 = task_data(mice50, tasks50, logpath)\n",
    "\n",
    "export_entropy300(tdata50, mice50, tasks50)\n",
    "view_averaged_prob_same_prev_avg(tdata50, mice50, tasks50)\n",
    "#view_averaged_prob_same_prev(tdata50, mice50, tasks50)\n",
    "\n",
    "# logpath = '~/PycharmProjects/RaspSkinnerBox/MiceAnalysis'\n",
    "mice30 = [6, 7, 8, 11, 12, 13, 14, 17, 18, 19, 21, 22, 23, 24]\n",
    "tasks30 = [\"All5_30\"]\n",
    "tdata30 = task_data(mice30, tasks30, logpath)\n",
    "export_entropy300(tdata30, mice30, tasks30)\n",
    "view_averaged_prob_same_prev_avg(tdata30, mice30, tasks30)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 10先~19先で基準と同じ穴を選ぶ確率を平均する\n",
    "def export_P_20(tdata, mice, tasks, indiv=False):\n",
    "    data = tdata.mice_task[(tdata.mice_task.mouse_id.isin(mice))&(tdata.mice_task.event_type.isin([\"reward\",\"failure\"]))]\n",
    "    avg = []\n",
    "    avg_c = dict(zip(mice,[[] for _ in mice]))\n",
    "    avg_i = dict(zip(mice,[[] for _ in mice]))\n",
    "    for task in tasks:\n",
    "        task_data = data[data.task.isin([task])]\n",
    "        # 全mice平均\n",
    "        for no in mice:\n",
    "            mice_data = task_data[task_data.mouse_id.isin([no])]\n",
    "            prob = []\n",
    "            prob_co = []\n",
    "            prob_in = []\n",
    "            all_data = mice_data.reset_index()\n",
    "            correct_data = mice_data[mice_data.event_type.isin([\"reward\"])].reset_index()\n",
    "            incorrect_data = mice_data[mice_data.event_type.isin([\"failure\"])].reset_index()\n",
    "            length = len(mice_data)- 20\n",
    "            if length <= 0:\n",
    "                continue\n",
    "            for idx, dat in all_data.drop(range(len(all_data)-20,len(all_data))).iterrows():\n",
    "                tmp = 0\n",
    "                for j in range(10,20):\n",
    "                    if dat[\"hole_no\"] == all_data[\"hole_no\"][idx + j]:\n",
    "                        tmp += 1\n",
    "                prob.append(tmp/10)\n",
    "            avg.append(np.average(np.array(prob)))\n",
    "            # correctデータのみを対象， taskぶち抜き\n",
    "            for idx, dat in correct_data.drop(range(len(correct_data)-20,len(correct_data))).iterrows():\n",
    "                tmp = 0\n",
    "                for j in range(10,20):\n",
    "                    if dat[\"hole_no\"] == correct_data[\"hole_no\"][idx + j]:\n",
    "                        tmp += 1\n",
    "                prob_co.append(tmp/10)\n",
    "            avg_c[no].append(np.array(np.average(np.array(prob_co))))\n",
    "            # incorrect\n",
    "            for idx, dat in incorrect_data.drop(range(len(incorrect_data)-20,len(incorrect_data))).iterrows():\n",
    "                tmp = 0\n",
    "                for j in range(10,20):\n",
    "                    if dat[\"hole_no\"] == incorrect_data[\"hole_no\"][idx + j]:\n",
    "                        tmp += 1\n",
    "                prob_in.append(tmp/10)\n",
    "            avg_i[no].append(np.average(np.array(prob_in)))\n",
    "    for mouse_id in mice:\n",
    "        avg_c[mouse_id] = np.average(avg_c[mouse_id])\n",
    "        avg_i[mouse_id] = np.average(avg_i[mouse_id])\n",
    "    if indiv:\n",
    "        return {\"c_same\":[np.average(np.array(list(avg_c.values())))],\"f_same\":[np.average(np.array(list(avg_i.values())))],\n",
    "               \"c_same_mice\":avg_c,\"f_same_mice\":avg_i}\n",
    "    return pd.DataFrame(data={\"task\":tasks,\"AVG\":[np.average(avg)]})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logpath = \"./\"\n",
    "mice50 = [64, 67, 68, 70, 71,  27,30,31,33,47,49,50]\n",
    "tasks50 = [\"All5_50\"]\n",
    "tdata50 = task_data(mice50, tasks50, logpath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 選択確率(マウスごとの選択偏りを引いた)グラフ\n",
    "mice = mice50\n",
    "tdata = tdata50\n",
    "tasks = tasks50\n",
    "\n",
    "m = []\n",
    "t = []\n",
    "csame = []\n",
    "fsame = []\n",
    "m_average = []\n",
    "\n",
    "for mouse_id in mice:\n",
    "    for task in tasks:\n",
    "        m += [mouse_id]\n",
    "        t += [task]\n",
    "        csame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['c_same']]\n",
    "        fsame += [tdata.task_prob[task][tdata.task_prob[task].mouse_id == mouse_id]['f_same']]\n",
    "        m_average += [export_P_20(tdata, [mouse_id], [task]).AVG]\n",
    "\n",
    "after_prob_df = pd.DataFrame(\n",
    "    data={'mouse_id': m, 'task': t, 'c_same': csame, 'f_same': fsame, 'm_average': m_average},\n",
    "    columns=['mouse_id', 'task', 'c_same', 'f_same', 'm_average']\n",
    ")\n",
    "\n",
    "for task in tasks:\n",
    "\n",
    "    c_same = np.array(after_prob_df[after_prob_df['task'].isin([task])]['c_same'].to_list())\n",
    "    c_same_avg = np.mean(c_same, axis=0)\n",
    "    c_same_std = np.std(c_same, axis=0)\n",
    "    c_same_var = np.var(c_same, axis=0)\n",
    "\n",
    "    c_same_norm = c_same - np.repeat(np.mean(c_same,axis=1),9).reshape(-1,9)\n",
    "    print(c_same_norm)\n",
    "    av = after_prob_df[after_prob_df['task'].isin([task])]['m_average'].to_numpy()\n",
    "    print(after_prob_df[after_prob_df['task'].isin([task])]['m_average'])\n",
    "    avm = np.repeat(av,9).reshape(-1,9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "x = 0.5\n",
    "p = np.linspace(0,1,100)\n",
    "q = p*x / (p*x + (1-p))\n",
    "\n",
    "plt.style.use('default')\n",
    "#fig = plt.figure(figsize=(8, 4), dpi=100)\n",
    "plt.plot(p, q, label=\"rewarded start\")\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 位相性\n",
    "def export_phase_data(tdata, mice, tasks):\n",
    "    for task in tasks:\n",
    "        data = tdata.mice_task[\n",
    "                (tdata.mice_task.task == task) &\n",
    "                (tdata.mice_task.event_type.isin([\"nose poke\"]))]\n",
    "        distance = np.array([\n",
    "            [0,1,2,3,4],\n",
    "            [1,0,1,2,3],\n",
    "            [2,1,0,1,2],\n",
    "            [3,2,1,0,1],\n",
    "            [4,3,2,1,0]])\n",
    "        mice_hole_after_times = {}\n",
    "        mice_hole_select_times = np.empty((0,6), int)\n",
    "        mice_distance = {}\n",
    "        mice_correct = {}\n",
    "        mice_incorrect = {}\n",
    "\n",
    "        for mouse_id in mice:\n",
    "            mice_data = data[data.mouse_id.isin([mouse_id])]\n",
    "            tmp = np.zeros(5*5).reshape(5,5)\n",
    "            if mice_data.empty:\n",
    "                continue\n",
    "\n",
    "            # 1a. 各マウス、各タスク毎に5x5の割合のマトリックスをcsvで出す。\n",
    "            all_data = mice_data.reset_index()\n",
    "            length = len(mice_data)- 1\n",
    "            if length <= 0:\n",
    "                continue\n",
    "            for idx, dat in all_data.drop(range(len(all_data)-1,len(all_data))).iterrows():\n",
    "                tmp[int((int(all_data[\"hole_no\"][idx])-1)/2)][int((int(all_data[\"hole_no\"][idx + 1])-1)/2)] += 1\n",
    "            mice_hole_after_times.update({mouse_id:tmp/len(all_data)})\n",
    "            # export\n",
    "            np.savetxt(\"data/1a_no{}_task{}_selection.csv\".format(mouse_id,task),mice_hole_after_times[mouse_id],fmt=\"%f\",delimiter=',')\n",
    "            \n",
    "            # 2a. 各選択がサブタスク内でどのような割合であるかを示す基礎データをcsvに出す（1列目マウスID, 2～6列目1,3,5,7,9の選択数)。\n",
    "            tmp_select_times = (all_data[\"hole_no\"].value_counts() + pd.Series([0,0,0,0,0],index=[\"1\",\"3\",\"5\",\"7\",\"9\"])).fillna(0)\n",
    "            mouse_hole_select_times = np.concatenate((np.array([mouse_id]),tmp_select_times.to_numpy()/np.sum(tmp_select_times.to_numpy())))\n",
    "            mice_hole_select_times = np.append(mice_hole_select_times,[mouse_hole_select_times],axis=0)\n",
    "\n",
    "            # 3a. 横軸距離(1→1なら0, 1→3なら1, 1→5なら2, 1→7なら3, 1→9なら4, 3→5なら1 ...)、縦軸度数の割合（0～4の距離） のcsvを各マウス、各タスク毎に出力\n",
    "            mouse_dist = [0,0,0,0,0]\n",
    "            for dist,idx in zip(distance,range(len(distance))):\n",
    "                for num,col in zip(dist,range(len(dist))):\n",
    "                    mouse_dist[num] += tmp[idx,col]\n",
    "            mouse_dist /= sum(mouse_dist)\n",
    "            mice_distance.update({mouse_id:mouse_dist})\n",
    "            # export \n",
    "            np.savetxt(\"data/3a_no{}_task{}_selection_distance.csv\".format(mouse_id,task),mouse_dist,delimiter=',',fmt=\"%f\")\n",
    "            \n",
    "            # 4a correctの次の移動距離とincorrectの次の移動距離\n",
    "            dt = tdata.mice_task[(tdata.mice_task.task.isin([task]))]\n",
    "            correct = [0,0,0,0,0]\n",
    "            incorrect = [0,0,0,0,0]\n",
    "            correct_session = dt[dt.is_correct.isin([1])].session_id.values.tolist()\n",
    "            incorrect_session = dt[dt.is_failure.isin([1])].session_id.values.tolist()\n",
    "            for idx, dat in all_data[all_data.session_id.isin(correct_session)].iterrows():\n",
    "                try:\n",
    "                    correct[int(abs((int(all_data[\"hole_no\"][idx])-1)/2-\n",
    "                            (int(all_data[\"hole_no\"][idx+1])-1)/2))] += 1 \n",
    "                except KeyError:\n",
    "                    continue\n",
    "            for idx, dat in all_data[all_data.session_id.isin(incorrect_session)].iterrows():\n",
    "                try:\n",
    "                    incorrect[int(abs((int(all_data[\"hole_no\"][idx])-1)/2-\n",
    "                            (int(all_data[\"hole_no\"][idx+1])-1)/2))] += 1 \n",
    "                except KeyError:\n",
    "                    continue\n",
    "            mice_correct.update({mouse_id:correct})\n",
    "            mice_incorrect.update({mouse_id:incorrect})\n",
    "            np.savetxt(\"data/4a_no{}_task{}_correct.csv\".format(mouse_id,task),correct,delimiter=',',fmt=\"%f\")\n",
    "            np.savetxt(\"data/4a_no{}_task{}_incorrect.csv\".format(mouse_id,task),incorrect,delimiter=',',fmt=\"%f\")\n",
    "          \n",
    "        # 2a export\n",
    "        np.savetxt(\"data/2a_task{}_selection_rate.csv\".format(task),mice_hole_select_times,delimiter=',',fmt=\"%f\")\n",
    "        # 4a \n",
    "        np.savetxt(\"data/4a_no{}_task{}_correct.csv\".format(mouse_id,task),correct,delimiter=',',fmt=\"%f\")\n",
    "          \n",
    "        # 1b. 1aを基に各タスク毎、全マウス分の平均をcsvで出す。\n",
    "        all_mean_1 = np.mean(np.array(list(mice_hole_after_times.values())),axis=0)\n",
    "        np.savetxt(\"data/1b_allmice_task{}_selection_mean.csv\".format(task),all_mean_1,delimiter=',',fmt=\"%f\")\n",
    "        # 2b. 2aを基に各タスク毎、全マウス分の平均をcsvで出す。\n",
    "        all_mean_2 = np.delete(np.mean(mice_hole_select_times,axis=0),0,0)\n",
    "        np.savetxt(\"data/2b_allmice_task{}_selection_rate_mean.csv\".format(task),all_mean_2,delimiter=',',fmt=\"%f\")\n",
    "        # 3b. 3aを基に各タスク毎、全マウス分の平均をcsvで出力。            \n",
    "        all_mean_3 = np.mean(np.array(list(mice_distance.values())), axis=0)\n",
    "        np.savetxt(\"data/3b_allmice_task{}_selection_distance_mean.csv\".format(task),all_mean_3,delimiter=',',fmt=\"%f\")\n",
    "        # 4b\n",
    "        all_mean_4_co = np.mean(np.array(list(mice_correct.values())),axis=0)\n",
    "        all_mean_4_in = np.mean(np.array(list(mice_incorrect.values())),axis=0)\n",
    "        np.savetxt(\"data/4b_allmice_task{}_correct_mean.csv\".format(task),all_mean_4_co,delimiter=',',fmt=\"%f\")\n",
    "        np.savetxt(\"data/4b_allmice_task{}_incorrect_mean.csv\".format(task),all_mean_4_in,delimiter=',',fmt=\"%f\")\n",
    "\n",
    "        \n",
    "    # 出力\n",
    "    # 1a. 各マウス、各タスク毎に5x5の割合のマトリックスをcsvで出す。\n",
    "    # 1b. 1aを基に各タスク毎、全マウス分の平均をcsvで出す。\n",
    "    # 2a. 各選択がサブタスク内でどのような割合であるかを示す基礎データをcsvに出す（1列目マウスID, 2～6列目1,3,5,7,9の選択数)。\n",
    "    # 2b. 2aを基に各タスク毎、全マウス分の平均をcsvで出す。\n",
    "    # 3a. 横軸距離(1→1なら0, 1→3なら1, 1→5なら2, 1→7なら3, 1→9なら4, 3→5なら1 ...)、縦軸度数の割合（0～4の距離） のcsvを各マウス、各タスク毎に出力\n",
    "    # 3b. 3aを基に各タスク毎、全マウス分の平均をcsvで出力。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "export_phase_data(tdata50,mice50,tasks50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count_task(tdata50,mice50,tasks50,[1,3,7,9]))\n",
    "# print(count_task(tdata50,mice50,tasks50,5))\n",
    "# print(count_task(tdata50,mice50,tasks50),[1,3,5,7,9])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "def export_P_20_filter(dc,mice):\n",
    "    hole_range = range(1,10)\n",
    "    avg_c = np.empty((0,len(hole_range)))\n",
    "    avg_f = np.empty((0,len(hole_range)))\n",
    "    avg_c_base = np.empty(0)\n",
    "    avg_i_base = np.empty(0)\n",
    "    \n",
    "    for no in mice:\n",
    "        mice_data = dc[dc.mouse_id.isin([no])]\n",
    "        prob_co = []\n",
    "        prob_in = []\n",
    "        prob_co_base = []\n",
    "        prob_in_base = []\n",
    "        correct_data = mice_data[mice_data.event_type.isin([\"reward\"])].reset_index()\n",
    "        incorrect_data = mice_data[mice_data.event_type.isin([\"failure\"])].reset_index()\n",
    "        length = len(mice_data)- 20\n",
    "        if length <= 0:\n",
    "            continue\n",
    "        \n",
    "        # c_same\n",
    "        # export: mice * hole prob\n",
    "        for idx, dat in correct_data.drop(range(len(correct_data)-10,len(correct_data))).iterrows():\n",
    "            tmp = np.zeros((len(hole_range),))\n",
    "            for j in hole_range:\n",
    "                if dat[\"hole_no\"] == correct_data[\"hole_no\"][idx + j]:\n",
    "                    tmp[j-1] += 1\n",
    "            prob_co.append(tmp)\n",
    "        avg_c = np.append(avg_c, np.array([np.sum(np.array(prob_co),axis=0)/(len(correct_data)-10)]),axis=0)\n",
    "        \n",
    "        # f_same\n",
    "        # export: mice * hole prob\n",
    "        for idx, dat in incorrect_data.drop(range(len(incorrect_data)-10,len(incorrect_data))).iterrows():\n",
    "            tmp = np.zeros((len(hole_range),))\n",
    "            for j in hole_range:\n",
    "                if dat[\"hole_no\"] == incorrect_data[\"hole_no\"][idx + j]:\n",
    "                    tmp[j-1] += 1\n",
    "            prob_in.append(tmp)\n",
    "        avg_f = np.append(avg_f, np.array([np.sum(np.array(prob_in),axis=0)/(len(incorrect_data)-10)]),axis=0)\n",
    "\n",
    "        # base correctデータのみを対象， taskぶち抜き \n",
    "        # export: mice \n",
    "        for idx, dat in correct_data.drop(range(len(correct_data)-20,len(correct_data))).iterrows():\n",
    "            tmp = 0\n",
    "            for j in range(10,20):\n",
    "                if dat[\"hole_no\"] == correct_data[\"hole_no\"][idx + j]:\n",
    "                    tmp += 1\n",
    "            prob_co_base.append(tmp/10)\n",
    "        avg_c_base = np.append(avg_c_base, np.average(np.array(prob_co_base)))\n",
    "        \n",
    "        # incorrect base\n",
    "        # export: mice\n",
    "        for idx, dat in incorrect_data.drop(range(len(incorrect_data)-20,len(incorrect_data))).iterrows():\n",
    "            tmp = 0\n",
    "            for j in range(10,20):\n",
    "                if dat[\"hole_no\"] == incorrect_data[\"hole_no\"][idx + j]:\n",
    "                    tmp += 1\n",
    "            prob_in_base.append(tmp/10)\n",
    "        avg_i_base = np.append(avg_i_base, np.average(np.array(prob_in_base)))\n",
    "        \n",
    "    return {\"c_same\":avg_c,\"f_same\":avg_f,\"c_same_base\":avg_c_base,\"f_same_base\":avg_i_base}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_task(tdata, mice, tasks,selection=[1,3,5,7,9]) -> dict:\n",
    "    # count_taskをclass task_dataの外で下記の仕様で再実装（classから消す必要はない）ßß\n",
    "        if isinstance(selection,str):\n",
    "            selection = [selection]\n",
    "        elif isinstance(selection,int):\n",
    "            selection = [str(selection)]\n",
    "        selection = [str(num)for num in selection]\n",
    "        dc = tdata.mice_task[tdata.mice_task[\"event_type\"].isin([\"reward\", \"failure\"]) & tdata.mice_task.task.isin(tasks)]\n",
    "        dc = dc.reset_index()\n",
    "\n",
    "        after_c_all_task = {}\n",
    "        after_f_all_task = {}\n",
    "\n",
    "        after_c_starts_task = {}\n",
    "        after_f_starts_task = {}\n",
    "\n",
    "        prob_index = [\"c_same\", \"f_same\"]\n",
    "        forward_trace = 10\n",
    "        task_prob = {}\n",
    "        task_prob_hole = dict(zip(selection,[[] for _ in selection]))\n",
    "        tmp_dt = dc[dc[\"hole_no\"].isin(selection)]\n",
    "        prob = export_P_20_filter(tmp_dt,mice)\n",
    "        prob[\"c_same\"] = np.hstack((prob[\"c_same_base\"].reshape((len(mice),1)), prob[\"c_same\"]))\n",
    "        prob[\"f_same\"] = np.hstack((prob[\"f_same_base\"].reshape((len(mice),1)), prob[\"f_same\"]))\n",
    "        \n",
    "        #         prob[\"c_same\"].to_csv(\"./data/mices_task{}_cistart.csv\".format(,\"\".join(task))\n",
    "        \n",
    "\n",
    "        correct_data = pd.DataFrame(prob[\"c_same\"],index=mice)\n",
    "        incorrect_data = pd.DataFrame(prob[\"f_same\"],index=mice)\n",
    "        # 3のタスク毎の任意の複数の選択肢毎の全マウス平均をcsv出力 \n",
    "        correct_data.to_csv(\"./data/mice_task{}_hole{}_cstart.csv\".format(\"-\".join(tasks),\"\".join(selection)))\n",
    "        incorrect_data.to_csv(\"./data/mice_task{}_hole{}_fstart.csv\".format(\"-\".join(tasks),\"\".join(selection)))\n",
    "\n",
    "        return task_prob\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count_task(tdata50,mice50,tasks50,[1,3,7,9])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logpath = \"~/Downloads/takarada\"\n",
    "mice30 = [6, 7, 8, 11, 12, 13, 14, 17, 18, 19, 21, 22, 23, 24]\n",
    "tasks30 = [\"All5_30\"]\n",
    "tdata30 = task_data(mice30, tasks30, logpath)\n",
    "export_entropy300(tdata30, mice30, tasks30)\n",
    "view_averaged_prob_same_prev_avg(tdata30, mice30, tasks30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyentrp import entropy as pent\n",
    "def calc_permutation_entropy(tdata,mice,tasks):\n",
    "    for tsk in tasks:\n",
    "        en_list1 = []\n",
    "        en_list2 = []\n",
    "        len_list = []\n",
    "        for mouse_no in mice:\n",
    "            data = tdata.mice_task[\n",
    "                (tdata.mice_task.mouse_id == mouse_no)\n",
    "                &(tdata.mice_task.task == tsk)\n",
    "                &(tdata.mice_task.hole_no.isin([\"1\",\"3\",\"5\",\"7\",\"9\"]))\n",
    "                &(tdata.mice_task.event_type.isin([\"reward\",\"failure\"]))\n",
    "            ][[\"mouse_id\",\"task\",\"hole_no\",\"event_type\",\"correct_times\"]]\n",
    "            data = data.reset_index()\n",
    "            leng = len(data)\n",
    "            choice_data1 = data.head(300).hole_no.astype(int).values\n",
    "            choice_data2 = data.head(150).hole_no.astype(int).values\n",
    "            en1 = pent.shannon_entropy(choice_data1)\n",
    "            en2 = pent.shannon_entropy(choice_data2)\n",
    "            en_list1.append(en1)\n",
    "            en_list2.append(en2)\n",
    "        en_list_vert = np.stack([mice, en_list1, en_list2])\n",
    "        np.savetxt(f\"./data/entropy/allmice_{tsk}.csv\",en_list_vert,delimiter=',',fmt=\"%f\")\n",
    "    return en_list1, en_list2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "a = export_daily_feeds(tdata30,mice30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a[a.feed < 70]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b = export_daily_feeds(tdata50,mice50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b[b.feed<70]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def export_daily_feeds(mice, end_task=\"\"):\n",
    "    feeds_data = pd.DataFrame(columns=[\"id\",\"timestamp\",\"day\",\"feed\",\"tasks\"])\n",
    "    # mouseid, 日付, 何日目, 粒数, そのときにやっていたタスクのリストをハイフン繋ぎ\n",
    "    for mouse_id in mice:\n",
    "        file = \"./no{:03d}_action.csv\".format(mouse_id)\n",
    "        data = pd.read_csv(file,names=[\"timestamps\", \"task\", \"session_id\", \"correct_times\", \"event_type\", \"hole_no\"],parse_dates=[0]) \n",
    "        if isinstance(data.iloc[0].timestamps, str):\n",
    "            data = pd.read_csv(file,parse_dates=[0])\n",
    "            data.columns = [\"timestamps\", \"task\", \"session_id\", \"correct_times\", \"event_type\", \"hole_no\"]\n",
    "        data = data[data.event_type.isin([\"reward\"])].reset_index()\n",
    "        if data.empty:\n",
    "            continue\n",
    "        start_timestamp = data.timestamps.iloc[0]\n",
    "        start_date = start_timestamp.date() \n",
    "        base_time = start_timestamp.time()\n",
    "        if end_task == \"\":\n",
    "            end_task = list(data.task.unique())[-1]\n",
    "        last_taskday = data[:data[data.task.isin([end_task])].index[-1]].timestamps.iat[-1]\n",
    "        \n",
    "        finish_date = last_taskday.date() - dt.timedelta(days=1)*(last_taskday.time() < base_time)\n",
    "        data = data[data.timestamps < dt.datetime.combine(finish_date+dt.timedelta(days=1),base_time)]\n",
    "        # task\n",
    "        task = \"-\".join(list(data.task.unique()))\n",
    "        # day\n",
    "        for d in pd.date_range(start_date,finish_date,freq=\"D\"):\n",
    "            # index:d\n",
    "            # range\n",
    "            rew = data.event_type[(data.timestamps > dt.datetime.combine(d,base_time)) & \n",
    "                 (data.timestamps < dt.datetime.combine(d+dt.timedelta(days=1),base_time))].count()\n",
    "            feeds_data = feeds_data.append(\n",
    "                pd.Series([mouse_id,d,(d.date()-start_timestamp.date() + dt.timedelta(days=1)).days,rew,task],index=[\"id\",\"timestamp\",\"day\",\"feed\",\"tasks\"]\n",
    "                                                                         ),ignore_index=True)\n",
    "        feeds_data[feeds_data.id.isin([mouse_id])].to_csv(\"./data/no{:03d}_feeds_summary_{}.csv\".format(mouse_id,\"-\".join(list(data.task.unique()))))\n",
    "    feeds_data.to_csv(\"./data/allmice_feeds_summary_{}.csv\".format(\"-\".join(list(data[:data[data.task.isin([end_task])].index[-1]].task.unique()))))\n",
    "    return feeds_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "    id  timestamp day feed          tasks\n0  139 2020-08-12   1  120  T0-left-right\n1  139 2020-08-13   2   73  T0-left-right\n2  139 2020-08-14   3   77  T0-left-right",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>timestamp</th>\n      <th>day</th>\n      <th>feed</th>\n      <th>tasks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>139</td>\n      <td>2020-08-12</td>\n      <td>1</td>\n      <td>120</td>\n      <td>T0-left-right</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>139</td>\n      <td>2020-08-13</td>\n      <td>2</td>\n      <td>73</td>\n      <td>T0-left-right</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>139</td>\n      <td>2020-08-14</td>\n      <td>3</td>\n      <td>77</td>\n      <td>T0-left-right</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 11
    }
   ],
   "source": [
    "mice_lever = [139]\n",
    "export_daily_feeds(mice_lever,\"left\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as collections\n",
    "import matplotlib.markers as markers\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# サブタスクの実施時間をCSV出力\n",
    "def export_subtask_duration_csv(mouse_id):\n",
    "    if not isinstance(mouse_id,list):\n",
    "        mouse_id = [mouse_id]\n",
    "    for no in mouse_id:\n",
    "        file = \"./no{:03d}_action.csv\".format(no)\n",
    "        data = pd.read_csv(file,names=[\"timestamps\", \"task\", \"session_id\", \"correct_times\", \"event_type\", \"hole_no\"],parse_dates=[0])\n",
    "        if isinstance(data.iloc[0].timestamps, str):\n",
    "            data = pd.read_csv(file,parse_dates=[0])\n",
    "            data.columns = [\"timestamps\", \"task\", \"session_id\", \"correct_times\", \"event_type\", \"hole_no\"]\n",
    "        data = data[[\"timestamps\",\"event_type\",\"task\",\"hole_no\"]]\n",
    "        tasks = data.task.unique()\n",
    "        print(tasks)\n",
    "        event_times = pd.pivot_table(data[data.event_type.isin([\"reward\",\"failure\",\"time over\"])],index=\"event_type\",columns=\"task\",aggfunc=\"count\").timestamps\n",
    "        task_duration = pd.DataFrame(data.groupby(\"task\").timestamps.max()-data.groupby(\"task\").timestamps.min())\n",
    "        task_duration.timestamps = task_duration.timestamps / np.timedelta64(1, 'h')\n",
    "        ret_val = event_times.append(task_duration.T).fillna(0)\n",
    "        ret_val = ret_val.rename(index={\"timestamps\":\"duration in hours\"})\n",
    "        # 列の順番をtaskをやった順でソート\n",
    "        ret_val = ret_val.loc[:,tasks]\n",
    "        ret_val.to_csv(\"./data/no{:03d}_summary.csv\".format(no))\n",
    "        # 各タスクの各選択肢毎のerror数, correct数, total trial数を出したい\n",
    "        tasks_df = []\n",
    "        for task in tasks: \n",
    "            tasks_data = data[data.task.isin([task])]\n",
    "            tasks_df.append(tasks_data[tasks_data.event_type.isin([\"failure\"])])\n",
    "            tasks_data = pd.pivot_table(tasks_data[tasks_data.event_type.isin([\"reward\",\"failure\",\"time over\"])],index=\"event_type\",columns=\"hole_no\",aggfunc=\"count\").timestamps.fillna(0)\n",
    "            tasks_data.loc[\"total_trials\"] = tasks_data.sum()\n",
    "            tasks_data.to_csv(\"./data/no{:03d}_{}_selection_trials.csv\".format(no,task))\n",
    "        # return tasks_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = export_subtask_duration_csv(145)\n",
    "a[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a[1].to_csv(\"./data/no145_left_fail.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}